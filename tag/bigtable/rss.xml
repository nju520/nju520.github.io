<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    
    <title>nju520.me</title>
    
    <link>http://localhost:4000</link>
    <description>nju520's Blog</description>
    <language>en-uk</language>
    <managingEditor> nju520</managingEditor>
    <atom:link href="rss" rel="self" type="application/rss+xml" />
    
<item>
  <title>分布式键值存储 Dynamo 的实现原理</title>
  <link>//dynamo</link>
  <author>nju520</author>
  <pubDate>2017-10-24T00:00:00+08:00</pubDate>
  <guid>//dynamo</guid>
  <description><![CDATA[
  <p>在最近的一周时间里，一直都在研究和阅读 Amazon 的一篇论文 <a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo: Amazon’s Highly Available Key-value Store</a>，论文中描述了 Amazon 的高可用分布式键值存储服务 Dynamo 的实现原理。</p>

<p><img src="https://img.nju520.me/2017-10-24-dynamodb.png" alt="dynamodb" /></p>

<p>之前在阅读 Google 的 <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a> 时写了一篇 <a href="https://nju520.me/bigtable-leveldb">浅析 Bigtable 和 LevelDB 的实现</a> 文章分析了 Bigtable 的单机版 LevelDB 的实现原理；在研究 Dynamo 时，作者发现 Dynamo 虽然和 Bigtable 同为 NoSQL，但是它们的实现却有着很大的不同，最主要的原因来自不同的应用场景和不同的目的。</p>

<h2 id="bigtable-和-dynamo">Bigtable 和 Dynamo</h2>

<p>Bigtable 和 Dynamo 两者分别是 Google 和 Amazon 两大巨头给出的存储海量数据的解决方法，作为 NoSQL 两者都具有分布式、容错以及可扩展的几大特性。</p>

<p><img src="https://img.nju520.me/2017-10-24-nosql-main-characteristics.png" alt="nosql-main-characteristics" /></p>

<p>虽然两者都是 NoSQL，并且有着相似的特性，但是它们在侧重的方向上有非常明显的不同，从两个数据库论文的标题中，我们就能看到 Amazon 的 Dynamo 追求的是高可用性并且提供的是类似 MongoDB 的 Key-value 文档存储，而 Bigtable 中描述的数据库却可以用于结构化的数据存储。</p>

<p>由于 Bigtable 和 Dynamo 都属于同一个类别 - NoSQL，所以它们经常会被放在一起进行对比，这篇文章不仅会介绍 Dynamo 的设计理念以及架构等问题，还会就其中的部分问题与 Bigtable 中相对应的概念进行对比，这样能够让我们更加清楚地了解不同的数据库对不同问题，因设计理念的差异做出的权衡。</p>

<h2 id="架构">架构</h2>

<p>在数据库领域中尤其是分布式数据库，最重要的就是服务的架构，多数的分布式系统在设计时都会假设服务运行在廉价的节点上，并没有出众的性能和也不能提供稳定的服务，所以水平扩展和容错的能力是分布式数据库的标配；但是不同的分布式数据库选用了不同的架构来组织大量的节点。</p>

<p>很多的分布式服务例如 GFS 和 Bigtable 都使用了带有主节点的架构来维护整个系统中的元数据，包括节点的位置等信息，而 Dynamo 的实现不同于这些中心化的分布式服务，在 Dynamo 中所有的节点都有着完全相同的职责，会对外界提供同样的服务，所以在整个系统中并不会出现单点故障的问题。</p>

<p><img src="https://img.nju520.me/2017-10-24-dynamo-architecture.png" alt="dynamo-architecture" /></p>

<p>去中心化的架构使得系统的水平扩展非常容易，节点可以在任何时候直接加入到整个 Dynamo 的集群中，并且只会造成集群中少量数据的迁移。</p>

<p>Bigtable 使用了中心化的架构，通过主节点来维护整个系统中全部的元数据信息，但是 Bigtable 本身其实并不会处理来自客户端的读写请求，所有请求都会由客户端直接和从节点通信，不过由于有了中心化的主节点，所以主节点一旦发生故障宕机就会造成服务的不可用，虽然 Bigtable 以及类似的服务通过其他方式解决这个问题，但是这个问题仍然是中心化的设计所造成的。</p>

<p><img src="https://img.nju520.me/2017-10-24-centralized-architecture.png" alt="centralized-architecture" /></p>

<p>中心化或者去中心化并不是一个绝对好或者绝对坏的选择，选择中心化的解决方案能够降低系统实现的复杂度，而去中心化的方式能够避免单点故障，让系统能够更好更快地增加新的节点，提供优秀的水平扩展能力。</p>

<h2 id="分片和复制">分片和复制</h2>

<p>Dynamo 在设计之初就定下了<strong>增量扩展</strong>（Incremental Scalability）的核心需求，这也就需要一种能够在一组节点中动态分片的机制，Dynamo 的分片策略依赖于<em>一致性哈希</em>，通过这种策略 Dynamo 能够将负载合理的分配到不同的存储节点上。</p>

<p>所有的键在存储之前都会通过哈希函数得到一个唯一的值，哈希函数的输出被看做是一个固定长度的环，也就是其输出的最大值和最小值是『连接』到一起的：</p>

<p><img src="https://img.nju520.me/2017-10-24-partition-in-dynamo.png" alt="partition-in-dynamo" /></p>

<p>每一个节点都会被 Dynamo 在这个环中分配一个随机的位置，而这个节点会处理从哈希的输出在当前节点前的所有键；假设我们有一个键值对 <code>(draven, developer)</code>，<code>Hash(draven)</code> 的结果位于上图中的绿色区域，从环中的位置开始按照<strong>顺时针</strong>的顺序寻找，找到的以第一个节点 B 就会成为协调者（coordinator）负责处理当前的键值对，上图中的每一个节点都会负责与其颜色相同的部分。</p>

<p>由于 Dynamo 系统中的每一个节点在刚刚加入当前的集群时，会被分配一个随机的位置，所以由于算法的随机性可能会导致不同节点处理的范围有所不同，最终每一个节点的负载也并不相同；为了解决这个问题，Dynamo 使用了一致性哈希算法的变种，将同一个物理节点分配到环中的多个位置（标记），成为多个虚拟节点，但是在这种策略下，如果当前的 Dynamo 节点一天处理上百万的请求，那么新增节点为了不影响已有节点的性能，会在后台进行启动，整个过程大约会<strong>消耗一整天</strong>的时间，这其实是很难接受的，除此之外这种策略还会造成系统进行日常归档极其缓慢。</p>

<p><img src="https://img.nju520.me/2017-10-24-equal-size-partition-in-dynamo.png" alt="equal-size-partition-in-dynamo" /></p>

<p>为了解决负载的不均衡的问题，除了上面使用虚拟节点的策略之外，Dynamo 论文中还提供了另外两种策略，其中性能相对较好的是将数据的哈希分成 Q 个大小相等的区域，S 个节点每一个处理 Q/S 个分区，当某一个节点因为故障或者其他原因需要退出集群时，会将它处理的数据分片随机分配给其它的节点，当有节点加入系统时，会从其它的节点中『接管』对应的数据分片。上图只是对这种策略下的分片情况简单展示，在真实环境中分片数 Q 的值远远大于节点数 S。</p>

<p>Dynamo 为了达到高可用性和持久性，防止由于节点宕机故障或者数据丢失，将同一份数据在协调者和随后的 <code>N-1</code> 个节点上备份了多次，N 是一个可以配置的值，在一般情况下都为 3。</p>

<p><img src="https://img.nju520.me/2017-10-24-replication-in-dynamo.png" alt="replication-in-dynamo" /></p>

<p>也就是说，上图中黄色区域的值会存储在三个节点 A、B 和 C 中，绿色的区域会被 B、C、D 三个节点处理，从另一个角度来看，A 节点会处理范围在 <code>(C, A]</code> 之间的值，而 B 节点会处理从 <code>(D, B]</code> 区域内的值。</p>

<p><img src="https://img.nju520.me/2017-10-24-replication-range-in-dynamo.png" alt="replication-range-in-dynamo" /></p>

<p>负责存储某一个特定键值对的节点列表叫做偏好列表（preference list），因为虚拟节点在环中会随机存在，为了保证出现节点故障时不会影响可用性和持久性，偏好列表中的全部节点必须都为<strong>不同的物理节点</strong>。</p>

<p>Bigtable 中对分片和复制的实现其实就与 Dynamo 中完全不同，这不仅是因为 Bigtable 的节点有主从之分，还因为 Bigtable 的设计理念与 Dynamo 完全不同。在 Bigtable 中，数据是按照键的顺序存储的，数据存储的单位都是 tablet，每一张表都由多个 tablet 组成，而每一个的 tablet 都有一个 tablet 服务器来处理，而 tablet 的位置都存储在 METADATA 表中。</p>

<p><img src="https://img.nju520.me/2017-10-24-partition-in-bigtable.png" alt="partition-in-bigtable" /></p>

<p>在 Bigtable 中，所有的 tablet 都在 GFS 中以 SSTable 的格式存储起来，这些 SSTable 都被分成了固定大小的块在 chunkserver 上存储，而每一个块也都会在存储在多个 chunkserver 中。</p>

<h2 id="读写请求的执行">读写请求的执行</h2>

<p>Dynamo 集群中的任意节点都能够接受来自客户端的对于任意键的读写请求，所有的请求都通过 RPC 调用执行，客户端在选择节点时有两种不同的策略：一种是通过一个负载均衡器根据负载选择不同的节点，另一种是通过一个清楚当前集群分片的库直接请求相应的节点。</p>

<p><img src="https://img.nju520.me/2017-10-24-node-selecting-strategies.png" alt="node-selecting-strategies" /></p>

<p>从上面我们就已经知道了处理读写请求的节点就叫做协调者（coordinator），前 N 个『健康』的节点会参与读写请求的处理；Dynamo 使用了 Quorum 一致性协议来保证系统中的一致性，协议中有两个可以配置的值：R 和 W，其中 R 是成功参与一个读请求的最小节点数，而 W 是成功参与写请求的最小节点数。</p>

<p><img src="https://img.nju520.me/2017-10-24-dynamo-read-write-operation.png" alt="dynamo-read-write-operation" /></p>

<p>当 R = 2 时，所有的读请求必须等待两个节点成功返回对应键的结果，才认为当前的请求结束了，也就是说读请求的时间取决于返回最慢的节点，对于写请求来说也是完全相同的；当协调者接收到了来自客户端的写请求 <code>put()</code> 时，它会创建一个新的向量时钟（vector clock），然后将新版本的信息存储在本地，之后向偏好列表（preference list）中的前 <code>N-1</code> 个节点发送消息，直到其中的 <code>W-1</code> 个返回这次请求才成功结束，读请求 <code>get()</code> 与上述请求的唯一区别就是，如果协调者发现节点中的数据出现了冲突，就会对冲突尝试进行解决并将结果重新写回对应的节点。</p>

<h2 id="冲突和向量时钟">冲突和向量时钟</h2>

<p>Dynamo 与目前的绝大多数分布式系统一样都提供了<strong>最终一致性</strong>，最终一致性能够允许我们异步的更新集群中的节点，<code>put()</code> 请求可能会在所有的节点后更新前就返回对应的结果了，在这时随后的 <code>get()</code> 就可能获取到过期的数据。</p>

<p><img src="https://img.nju520.me/2017-10-24-inconsistent-in-dynamo.png" alt="inconsistent-in-dynamo" /></p>

<p>如果在系统中出现了节点故障宕机，那么数据的更新可能在一段时间内都不会到达失效的节点，这也是在使用 Dynamo 或者使用相似原理的系统时会遇到的问题，Amazon 中的很多应用虽然都能够忍受这种数据层面可能发生的不一致性，但是有些对业务数据一致性非常高的应用在选择 Dynamo 时就需要好好考虑了。</p>

<p>因为 Dynamo 在工作的过程中不同的节点可能会发生数据不一致的问题，这种问题肯定是需要解决的，Dynamo 能够确保<strong>一旦数据之间发生了冲突不会丢失</strong>，但是可能会有<strong>已被删除的数据重新出现</strong>的问题。</p>

<p>在多数情况下，Dynamo 中的最新版本的数据都会取代之前的版本，系统在这时可以通过语法调解（syntactic reconcile）数据库中的正确版本。但是版本也可能会出现分支，在这时，Dynamo 就会返回所有它无法处理的数据版本，由客户端在多个版本的数据中选择或者创建（collapse）合适的版本返回给 Dynamo，其实这个过程比较像出现冲突的 <code>git merge</code> 操作，git 没有办法判断当前的哪个版本是合适的，所以只能由开发者对分支之间的冲突进行处理。</p>

<p><img src="https://img.nju520.me/2017-10-24-version-evolution-in-dynamo.png" alt="version-evolution-in-dynamo" /></p>

<p>上图中的每一个对象的版本 Dx 中存储着一个或多个向量时钟 <code>[Sn, N]</code>，每次 Dynamo 对数据进行写入时都会更新向量时钟的版本，节点 Sx 第一次写入时向量时钟为 <code>[Sx, 1]</code>，第二次为 <code>[Sx, 2]</code>，在这时假设节点 Sy 和 Sz 都不知道 Sx 已经对节点进行写入了，它们接收到了来自其他客户端的请求，在本地也对同样键做出了写入并分别生成了不同的时钟 <code>[Sy, 1]</code> 和 <code>[Sz, 1]</code>，当客户端再次使用 <code>get()</code> 请求时就会发现数据出现了冲突，由于 Dynamo 无法根据向量时钟自动解决，所以它需要手动合并三个不同的数据版本。</p>

<p>论文中对 24 小时内的请求进行了统计，其中 99.94% 的请求仅会返回一个版本，0.00057% 的请求会返回两个版本，0.00047 的请求会返回三个版本，0.000009% 的请求会返回四个版本，虽然论文中说：</p>

<blockquote>
  <p>This shows that divergent versions are created rarely.</p>
</blockquote>

<p>但是作者仍然认为在海量的数据面前 99.94% 并不是一个特别高的百分比，处理分歧的数据版本仍然会带来额外的工作量和负担。虽然在这种情况下，数据库本身确实没有足够的信息来解决数据的不一致问题，也确实只能由客户端去解决冲突，但是这种将问题抛给上层去解决的方式并不友好，论文中也提到了 Amazon 中使用 Dynamo 的应用程序也都是能够适应并解决这些数据不一致的问题的，不过对于作者来说，仅仅这一个问题就成为不选择 Dynamo 的理由了。</p>

<h2 id="节点的增删">节点的增删</h2>

<p>因为在分布式系统中节点的失效是非常常见的事情，而节点也很少会因为某些原因永久失效，往往大部分节点会临时宕机然后快速重新加入系统；由于这些原因，Dynamo 选择使用了显式的机制向系统中添加和移除节点。</p>

<p><img src="https://img.nju520.me/2017-10-24-ring-membership.png" alt="ring-membership" /></p>

<p>添加节点时可以使用命令行工具或者浏览器连接 Dynamo 中的任意节点后触发一个成员变动的事件，这个事件会从当前的环中移除或者向环中添加一个新的节点，当节点的信息发生改变时，该节点会通过 Gossip 协议通知它所能通知的最多的节点。</p>

<p><img src="https://img.nju520.me/2017-10-24-gossip-protocol.png" alt="gossip-protoco" /></p>

<p>在 Gossip 协议中，每次通讯的两个节点会对当前系统中的节点信息达成一致；通过节点之间互相传递成员信息，最终整个 Dyanmo 的集群中所有的节点都会就成员信息达成一致，如上图所示，”gossip” 首先会被 C 节点接收，然后它会传递给它能接触到的最多的节点 A、D、F、G 四个节点，然后 “gossip” 会进行二次传播传递给系统中的灰色节点，到此为止系统中的所有节点都得到了最新的 “gossip” 消息。</p>

<p>当我们向 Dynamo 中加入了新的节点时，会发生节点之间的分片转移，假设我们连接上了 Dynamo 数据库，然后添加了一个 X 节点，该节点被分配到了如下图所示的 A 和 B 节点之间。</p>

<p><img src="https://img.nju520.me/2017-10-24-adding-storage-node.png" alt="adding-storage-node" /></p>

<p>新引入的节点 X 会从三个节点 C、D、E 中接受它们管理的分片的一部分，也就是上图中彩色的 <code>(E, A]</code>、<code>(A, B]</code> 和 <code>(B, X]</code> 三个部分，在 X 节点加入集群之前分别属于与其颜色相同的节点管理。</p>

<p>Dynamo 由于其去中心化的架构，节点增删的事件都需要通过 Gossip 协议进行传递，然而拥有主从节点之分的 Bigtable 就不需要上述的方式对集群中的节点进行增删了，它可以直接通过用于管理其他从节点的服务直接注册新的节点或者撤下已有的节点。</p>

<h2 id="副本同步">副本同步</h2>

<p>在 Dynamo 运行的过程中，由于一些情况会造成不同节点中的数据不一致的问题，Dynamo 使用了反信息熵（anti-entropy）的策略保证所有的副本存储的信息都是同步的。</p>

<p>为了快速确认多个副本之间的数据的一致性并避免大量的数据传输，Dynamo 使用了 <a href="https://en.wikipedia.org/wiki/Merkle_tree">Merkle tree</a> 对不同节点中的数据进行快速验证。</p>

<p><img src="https://img.nju520.me/2017-10-24-merkle-hash-tree.png" alt="merkle-hash-tree" /></p>

<p>在 Merkle 树中，所有父节点中的内容都是叶子节点的哈希，通过这种方式构建的树形结构能够保证整棵树不会被篡改，任何的改动都能被立刻发现。</p>

<p>Dynamo 中的每一个节点都为其持有的键的范围维护了一颗 Merkle 树，在验证两份节点中的数据是否相同时，只需要发送根节点中的哈希值，如果相同那么说明两棵树的内容全部相同，否则就会依次对比不同层级节点中的内容，直到找出不同的副本，这种做法虽然能够减少数据的传输并能够快速找到副本之间的不同，但是当有新的节点加入或者旧的节点退出时会导致大量的 Merkle 树重新计算。</p>

<h2 id="总结">总结</h2>

<p>在 Dynamo 的论文公开之后，有一篇文章将 Dynamo 的设计称作 <a href="http://jsensarma.com/blog/?p=55">“A flawed architecture”</a>，这篇文章的作者在文中对 Dynamo 的实现进行了分析，主要对其最终一致性和 Quorom 机制进行了批评，它在 <a href="https://news.ycombinator.com/item?id=915212">HackerNews</a> 上也引起了广泛的讨论，帖子中的很多内容都值得一看，能够帮助我们了解 Dynamo 的设计原理，而 Amazon 的 CTO 对于这篇文章也发了一条 Twitter：</p>

<p><img src="https://img.nju520.me/2017-10-24-amazon-cto-twitter-about-dynamo.png" alt="amazon-cto-twitter-about-dynamo" /></p>

<p>不管如何，Dynamo 作为支撑亚马逊业务的底层服务，其实现原理和思想对于整个社区都是非常有价值的，然而它使用的去中心化的策略也带了很多问题，虽然作者可能会因为这个原因在选择数据库时不会 Dynamo，不过相信它也是有合适的应用场景的。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo: Amazon’s Highly Available Key-value Store</a></li>
  <li><a href="http://jsensarma.com/blog/?p=55">Dynamo: A flawed architecture – Part I</a></li>
  <li><a href="http://jsensarma.com/blog/?p=64">Dynamo – Part I: a followup and re-rebuttals</a></li>
  <li><a href="https://www.slideshare.net/GrishaWeintraub/presentation-46722530">Dynamo and BigTable - Review and Comparison</a></li>
  <li><a href="http://vschart.com/compare/dynamo-db/vs/bigtable">DynamoDB vs. BigTable · vsChart</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Merkle_tree">Merkle tree</a></li>
  <li><a href="https://link.springer.com/content/pdf/10.1007/3-540-48184-2_32.pdf">A Digital Signature Based on a Conventional Encryption Function</a></li>
  <li><a href="http://www.raychase.net/2396">Dynamo 的实现技术和去中心化</a></li>
  <li><a href="https://nju520.me/bigtable-leveldb">浅析 Bigtable 和 LevelDB 的实现</a></li>
</ul>

  ]]></description>
</item>

<item>
  <title>浅析 Bigtable 和 LevelDB 的实现</title>
  <link>//bigtable-leveldb</link>
  <author>nju520</author>
  <pubDate>2017-08-12T00:00:00+08:00</pubDate>
  <guid>//bigtable-leveldb</guid>
  <description><![CDATA[
  <p>在 2006 年的 OSDI 上，Google 发布了名为 <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a> 的论文，其中描述了一个用于管理结构化数据的分布式存储系统  - Bigtable 的数据模型、接口以及实现等内容。</p>

<p><img src="https://img.nju520.me/2017-08-12-leveldb-logo.png-1000width" alt="leveldb-logo" /></p>

<p>本文会先对 Bigtable 一文中描述的分布式存储系统进行简单的描述，然后对 Google 开源的 KV 存储数据库 <a href="https://github.com/google/leveldb">LevelDB</a> 进行分析；LevelDB 可以理解为单点的 Bigtable 的系统，虽然其中没有 Bigtable 中与 tablet 管理以及一些分布式相关的逻辑，不过我们可以通过对 LevelDB 源代码的阅读增加对 Bigtable 的理解。</p>

<h2 id="bigtable">Bigtable</h2>

<p>Bigtable 是一个用于管理<strong>结构化数据</strong>的分布式存储系统，它有非常优秀的扩展性，可以同时处理上千台机器中的 PB 级别的数据；Google 中的很多项目，包括 Web 索引都使用 Bigtable 来存储海量的数据；Bigtable 的论文中声称它实现了四个目标：</p>

<p><img src="https://img.nju520.me/2017-08-12-Goals-of-Bigtable.jpg-1000width" alt="Goals-of-Bigtable" /></p>

<p>在作者看来这些目标看看就好，其实并没有什么太大的意义，所有的项目都会对外宣称它们达到了高性能、高可用性等等特性，我们需要关注的是 Bigtable 到底是如何实现的。</p>

<h3 id="数据模型">数据模型</h3>

<p>Bigtable 与数据库在很多方面都非常相似，但是它提供了与数据库不同的接口，它并没有支持全部的关系型数据模型，反而使用了简单的数据模型，使数据可以被更灵活的控制和管理。</p>

<p>在实现中，Bigtable 其实就是一个稀疏的、分布式的、多维持久有序哈希。</p>

<blockquote>
  <p>A Bigtable is a sparse, distributed, persistent multi-dimensional sorted map.</p>
</blockquote>

<p>它的定义其实也就决定了其数据模型非常简单并且易于实现，我们使用 <code>row</code>、<code>column</code> 和 <code>timestamp</code> 三个字段作为这个哈希的键，值就是一个字节数组，也可以理解为字符串。</p>

<p><img src="https://img.nju520.me/2017-08-12-Bigtable-DataModel-Row-Column-Timestamp-Value.jpg-1000width" alt="Bigtable-DataModel-Row-Column-Timestamp-Value" /></p>

<p>这里最重要的就是 <code>row</code> 的值，它的长度最大可以为 64KB，对于同一 <code>row</code> 下数据的读写都可以看做是原子的；因为 Bigtable 是按照 <code>row</code> 的值使用字典顺序进行排序的，每一段 <code>row</code> 的范围都会被 Bigtable 进行分区，并交给一个 tablet 进行处理。</p>

<h3 id="实现">实现</h3>

<p>在这一节中，我们将介绍 Bigtable 论文对于其本身实现的描述，其中包含很多内容：tablet 的组织形式、tablet 的管理、读写请求的处理以及数据的压缩等几个部分。</p>

<h4 id="tablet-的组织形式">tablet 的组织形式</h4>

<p>我们使用类似 B+ 树的三层结构来存储 tablet 的位置信息，第一层是一个单独的 <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf">Chubby</a> 文件，其中保存了根 tablet 的位置。</p>

<blockquote>
  <p>Chubby 是一个分布式锁服务，我们可能会在后面的文章中介绍它。</p>
</blockquote>

<p><img src="https://img.nju520.me/2017-08-12-Tablet-Location-Hierarchy.jpg-1000width" alt="Tablet-Location-Hierarchy" /></p>

<p>每一个 METADATA tablet 包括根节点上的 tablet 都存储了 tablet 的位置和该 tablet 中 key 的最小值和最大值；每一个 METADATA 行大约在内存中存储了 1KB 的数据，如果每一个 METADATA tablet 的大小都为 128MB，那么整个三层结构可以存储 2^61 字节的数据。</p>

<h4 id="tablet-的管理">tablet 的管理</h4>

<p>既然在整个 Bigtable 中有着海量的 tablet 服务器以及数据的分片 tablet，那么 Bigtable 是如何管理海量的数据呢？Bigtable 与很多的分布式系统一样，使用一个主服务器将 tablet 分派给不同的服务器节点。</p>

<p><img src="https://img.nju520.me/2017-08-12-Master-Manage-Tablet-Servers-And-Tablets.jpg-1000width" alt="Master-Manage-Tablet-Servers-And-Tablets" /></p>

<p>为了减轻主服务器的负载，所有的客户端仅仅通过 Master 获取 tablet 服务器的位置信息，它并不会在每次读写时都请求 Master 节点，而是直接与 tablet 服务器相连，同时客户端本身也会保存一份 tablet 服务器位置的缓存以减少与 Master 通信的次数和频率。</p>

<h4 id="读写请求的处理">读写请求的处理</h4>

<p>从读写请求的处理，我们其实可以看出整个 Bigtable 中的各个部分是如何协作的，包括日志、memtable 以及 SSTable 文件。</p>

<p><img src="https://img.nju520.me/2017-08-12-Tablet-Serving.jpg-1000width" alt="Tablet-Serving" /></p>

<p>当有客户端向 tablet 服务器发送写操作时，它会先向 tablet 服务器中的日志追加一条记录，在日志成功追加之后再向 memtable 中插入该条记录；这与现在大多的数据库的实现完全相同，通过顺序写向日志追加记录，然后再向数据库随机写，因为随机写的耗时远远大于追加内容，如果直接进行随机写，可能由于发生设备故障造成数据丢失。</p>

<p>当 tablet 服务器接收到读操作时，它会在 memtable 和 SSTable 上进行合并查找，因为 memtable 和 SSTable 中对于键值的存储都是字典顺序的，所以整个读操作的执行会非常快。</p>

<h4 id="表的压缩">表的压缩</h4>

<p>随着写操作的进行，memtable 会随着事件的推移逐渐增大，当 memtable 的大小超过一定的阈值时，就会将当前的 memtable 冻结，并且创建一个新的 memtable，被冻结的 memtable 会被转换为一个 SSTable 并且写入到 GFS 系统中，这种压缩方式也被称作 <em>Minor Compaction</em>。</p>

<p><img src="https://img.nju520.me/2017-08-12-Minor-Compaction.jpg-1000width" alt="Minor-Compaction" /></p>

<p>每一个 Minor Compaction 都能够创建一个新的 SSTable，它能够有效地降低内存的占用并且降低服务进程异常退出后，过大的日志导致的过长的恢复时间。既然有用于压缩 memtable 中数据的 Minor Compaction，那么就一定有一个对应的 Major Compaction 操作。</p>

<p><img src="https://img.nju520.me/2017-08-12-Major-Compaction.jpg-1000width" alt="Major-Compaction" /></p>

<p>Bigtable 会在<strong>后台周期性</strong>地进行 <em>Major Compaction</em>，将 memtable 中的数据和一部分的 SSTable 作为输入，将其中的键值进行归并排序，生成新的 SSTable 并移除原有的 memtable 和 SSTable，新生成的 SSTable 中包含前两者的全部数据和信息，并且将其中一部分标记未删除的信息彻底清除。</p>

<h4 id="小结">小结</h4>

<p>到这里为止，对于 Google 的 Bigtable 论文的介绍就差不多完成了，当然本文只介绍了其中的一部分内容，关于压缩算法的实现细节、缓存以及提交日志的实现等问题我们都没有涉及，想要了解更多相关信息的读者，这里强烈推荐去看一遍 Bigtable 这篇论文的原文 <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a> 以增强对其实现的理解。</p>

<h2 id="leveldb">LevelDB</h2>

<p>文章前面对于 Bigtable 的介绍其实都是对 <a href="https://github.com/google/leveldb">LevelDB</a> 这部分内容所做的铺垫，当然这并不是说前面的内容就不重要，LevelDB 是对 Bigtable 论文中描述的键值存储系统的单机版的实现，它提供了一个极其高速的键值存储系统，并且由 Bigtable 的作者 <a href="https://research.google.com/pubs/jeff.html">Jeff Dean</a> 和 <a href="https://research.google.com/pubs/SanjayGhemawat.html">Sanjay Ghemawat</a> 共同完成，可以说高度复刻了 Bigtable 论文中对于其实现的描述。</p>

<p>因为 Bigtable 只是一篇论文，同时又因为其实现依赖于 Google 的一些不开源的基础服务：GFS、Chubby 等等，我们很难接触到它的源代码，不过我们可以通过 LevelDB 更好地了解这篇论文中提到的诸多内容和思量。</p>

<h3 id="概述">概述</h3>

<p>LevelDB 作为一个键值存储的『仓库』，它提供了一组非常简单的增删改查接口：</p>

<pre><code class="language-cpp">class DB {
 public:
  virtual Status Put(const WriteOptions&amp; options, const Slice&amp; key, const Slice&amp; value) = 0;
  virtual Status Delete(const WriteOptions&amp; options, const Slice&amp; key) = 0;
  virtual Status Write(const WriteOptions&amp; options, WriteBatch* updates) = 0;
  virtual Status Get(const ReadOptions&amp; options, const Slice&amp; key, std::string* value) = 0;
}
</code></pre>

<blockquote>
  <p><code>Put</code> 方法在内部最终会调用 <code>Write</code> 方法，只是在上层为调用者提供了两个不同的选择。</p>
</blockquote>

<p><code>Get</code> 和 <code>Put</code> 是 LevelDB 为上层提供的用于读写的接口，如果我们能够对读写的过程有一个非常清晰的认知，那么理解 LevelDB 的实现就不是那么困难了。</p>

<p>在这一节中，我们将先通过对读写操作的分析了解整个工程中的一些实现，并在遇到问题和新的概念时进行解释，我们会在这个过程中一步一步介绍 LevelDB 中一些重要模块的实现以达到掌握它的原理的目标。</p>

<h3 id="从写操作开始">从写操作开始</h3>

<p>首先来看 <code>Get</code> 和 <code>Put</code> 两者中的写方法：</p>

<pre><code class="language-cpp">Status DB::Put(const WriteOptions&amp; opt, const Slice&amp; key, const Slice&amp; value) {
  WriteBatch batch;
  batch.Put(key, value);
  return Write(opt, &amp;batch);
}

Status DBImpl::Write(const WriteOptions&amp; options, WriteBatch* my_batch) {
    ...
}
</code></pre>

<p>正如上面所介绍的，<code>DB::Put</code> 方法将传入的参数封装成了一个 <code>WritaBatch</code>，然后仍然会执行 <code>DBImpl::Write</code> 方法向数据库中写入数据；写入方法 <code>DBImpl::Write</code> 其实是一个是非常复杂的过程，包含了很多对上下文状态的判断，我们先来看一个写操作的整体逻辑：</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Put.jpg-1000width" alt="LevelDB-Put" /></p>

<p>从总体上看，LevelDB 在对数据库执行写操作时，会有三个步骤：</p>

<ol>
  <li>调用 <code>MakeRoomForWrite</code> 方法为即将进行的写入提供足够的空间；
    <ul>
      <li>在这个过程中，由于 memtable 中空间的不足可能会冻结当前的 memtable，发生 Minor Compaction 并创建一个新的 <code>MemTable</code> 对象；</li>
      <li>在某些条件满足时，也可能发生 Major Compaction，对数据库中的 SSTable 进行压缩；</li>
    </ul>
  </li>
  <li>通过 <code>AddRecord</code> 方法向日志中追加一条写操作的记录；</li>
  <li>再向日志成功写入记录后，我们使用 <code>InsertInto</code> 直接插入 memtable 中，完成整个写操作的流程；</li>
</ol>

<p>在这里，我们并不会提供 LevelDB 对于 <code>Put</code> 方法实现的全部代码，只会展示一份精简后的代码，帮助我们大致了解一下整个写操作的流程：</p>

<pre><code class="language-cpp">Status DBImpl::Write(const WriteOptions&amp; options, WriteBatch* my_batch) {
  Writer w(&amp;mutex_);
  w.batch = my_batch;

  MakeRoomForWrite(my_batch == NULL);

  uint64_t last_sequence = versions_-&gt;LastSequence();
  Writer* last_writer = &amp;w;
  WriteBatch* updates = BuildBatchGroup(&amp;last_writer);
  WriteBatchInternal::SetSequence(updates, last_sequence + 1);
  last_sequence += WriteBatchInternal::Count(updates);

  log_-&gt;AddRecord(WriteBatchInternal::Contents(updates));
  WriteBatchInternal::InsertInto(updates, mem_);

  versions_-&gt;SetLastSequence(last_sequence);
  return Status::OK();
}
</code></pre>

<h4 id="不可变的-memtable">不可变的 memtable</h4>

<p>在写操作的实现代码 <code>DBImpl::Put</code> 中，写操作的准备过程 <code>MakeRoomForWrite</code> 是我们需要注意的一个方法：</p>

<pre><code class="language-cpp">Status DBImpl::MakeRoomForWrite(bool force) {
  uint64_t new_log_number = versions_-&gt;NewFileNumber();
  WritableFile* lfile = NULL;
  env_-&gt;NewWritableFile(LogFileName(dbname_, new_log_number), &amp;lfile);

  delete log_;
  delete logfile_;
  logfile_ = lfile;
  logfile_number_ = new_log_number;
  log_ = new log::Writer(lfile);
  imm_ = mem_;
  has_imm_.Release_Store(imm_);
  mem_ = new MemTable(internal_comparator_);
  mem_-&gt;Ref();
  MaybeScheduleCompaction();
  return Status::OK();
}
</code></pre>

<p>当 LevelDB 中的 memtable 已经被数据填满导致内存已经快不够用的时候，我们会开始对 memtable 中的数据进行冻结并创建一个新的 <code>MemTable</code> 对象。</p>

<p><img src="https://img.nju520.me/2017-08-12-Immutable-MemTable.jpg-1000width" alt="Immutable-MemTable" /></p>

<p>你可以看到，与 Bigtable 中论文不同的是，LevelDB 中引入了一个不可变的 memtable 结构 imm，它的结构与 memtable 完全相同，只是其中的所有数据都是不可变的。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Serving.jpg-1000width" alt="LevelDB-Serving" /></p>

<p>在切换到新的 memtable 之后，还可能会执行 <code>MaybeScheduleCompaction</code> 来触发一次 Minor Compaction 将 imm 中数据固化成数据库中的 SSTable；imm 的引入能够解决由于 memtable 中数据过大导致压缩时不可写入数据的问题。</p>

<p>引入 imm 后，如果 memtable 中的数据过多，我们可以直接将 memtable 指针赋值给 imm，然后创建一个新的 MemTable 实例，这样就可以继续接受外界的写操作，不再需要等待 Minor Compaction 的结束了。</p>

<h4 id="日志记录的格式">日志记录的格式</h4>

<p>作为一个持久存储的 KV 数据库，LevelDB 一定要有日志模块以支持错误发生时恢复数据，我们想要深入了解 LevelDB 的实现，那么日志的格式是一定绕不开的问题；这里并不打算展示用于追加日志的方法 <code>AddRecord</code> 的实现，因为方法中只是实现了对表头和字符串的拼接。</p>

<p>日志在 LevelDB 是以块的形式存储的，每一个块的长度都是 32KB，<strong>固定的块长度</strong>也就决定了日志可能存放在块中的任意位置，LevelDB 中通过引入一位 <code>RecordType</code> 来表示当前记录在块中的位置：</p>

<pre><code class="language-cpp">enum RecordType {
  // Zero is reserved for preallocated files
  kZeroType = 0,
  kFullType = 1,
  // For fragments
  kFirstType = 2,
  kMiddleType = 3,
  kLastType = 4
};
</code></pre>

<p>日志记录的类型存储在该条记录的头部，其中还存储了 4 字节日志的 CRC 校验、记录的长度等信息：</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-log-format-and-recordtype.jpg-1000width" alt="LevelDB-log-format-and-recordtype" /></p>

<p>上图中一共包含 4 个块，其中存储着 6 条日志记录，我们可以通过 <code>RecordType</code> 对每一条日志记录或者日志记录的一部分进行标记，并在日志需要使用时通过该信息重新构造出这条日志记录。</p>

<pre><code class="language-cpp">virtual Status Sync() {
  Status s = SyncDirIfManifest();
  if (fflush_unlocked(file_) != 0 ||
      fdatasync(fileno(file_)) != 0) {
    s = Status::IOError(filename_, strerror(errno));
  }
  return s;
}
</code></pre>

<p>因为向日志中写新记录都是顺序写的，所以它写入的速度非常快，当在内存中写入完成时，也会直接将缓冲区的这部分的内容 <code>fflush</code> 到磁盘上，实现对记录的持久化，用于之后的错误恢复等操作。</p>

<h4 id="记录的插入">记录的插入</h4>

<p>当一条数据的记录写入日志时，这条记录仍然无法被查询，只有当该数据写入 memtable 后才可以被查询，而这也是这一节将要介绍的内容，无论是数据的插入还是数据的删除都会向 memtable 中添加一条记录。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Memtable-Key-Value-Format.jpg-1000width" alt="LevelDB-Memtable-Key-Value-Format" /></p>

<p>添加和删除的记录的区别就是它们使用了不用的 <code>ValueType</code> 标记，插入的数据会将其设置为 <code>kTypeValue</code>，删除的操作会标记为 <code>kTypeDeletion</code>；但是它们实际上都向 memtable 中插入了一条数据。</p>

<pre><code class="language-cpp">virtual void Put(const Slice&amp; key, const Slice&amp; value) {
  mem_-&gt;Add(sequence_, kTypeValue, key, value);
  sequence_++;
}
virtual void Delete(const Slice&amp; key) {
  mem_-&gt;Add(sequence_, kTypeDeletion, key, Slice());
  sequence_++;
}
</code></pre>

<p>我们可以看到它们都调用了 memtable 的 <code>Add</code> 方法，向其内部的数据结构 skiplist 以上图展示的格式插入数据，这条数据中既包含了该记录的键值、序列号以及这条记录的种类，这些字段会在拼接后存入 skiplist；既然我们并没有在 memtable 中对数据进行删除，那么我们是如何保证每次取到的数据都是最新的呢？首先，在 skiplist 中，我们使用了自己定义的一个 <code>comparator</code>：</p>

<pre><code class="language-cpp">int InternalKeyComparator::Compare(const Slice&amp; akey, const Slice&amp; bkey) const {
  int r = user_comparator_-&gt;Compare(ExtractUserKey(akey), ExtractUserKey(bkey));
  if (r == 0) {
    const uint64_t anum = DecodeFixed64(akey.data() + akey.size() - 8);
    const uint64_t bnum = DecodeFixed64(bkey.data() + bkey.size() - 8);
    if (anum &gt; bnum) {
      r = -1;
    } else if (anum &lt; bnum) {
      r = +1;
    }
  }
  return r;
}
</code></pre>

<blockquote>
  <p>比较的两个 key 中的数据可能包含的内容都不完全相同，有的会包含键值、序列号等全部信息，但是例如从 <code>Get</code> 方法调用过来的 key 中可能就只包含键的长度、键值和序列号了，但是这并不影响这里对数据的提取，因为我们只从每个 key 的头部提取信息，所以无论是完整的 key/value 还是单独的 key，我们都不会取到 key 之外的任何数据。</p>
</blockquote>

<p>该方法分别从两个不同的 key 中取出键和序列号，然后对它们进行比较；比较的过程就是使用 <code>InternalKeyComparator</code> 比较器，它通过 <code>user_key</code> 和 <code>sequence_number</code> 进行排序，其中 <code>user_key</code> 按照递增的顺序排序、<code>sequence_number</code> 按照递减的顺序排序，因为随着数据的插入序列号是不断递增的，所以我们可以保证先取到的都是最新的数据或者删除信息。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-MemTable-SkipList.jpg-1000width" alt="LevelDB-MemTable-SkipList" /></p>

<p>在序列号的帮助下，我们并不需要对历史数据进行删除，同时也能加快写操作的速度，提升 LevelDB 的写性能。</p>

<h3 id="数据的读取">数据的读取</h3>

<p>从 LevelDB 中读取数据其实并不复杂，memtable 和 imm 更像是两级缓存，它们在内存中提供了更快的访问速度，如果能直接从内存中的这两处直接获取到响应的值，那么它们一定是最新的数据。</p>

<blockquote>
  <p>LevelDB 总会将新的键值对写在最前面，并在数据压缩时删除历史数据。</p>
</blockquote>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Read-Processes.jpg-1000width" alt="LevelDB-Read-Processes" /></p>

<p>数据的读取是按照 MemTable、Immutable MemTable 以及不同层级的 SSTable 的顺序进行的，前两者都是在内存中，后面不同层级的 SSTable 都是以 <code>*.ldb</code> 文件的形式持久存储在磁盘上，而正是因为有着不同层级的 SSTable，所以我们的数据库的名字叫做 LevelDB。</p>

<p>精简后的读操作方法的实现代码是这样的，方法的脉络非常清晰，作者相信这里也不需要过多的解释：</p>

<pre><code class="language-cpp">Status DBImpl::Get(const ReadOptions&amp; options, const Slice&amp; key, std::string* value) {
  LookupKey lkey(key, versions_-&gt;LastSequence());
  if (mem_-&gt;Get(lkey, value, NULL)) {
    // Done
  } else if (imm_ != NULL &amp;&amp; imm_-&gt;Get(lkey, value, NULL)) {
    // Done
  } else {
    versions_-&gt;current()-&gt;Get(options, lkey, value, NULL);
  }

  MaybeScheduleCompaction();
  return Status::OK();
}
</code></pre>

<p>当 LevelDB 在 memtable 和 imm 中查询到结果时，如果查询到了数据并不一定表示当前的值一定存在，它仍然需要判断 <code>ValueType</code> 来确定当前记录是否被删除。</p>

<h4 id="多层级的-sstable">多层级的 SSTable</h4>

<p>当 LevelDB 在内存中没有找到对应的数据时，它才会到磁盘中多个层级的 SSTable 中进行查找，这个过程就稍微有一点复杂了，LevelDB 会在多个层级中逐级进行查找，并且不会跳过其中的任何层级；在查找的过程就涉及到一个非常重要的数据结构 <code>FileMetaData</code>：</p>

<p><img src="https://img.nju520.me/2017-08-12-FileMetaData.jpg-1000width" alt="FileMetaData" /></p>

<p><code>FileMetaData</code> 中包含了整个文件的全部信息，其中包括键的最大值和最小值、允许查找的次数、文件被引用的次数、文件的大小以及文件号，因为所有的 <code>SSTable</code> 都是以固定的形式存储在同一目录下的，所以我们可以通过文件号轻松查找到对应的文件。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Level0-Layer.jpg-1000width" alt="LevelDB-Level0-Laye" /></p>

<p>查找的顺序就是从低到高了，LevelDB 首先会在 Level0 中查找对应的键。但是，与其他层级不同，Level0 中多个 SSTable 的键的范围有重合部分的，在查找对应值的过程中，会依次查找 Level0 中固定的 4 个 SSTable。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-LevelN-Layers.jpg-1000width" alt="LevelDB-LevelN-Layers" /></p>

<p>但是当涉及到更高层级的 SSTable 时，因为同一层级的 SSTable 都是没有重叠部分的，所以我们在查找时可以利用已知的 SSTable 中的极值信息 <code>smallest/largest</code> 快速查找到对应的 SSTable，再判断当前的 SSTable 是否包含查询的 key，如果不存在，就继续查找下一个层级直到最后的一个层级 <code>kNumLevels</code>（默认为 7 级）或者查询到了对应的值。</p>

<h4 id="sstable-的合并">SSTable 的『合并』</h4>

<p>既然 LevelDB 中的数据是通过多个层级的 SSTable 组织的，那么它是如何对不同层级中的 SSTable 进行合并和压缩的呢；与 Bigtable 论文中描述的两种 Compaction 几乎完全相同，LevelDB 对这两种压缩的方式都进行了实现。</p>

<p>无论是读操作还是写操作，在执行的过程中都可能调用 <code>MaybeScheduleCompaction</code> 来尝试对数据库中的 SSTable 进行合并，当合并的条件满足时，最终都会执行 <code>BackgroundCompaction</code> 方法在后台完成这个步骤。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-BackgroundCompaction-Processes.jpg-1000width" alt="LevelDB-BackgroundCompaction-Processes" /></p>

<p>这种合并分为两种情况，一种是 Minor Compaction，即内存中的数据超过了 memtable 大小的最大限制，改 memtable 被冻结为不可变的 imm，然后执行方法 <code>CompactMemTable()</code> 对内存表进行压缩。</p>

<pre><code class="language-cpp">void DBImpl::CompactMemTable() {
  VersionEdit edit;
  Version* base = versions_-&gt;current();
  WriteLevel0Table(imm_, &amp;edit, base);
  versions_-&gt;LogAndApply(&amp;edit, &amp;mutex_);
  DeleteObsoleteFiles();
}
</code></pre>

<p><code>CompactMemTable</code> 会执行 <code>WriteLevel0Table</code> 将当前的 imm 转换成一个 Level0 的 SSTable 文件，同时由于 Level0 层级的文件变多，可能会继续触发一个新的 Major Compaction，在这里我们就需要在这里选择需要压缩的合适的层级：</p>

<pre><code class="language-cpp">Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) {
  FileMetaData meta;
  meta.number = versions_-&gt;NewFileNumber();
  Iterator* iter = mem-&gt;NewIterator();
  BuildTable(dbname_, env_, options_, table_cache_, iter, &amp;meta);

  const Slice min_user_key = meta.smallest.user_key();
  const Slice max_user_key = meta.largest.user_key();
  int level = base-&gt;PickLevelForMemTableOutput(min_user_key, max_user_key);
  edit-&gt;AddFile(level, meta.number, meta.file_size, meta.smallest, meta.largest);
  return Status::OK();
}
</code></pre>

<p>所有对当前 SSTable 数据的修改由一个统一的 <code>VersionEdit</code> 对象记录和管理，我们会在后面介绍这个对象的作用和实现，如果成功写入了就会返回这个文件的元数据 <code>FileMetaData</code>，最后调用 <code>VersionSet</code> 的方法 <code>LogAndApply</code> 将文件中的全部变化如实记录下来，最后做一些数据的清理工作。</p>

<p>当然如果是 Major Compaction 就稍微有一些复杂了，不过整理后的 <code>BackgroundCompaction</code> 方法的逻辑非常清晰：</p>

<pre><code class="language-cpp">void DBImpl::BackgroundCompaction() {
  if (imm_ != NULL) {
    CompactMemTable();
    return;
  }

  Compaction* c = versions_-&gt;PickCompaction();
  CompactionState* compact = new CompactionState(c);
  DoCompactionWork(compact);
  CleanupCompaction(compact);
  DeleteObsoleteFiles();
}
</code></pre>

<p>我们从当前的 <code>VersionSet</code> 中找到需要压缩的文件信息，将它们打包存入一个 <code>Compaction</code> 对象，该对象需要选择两个层级的 SSTable，低层级的表很好选择，只需要选择大小超过限制的或者查询次数太多的 SSTable；当我们选择了低层级的一个 SSTable 后，就在更高的层级选择与该 SSTable 有重叠键的 SSTable 就可以了，通过 <code>FileMetaData</code> 中数据的帮助我们可以很快找到待压缩的全部数据。</p>

<blockquote>
  <p>查询次数太多的意思就是，当客户端调用多次 <code>Get</code> 方法时，如果这次 <code>Get</code> 方法在某个层级的 SSTable 中找到了对应的键，那么就算做上一层级中包含该键的 SSTable 的一次查找，也就是这次查找由于不同层级键的覆盖范围造成了更多的耗时，每个 SSTable 在创建之后的 <code>allowed_seeks</code> 都为 100 次，当 <code>allowed_seeks &lt; 0</code> 时就会触发该文件的与更高层级和合并，以减少以后查询的查找次数。</p>
</blockquote>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Pick-Compactions.jpg-1000width" alt="LevelDB-Pick-Compactions" /></p>

<p>LevelDB 中的 <code>DoCompactionWork</code> 方法会对所有传入的 SSTable 中的键值使用归并排序进行合并，最后会在高高层级（图中为 Level2）中生成一个新的 SSTable。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-After-Compactions.jpg-1000width" alt="LevelDB-After-Compactions" /></p>

<p>这样下一次查询 17~40 之间的值时就可以减少一次对 SSTable 中数据的二分查找以及读取文件的时间，提升读写的性能。</p>

<h4 id="存储-db-状态的-versionset">存储 db 状态的 VersionSet</h4>

<p>LevelDB 中的所有状态其实都是被一个 <code>VersionSet</code> 结构所存储的，一个 <code>VersionSet</code> 包含一组 <code>Version</code> 结构体，所有的 <code>Version</code> 包括历史版本都是通过双向链表连接起来的，但是只有一个版本是当前版本。</p>

<p><img src="https://img.nju520.me/2017-08-12-VersionSet-Version-And-VersionEdit.jpg-1000width" alt="VersionSet-Version-And-VersionEdit" /></p>

<p>当 LevelDB 中的 SSTable 发生变动时，它会生成一个 <code>VersionEdit</code> 结构，最终执行 <code>LogAndApply</code> 方法：</p>

<pre><code class="language-cpp">Status VersionSet::LogAndApply(VersionEdit* edit, port::Mutex* mu) {
  Version* v = new Version(this);
  Builder builder(this, current_);
  builder.Apply(edit);
  builder.SaveTo(v);

  std::string new_manifest_file;
  new_manifest_file = DescriptorFileName(dbname_, manifest_file_number_);
  env_-&gt;NewWritableFile(new_manifest_file, &amp;descriptor_file_);

  std::string record;
  edit-&gt;EncodeTo(&amp;record);
  descriptor_log_-&gt;AddRecord(record);
  descriptor_file_-&gt;Sync();

  SetCurrentFile(env_, dbname_, manifest_file_number_);
  AppendVersion(v);

  return Status::OK();
}
</code></pre>

<p>该方法的主要工作是使用当前版本和 <code>VersionEdit</code> 创建一个新的版本对象，然后将 <code>Version</code> 的变更追加到 MANIFEST 日志中，并且改变数据库中全局当前版本信息。</p>

<blockquote>
  <p>MANIFEST 文件中记录了 LevelDB 中所有层级中的表、每一个 SSTable 的 Key 范围和其他重要的元数据，它以日志的格式存储，所有对文件的增删操作都会追加到这个日志中。</p>
</blockquote>

<h4 id="sstable-的格式">SSTable 的格式</h4>

<p>SSTable 中其实存储的不只是数据，其中还保存了一些元数据、索引等信息，用于加速读写操作的速度，虽然在 Bigtable 的论文中并没有给出 SSTable 的数据格式，不过在 LevelDB 的实现中，我们可以发现 SSTable 是以这种格式存储数据的：</p>

<p><img src="https://img.nju520.me/2017-08-12-SSTable-Format.jpg-1000width" alt="SSTable-Format" /></p>

<p>当 LevelDB 读取 SSTable 存在的 <code>ldb</code> 文件时，会先读取文件中的 <code>Footer</code> 信息。</p>

<p><img src="https://img.nju520.me/2017-08-12-SSTable-Footer.jpg-1000width" alt="SSTable-Foote" /></p>

<p>整个 <code>Footer</code> 在文件中占用 48 个字节，我们能在其中拿到 MetaIndex 块和 Index 块的位置，再通过其中的索引继而找到对应值存在的位置。</p>

<p><code>TableBuilder::Rep</code> 结构体中就包含了一个文件需要创建的全部信息，包括数据块、索引块等等：</p>

<pre><code class="language-cpp">struct TableBuilder::Rep {
  WritableFile* file;
  uint64_t offset;
  BlockBuilder data_block;
  BlockBuilder index_block;
  std::string last_key;
  int64_t num_entries;
  bool closed;
  FilterBlockBuilder* filter_block;
  ...
}
</code></pre>

<p>到这里，我们就完成了对整个数据读取过程的解析了；对于读操作，我们可以理解为 LevelDB 在它内部的『多级缓存』中依次查找是否存在对应的键，如果存在就会直接返回，唯一与缓存不同可能就是，在数据『命中』后，它并不会把数据移动到更近的地方，而是会把数据移到更远的地方来减少下一次的访问时间，虽然这么听起来却是不可思议，不过仔细想一下确实是这样。</p>

<h2 id="小结-1">小结</h2>

<p>在这篇文章中，我们通过对 LevelDB 源代码中读写操作的分析，了解了整个框架的绝大部分实现细节，包括 LevelDB 中存储数据的格式、多级 SSTable、如何进行合并以及管理版本等信息，不过由于篇幅所限，对于其中的一些问题并没有展开详细地进行介绍和分析，例如错误恢复以及缓存等问题；但是对 LevelDB 源代码的阅读，加深了我们对 Bigtable 论文中描述的分布式 KV 存储数据库的理解。</p>

<p>LevelDB 的源代码非常易于阅读，也是学习 C++ 语言非常优秀的资源，如果对文章的内容有疑问，可以在博客下面留言。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a></li>
  <li><a href="https://github.com/google/leveldb">LevelDB</a></li>
  <li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf">The Chubby lock service for loosely-coupled distributed systems</a></li>
  <li><a href="https://github.com/google/leveldb/blob/master/doc/impl.md">LevelDB · Impl</a></li>
  <li><a href="http://bean-li.github.io/leveldb-sstable/">leveldb 中的 SSTable</a></li>
</ul>

  ]]></description>
</item>


  </channel>
</rss>
