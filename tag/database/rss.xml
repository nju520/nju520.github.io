<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    
    <title>nju520.me</title>
    
    <link>http://localhost:4000</link>
    <description>nju520's Blog</description>
    <language>en-uk</language>
    <managingEditor> nju520</managingEditor>
    <atom:link href="rss" rel="self" type="application/rss+xml" />
    
<item>
  <title>分布式键值存储 Dynamo 的实现原理</title>
  <link>//dynamo</link>
  <author>nju520</author>
  <pubDate>2017-10-24T00:00:00+08:00</pubDate>
  <guid>//dynamo</guid>
  <description><![CDATA[
  <p>在最近的一周时间里，一直都在研究和阅读 Amazon 的一篇论文 <a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo: Amazon’s Highly Available Key-value Store</a>，论文中描述了 Amazon 的高可用分布式键值存储服务 Dynamo 的实现原理。</p>

<p><img src="https://img.nju520.me/2017-10-24-dynamodb.png" alt="dynamodb" /></p>

<p>之前在阅读 Google 的 <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a> 时写了一篇 <a href="https://hwbnju.com/bigtable-leveldb">浅析 Bigtable 和 LevelDB 的实现</a> 文章分析了 Bigtable 的单机版 LevelDB 的实现原理；在研究 Dynamo 时，作者发现 Dynamo 虽然和 Bigtable 同为 NoSQL，但是它们的实现却有着很大的不同，最主要的原因来自不同的应用场景和不同的目的。</p>

<h2 id="bigtable-和-dynamo">Bigtable 和 Dynamo</h2>

<p>Bigtable 和 Dynamo 两者分别是 Google 和 Amazon 两大巨头给出的存储海量数据的解决方法，作为 NoSQL 两者都具有分布式、容错以及可扩展的几大特性。</p>

<p><img src="https://img.nju520.me/2017-10-24-nosql-main-characteristics.png" alt="nosql-main-characteristics" /></p>

<p>虽然两者都是 NoSQL，并且有着相似的特性，但是它们在侧重的方向上有非常明显的不同，从两个数据库论文的标题中，我们就能看到 Amazon 的 Dynamo 追求的是高可用性并且提供的是类似 MongoDB 的 Key-value 文档存储，而 Bigtable 中描述的数据库却可以用于结构化的数据存储。</p>

<p>由于 Bigtable 和 Dynamo 都属于同一个类别 - NoSQL，所以它们经常会被放在一起进行对比，这篇文章不仅会介绍 Dynamo 的设计理念以及架构等问题，还会就其中的部分问题与 Bigtable 中相对应的概念进行对比，这样能够让我们更加清楚地了解不同的数据库对不同问题，因设计理念的差异做出的权衡。</p>

<h2 id="架构">架构</h2>

<p>在数据库领域中尤其是分布式数据库，最重要的就是服务的架构，多数的分布式系统在设计时都会假设服务运行在廉价的节点上，并没有出众的性能和也不能提供稳定的服务，所以水平扩展和容错的能力是分布式数据库的标配；但是不同的分布式数据库选用了不同的架构来组织大量的节点。</p>

<p>很多的分布式服务例如 GFS 和 Bigtable 都使用了带有主节点的架构来维护整个系统中的元数据，包括节点的位置等信息，而 Dynamo 的实现不同于这些中心化的分布式服务，在 Dynamo 中所有的节点都有着完全相同的职责，会对外界提供同样的服务，所以在整个系统中并不会出现单点故障的问题。</p>

<p><img src="https://img.nju520.me/2017-10-24-dynamo-architecture.png" alt="dynamo-architecture" /></p>

<p>去中心化的架构使得系统的水平扩展非常容易，节点可以在任何时候直接加入到整个 Dynamo 的集群中，并且只会造成集群中少量数据的迁移。</p>

<p>Bigtable 使用了中心化的架构，通过主节点来维护整个系统中全部的元数据信息，但是 Bigtable 本身其实并不会处理来自客户端的读写请求，所有请求都会由客户端直接和从节点通信，不过由于有了中心化的主节点，所以主节点一旦发生故障宕机就会造成服务的不可用，虽然 Bigtable 以及类似的服务通过其他方式解决这个问题，但是这个问题仍然是中心化的设计所造成的。</p>

<p><img src="https://img.nju520.me/2017-10-24-centralized-architecture.png" alt="centralized-architecture" /></p>

<p>中心化或者去中心化并不是一个绝对好或者绝对坏的选择，选择中心化的解决方案能够降低系统实现的复杂度，而去中心化的方式能够避免单点故障，让系统能够更好更快地增加新的节点，提供优秀的水平扩展能力。</p>

<h2 id="分片和复制">分片和复制</h2>

<p>Dynamo 在设计之初就定下了<strong>增量扩展</strong>（Incremental Scalability）的核心需求，这也就需要一种能够在一组节点中动态分片的机制，Dynamo 的分片策略依赖于<em>一致性哈希</em>，通过这种策略 Dynamo 能够将负载合理的分配到不同的存储节点上。</p>

<p>所有的键在存储之前都会通过哈希函数得到一个唯一的值，哈希函数的输出被看做是一个固定长度的环，也就是其输出的最大值和最小值是『连接』到一起的：</p>

<p><img src="https://img.nju520.me/2017-10-24-partition-in-dynamo.png" alt="partition-in-dynamo" /></p>

<p>每一个节点都会被 Dynamo 在这个环中分配一个随机的位置，而这个节点会处理从哈希的输出在当前节点前的所有键；假设我们有一个键值对 <code>(hacker, developer)</code>，<code>Hash(hacker)</code> 的结果位于上图中的绿色区域，从环中的位置开始按照<strong>顺时针</strong>的顺序寻找，找到的以第一个节点 B 就会成为协调者（coordinator）负责处理当前的键值对，上图中的每一个节点都会负责与其颜色相同的部分。</p>

<p>由于 Dynamo 系统中的每一个节点在刚刚加入当前的集群时，会被分配一个随机的位置，所以由于算法的随机性可能会导致不同节点处理的范围有所不同，最终每一个节点的负载也并不相同；为了解决这个问题，Dynamo 使用了一致性哈希算法的变种，将同一个物理节点分配到环中的多个位置（标记），成为多个虚拟节点，但是在这种策略下，如果当前的 Dynamo 节点一天处理上百万的请求，那么新增节点为了不影响已有节点的性能，会在后台进行启动，整个过程大约会<strong>消耗一整天</strong>的时间，这其实是很难接受的，除此之外这种策略还会造成系统进行日常归档极其缓慢。</p>

<p><img src="https://img.nju520.me/2017-10-24-equal-size-partition-in-dynamo.png" alt="equal-size-partition-in-dynamo" /></p>

<p>为了解决负载的不均衡的问题，除了上面使用虚拟节点的策略之外，Dynamo 论文中还提供了另外两种策略，其中性能相对较好的是将数据的哈希分成 Q 个大小相等的区域，S 个节点每一个处理 Q/S 个分区，当某一个节点因为故障或者其他原因需要退出集群时，会将它处理的数据分片随机分配给其它的节点，当有节点加入系统时，会从其它的节点中『接管』对应的数据分片。上图只是对这种策略下的分片情况简单展示，在真实环境中分片数 Q 的值远远大于节点数 S。</p>

<p>Dynamo 为了达到高可用性和持久性，防止由于节点宕机故障或者数据丢失，将同一份数据在协调者和随后的 <code>N-1</code> 个节点上备份了多次，N 是一个可以配置的值，在一般情况下都为 3。</p>

<p><img src="https://img.nju520.me/2017-10-24-replication-in-dynamo.png" alt="replication-in-dynamo" /></p>

<p>也就是说，上图中黄色区域的值会存储在三个节点 A、B 和 C 中，绿色的区域会被 B、C、D 三个节点处理，从另一个角度来看，A 节点会处理范围在 <code>(C, A]</code> 之间的值，而 B 节点会处理从 <code>(D, B]</code> 区域内的值。</p>

<p><img src="https://img.nju520.me/2017-10-24-replication-range-in-dynamo.png" alt="replication-range-in-dynamo" /></p>

<p>负责存储某一个特定键值对的节点列表叫做偏好列表（preference list），因为虚拟节点在环中会随机存在，为了保证出现节点故障时不会影响可用性和持久性，偏好列表中的全部节点必须都为<strong>不同的物理节点</strong>。</p>

<p>Bigtable 中对分片和复制的实现其实就与 Dynamo 中完全不同，这不仅是因为 Bigtable 的节点有主从之分，还因为 Bigtable 的设计理念与 Dynamo 完全不同。在 Bigtable 中，数据是按照键的顺序存储的，数据存储的单位都是 tablet，每一张表都由多个 tablet 组成，而每一个的 tablet 都有一个 tablet 服务器来处理，而 tablet 的位置都存储在 METADATA 表中。</p>

<p><img src="https://img.nju520.me/2017-10-24-partition-in-bigtable.png" alt="partition-in-bigtable" /></p>

<p>在 Bigtable 中，所有的 tablet 都在 GFS 中以 SSTable 的格式存储起来，这些 SSTable 都被分成了固定大小的块在 chunkserver 上存储，而每一个块也都会在存储在多个 chunkserver 中。</p>

<h2 id="读写请求的执行">读写请求的执行</h2>

<p>Dynamo 集群中的任意节点都能够接受来自客户端的对于任意键的读写请求，所有的请求都通过 RPC 调用执行，客户端在选择节点时有两种不同的策略：一种是通过一个负载均衡器根据负载选择不同的节点，另一种是通过一个清楚当前集群分片的库直接请求相应的节点。</p>

<p><img src="https://img.nju520.me/2017-10-24-node-selecting-strategies.png" alt="node-selecting-strategies" /></p>

<p>从上面我们就已经知道了处理读写请求的节点就叫做协调者（coordinator），前 N 个『健康』的节点会参与读写请求的处理；Dynamo 使用了 Quorum 一致性协议来保证系统中的一致性，协议中有两个可以配置的值：R 和 W，其中 R 是成功参与一个读请求的最小节点数，而 W 是成功参与写请求的最小节点数。</p>

<p><img src="https://img.nju520.me/2017-10-24-dynamo-read-write-operation.png" alt="dynamo-read-write-operation" /></p>

<p>当 R = 2 时，所有的读请求必须等待两个节点成功返回对应键的结果，才认为当前的请求结束了，也就是说读请求的时间取决于返回最慢的节点，对于写请求来说也是完全相同的；当协调者接收到了来自客户端的写请求 <code>put()</code> 时，它会创建一个新的向量时钟（vector clock），然后将新版本的信息存储在本地，之后向偏好列表（preference list）中的前 <code>N-1</code> 个节点发送消息，直到其中的 <code>W-1</code> 个返回这次请求才成功结束，读请求 <code>get()</code> 与上述请求的唯一区别就是，如果协调者发现节点中的数据出现了冲突，就会对冲突尝试进行解决并将结果重新写回对应的节点。</p>

<h2 id="冲突和向量时钟">冲突和向量时钟</h2>

<p>Dynamo 与目前的绝大多数分布式系统一样都提供了<strong>最终一致性</strong>，最终一致性能够允许我们异步的更新集群中的节点，<code>put()</code> 请求可能会在所有的节点后更新前就返回对应的结果了，在这时随后的 <code>get()</code> 就可能获取到过期的数据。</p>

<p><img src="https://img.nju520.me/2017-10-24-inconsistent-in-dynamo.png" alt="inconsistent-in-dynamo" /></p>

<p>如果在系统中出现了节点故障宕机，那么数据的更新可能在一段时间内都不会到达失效的节点，这也是在使用 Dynamo 或者使用相似原理的系统时会遇到的问题，Amazon 中的很多应用虽然都能够忍受这种数据层面可能发生的不一致性，但是有些对业务数据一致性非常高的应用在选择 Dynamo 时就需要好好考虑了。</p>

<p>因为 Dynamo 在工作的过程中不同的节点可能会发生数据不一致的问题，这种问题肯定是需要解决的，Dynamo 能够确保<strong>一旦数据之间发生了冲突不会丢失</strong>，但是可能会有<strong>已被删除的数据重新出现</strong>的问题。</p>

<p>在多数情况下，Dynamo 中的最新版本的数据都会取代之前的版本，系统在这时可以通过语法调解（syntactic reconcile）数据库中的正确版本。但是版本也可能会出现分支，在这时，Dynamo 就会返回所有它无法处理的数据版本，由客户端在多个版本的数据中选择或者创建（collapse）合适的版本返回给 Dynamo，其实这个过程比较像出现冲突的 <code>git merge</code> 操作，git 没有办法判断当前的哪个版本是合适的，所以只能由开发者对分支之间的冲突进行处理。</p>

<p><img src="https://img.nju520.me/2017-10-24-version-evolution-in-dynamo.png" alt="version-evolution-in-dynamo" /></p>

<p>上图中的每一个对象的版本 Dx 中存储着一个或多个向量时钟 <code>[Sn, N]</code>，每次 Dynamo 对数据进行写入时都会更新向量时钟的版本，节点 Sx 第一次写入时向量时钟为 <code>[Sx, 1]</code>，第二次为 <code>[Sx, 2]</code>，在这时假设节点 Sy 和 Sz 都不知道 Sx 已经对节点进行写入了，它们接收到了来自其他客户端的请求，在本地也对同样键做出了写入并分别生成了不同的时钟 <code>[Sy, 1]</code> 和 <code>[Sz, 1]</code>，当客户端再次使用 <code>get()</code> 请求时就会发现数据出现了冲突，由于 Dynamo 无法根据向量时钟自动解决，所以它需要手动合并三个不同的数据版本。</p>

<p>论文中对 24 小时内的请求进行了统计，其中 99.94% 的请求仅会返回一个版本，0.00057% 的请求会返回两个版本，0.00047 的请求会返回三个版本，0.000009% 的请求会返回四个版本，虽然论文中说：</p>

<blockquote>
  <p>This shows that divergent versions are created rarely.</p>
</blockquote>

<p>但是作者仍然认为在海量的数据面前 99.94% 并不是一个特别高的百分比，处理分歧的数据版本仍然会带来额外的工作量和负担。虽然在这种情况下，数据库本身确实没有足够的信息来解决数据的不一致问题，也确实只能由客户端去解决冲突，但是这种将问题抛给上层去解决的方式并不友好，论文中也提到了 Amazon 中使用 Dynamo 的应用程序也都是能够适应并解决这些数据不一致的问题的，不过对于作者来说，仅仅这一个问题就成为不选择 Dynamo 的理由了。</p>

<h2 id="节点的增删">节点的增删</h2>

<p>因为在分布式系统中节点的失效是非常常见的事情，而节点也很少会因为某些原因永久失效，往往大部分节点会临时宕机然后快速重新加入系统；由于这些原因，Dynamo 选择使用了显式的机制向系统中添加和移除节点。</p>

<p><img src="https://img.nju520.me/2017-10-24-ring-membership.png" alt="ring-membership" /></p>

<p>添加节点时可以使用命令行工具或者浏览器连接 Dynamo 中的任意节点后触发一个成员变动的事件，这个事件会从当前的环中移除或者向环中添加一个新的节点，当节点的信息发生改变时，该节点会通过 Gossip 协议通知它所能通知的最多的节点。</p>

<p><img src="https://img.nju520.me/2017-10-24-gossip-protocol.png" alt="gossip-protoco" /></p>

<p>在 Gossip 协议中，每次通讯的两个节点会对当前系统中的节点信息达成一致；通过节点之间互相传递成员信息，最终整个 Dyanmo 的集群中所有的节点都会就成员信息达成一致，如上图所示，”gossip” 首先会被 C 节点接收，然后它会传递给它能接触到的最多的节点 A、D、F、G 四个节点，然后 “gossip” 会进行二次传播传递给系统中的灰色节点，到此为止系统中的所有节点都得到了最新的 “gossip” 消息。</p>

<p>当我们向 Dynamo 中加入了新的节点时，会发生节点之间的分片转移，假设我们连接上了 Dynamo 数据库，然后添加了一个 X 节点，该节点被分配到了如下图所示的 A 和 B 节点之间。</p>

<p><img src="https://img.nju520.me/2017-10-24-adding-storage-node.png" alt="adding-storage-node" /></p>

<p>新引入的节点 X 会从三个节点 C、D、E 中接受它们管理的分片的一部分，也就是上图中彩色的 <code>(E, A]</code>、<code>(A, B]</code> 和 <code>(B, X]</code> 三个部分，在 X 节点加入集群之前分别属于与其颜色相同的节点管理。</p>

<p>Dynamo 由于其去中心化的架构，节点增删的事件都需要通过 Gossip 协议进行传递，然而拥有主从节点之分的 Bigtable 就不需要上述的方式对集群中的节点进行增删了，它可以直接通过用于管理其他从节点的服务直接注册新的节点或者撤下已有的节点。</p>

<h2 id="副本同步">副本同步</h2>

<p>在 Dynamo 运行的过程中，由于一些情况会造成不同节点中的数据不一致的问题，Dynamo 使用了反信息熵（anti-entropy）的策略保证所有的副本存储的信息都是同步的。</p>

<p>为了快速确认多个副本之间的数据的一致性并避免大量的数据传输，Dynamo 使用了 <a href="https://en.wikipedia.org/wiki/Merkle_tree">Merkle tree</a> 对不同节点中的数据进行快速验证。</p>

<p><img src="https://img.nju520.me/2017-10-24-merkle-hash-tree.png" alt="merkle-hash-tree" /></p>

<p>在 Merkle 树中，所有父节点中的内容都是叶子节点的哈希，通过这种方式构建的树形结构能够保证整棵树不会被篡改，任何的改动都能被立刻发现。</p>

<p>Dynamo 中的每一个节点都为其持有的键的范围维护了一颗 Merkle 树，在验证两份节点中的数据是否相同时，只需要发送根节点中的哈希值，如果相同那么说明两棵树的内容全部相同，否则就会依次对比不同层级节点中的内容，直到找出不同的副本，这种做法虽然能够减少数据的传输并能够快速找到副本之间的不同，但是当有新的节点加入或者旧的节点退出时会导致大量的 Merkle 树重新计算。</p>

<h2 id="总结">总结</h2>

<p>在 Dynamo 的论文公开之后，有一篇文章将 Dynamo 的设计称作 <a href="http://jsensarma.com/blog/?p=55">“A flawed architecture”</a>，这篇文章的作者在文中对 Dynamo 的实现进行了分析，主要对其最终一致性和 Quorom 机制进行了批评，它在 <a href="https://news.ycombinator.com/item?id=915212">HackerNews</a> 上也引起了广泛的讨论，帖子中的很多内容都值得一看，能够帮助我们了解 Dynamo 的设计原理，而 Amazon 的 CTO 对于这篇文章也发了一条 Twitter：</p>

<p><img src="https://img.nju520.me/2017-10-24-amazon-cto-twitter-about-dynamo.png" alt="amazon-cto-twitter-about-dynamo" /></p>

<p>不管如何，Dynamo 作为支撑亚马逊业务的底层服务，其实现原理和思想对于整个社区都是非常有价值的，然而它使用的去中心化的策略也带了很多问题，虽然作者可能会因为这个原因在选择数据库时不会 Dynamo，不过相信它也是有合适的应用场景的。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo: Amazon’s Highly Available Key-value Store</a></li>
  <li><a href="http://jsensarma.com/blog/?p=55">Dynamo: A flawed architecture – Part I</a></li>
  <li><a href="http://jsensarma.com/blog/?p=64">Dynamo – Part I: a followup and re-rebuttals</a></li>
  <li><a href="https://www.slideshare.net/GrishaWeintraub/presentation-46722530">Dynamo and BigTable - Review and Comparison</a></li>
  <li><a href="http://vschart.com/compare/dynamo-db/vs/bigtable">DynamoDB vs. BigTable · vsChart</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Merkle_tree">Merkle tree</a></li>
  <li><a href="https://link.springer.com/content/pdf/10.1007/3-540-48184-2_32.pdf">A Digital Signature Based on a Conventional Encryption Function</a></li>
  <li><a href="http://www.raychase.net/2396">Dynamo 的实现技术和去中心化</a></li>
  <li><a href="https://hwbnju.com/bigtable-leveldb">浅析 Bigtable 和 LevelDB 的实现</a></li>
</ul>

  ]]></description>
</item>

<item>
  <title>全面理解 ActiveRecord</title>
  <link>//activerecord</link>
  <author>nju520</author>
  <pubDate>2017-10-21T00:00:00+08:00</pubDate>
  <guid>//activerecord</guid>
  <description><![CDATA[
  <p>最近事情并不是特别多，看了一些数据库相关的书籍，最后想到自己并不了解每天都在用的 ActiveRecord，对于它是如何创建模型、建立关系、执行 SQL 查询以及完成数据库迁移的，作者一直都有着自己的猜测，但是真正到源代码中去寻找答案一直都是没有做过的。</p>

<p><img src="https://img.nju520.me/2017-10-21-activerecord-architecture.png" alt="activerecord-architecture" /></p>

<p>我们可以将 ActiveRecord 理解为一个不同 SQL 数据库的 Wrapper，同时为上层提供一种简洁、优雅的 API 或者说 DSL，能够极大得减轻开发者的负担并提升工作效率。</p>

<p>文章分四个部分介绍了 ActiveRecord 中的重要内容，模型的创建过程、Scope 和查询的实现、模型关系的实现以及最后的 Migrations 任务的实现和执行过程，各个模块之间没有太多的关联，由于文章内容比较多，如果读者只对某一部分的内容感兴趣，可以只挑选一部分进行阅读。</p>

<h2 id="模型的创建过程">模型的创建过程</h2>

<p>在这篇文章中，我们会先分析阅读 ActiveRecord 是如何创建模型并将数据插入到数据库中的，由于 ActiveRecord 的源码变更非常迅速，这里使用的 ActiveRecord 版本是 v5.1.4，如果希望重现文中对方法的追踪过程可以 checkout 到 v5.1.4 的标签上并使用如下所示的命令安装指定版本的 ActiveRecord：</p>

<pre><code class="language-shell">$ gem install activerecord -v '5.1.4'
</code></pre>

<h3 id="引入-activerecord">引入 ActiveRecord</h3>

<p>在正式开始使用 <a href="https://github.com/pry/pry">pry</a> 对方法进行追踪之前，我们需要现在 pry 中 <code>require</code> 对应的 gem，并且创建一个用于追踪的模型类：</p>

<pre><code class="language-ruby">pry(main)&gt; require 'active_record'
=&gt; true
pry(main)&gt; class Post &lt; ActiveRecord::Base; end
=&gt; nil
</code></pre>

<p>这个步骤非常的简单，这里也不多说什么了，只是创建了一个继承自 <code>ActiveRecord::Base</code> 的类 <code>Post</code>，虽然我们并没有在数据库中创建对应的表结构，不过目前来说已经够用了。</p>

<h3 id="从-postcreate-开始">从 Post.create 开始</h3>

<p>使用过 ActiveRecord 的人都知道，当我们使用 <code>Post.create</code> 方法的时候就会在数据库中创建一条数据记录，所以在这里我们就将该方法作为入口一探究竟：</p>

<pre><code class="language-ruby">pry(main)&gt; $ Post.create

From: lib/active_record/persistence.rb @ line 29:
Owner: ActiveRecord::Persistence::ClassMethods

def create(attributes = nil, &amp;block)
  if attributes.is_a?(Array)
    attributes.collect { |attr| create(attr, &amp;block) }
  else
    object = new(attributes, &amp;block)
    object.save
    object
  end
end
</code></pre>

<blockquote>
  <p><code>$</code> 是 pry 为我们提供的用于查看方法源代码的工具，这篇文章中会省略 <code>$</code> 方法的一部分输出，还可能会对方法做一些简化减少理解方法实现时的干扰。</p>
</blockquote>

<p>通过 pry 的输出，我们可以在 ActiveRecord 的 <code>lib/active_record/persistence.rb</code> 文件中找到 <code>ActiveRecord::Base.create</code> 方法的实现，如果传入的参数是一个 <code>Hash</code>，该方法会先后执行 <code>ActiveRecord::Base.new</code> 和 <code>ActiveRecord::Base#save!</code> 创建一个新的对象并保存。</p>

<h4 id="使用-pry-追踪-save">使用 pry 追踪 #save!</h4>

<p><code>ActiveRecord::Base.new</code> 在大多数情况下都会调用父类的 <code>#initialize</code> 方法初始化实例，所以没有什么好说的，而 <code>ActiveRecord::Base#save!</code> 方法就做了很多事情：</p>

<pre><code class="language-ruby">pry(main)&gt; $ ActiveRecord::Base#save!

From: lib/active_record/suppressor.rb @ line 45:
Owner: ActiveRecord::Suppressor

def save!(*) # :nodoc:
  SuppressorRegistry.suppressed[self.class.name] ? true : super
end
</code></pre>

<p>首先是使用 <code>SuppressorRegistry</code> 来判断是否需要对当前的存取请求进行抑制，然后执行 <code>super</code> 方法，由于从上述代码中没有办法知道这里的 <code>super</code> 到底是什么，所以我们就需要通过 <code>.ancestors</code> 方法看看 <code>ActiveRecord::Base</code> 到底有哪些父类了：</p>

<pre><code class="language-ruby">pry(main)&gt; ActiveRecord::Base.ancestors
=&gt; [ActiveRecord::Base,
 ActiveRecord::Suppressor,
 ...
 ActiveRecord::Persistence,
 ActiveRecord::Core,
 ActiveSupport::ToJsonWithActiveSupportEncoder,
 Object,
 ...
 Kernel,
 BasicObject]

pry(main)&gt; ActiveRecord::Base.ancestors.count
=&gt; 65
</code></pre>

<p>使用 <code>.ancestors</code> 方法，你就可以看到整个方法调用链上包含 64 个父类，在这时简单的使用 pry 就已经不能帮助我们理解方法的调用过程了，因为 pry 没法查看当前的方法在父类中是否存在，我们需要从工程中分析哪些类的 <code>#save!</code> 方法在整个过程中被执行了并根据上述列表排出它们执行的顺序；经过分析，我们得到如下的结果：</p>

<p><img src="https://img.nju520.me/2017-10-21-activerecord-base-save.png" alt="activerecord-base-save" /></p>

<p>从 <code>ActiveRecord::Suppressor</code> 到 <code>ActiveRecord::Persistence</code> 一共有五个 module 实现了 <code>#save!</code> 方法，上面我们已经知道了 <code>ActiveRecord::Suppressor#save!</code> 模块提供了对保存的抑制功能，接下来将依次看后四个方法都在保存模型的过程中做了什么。</p>

<h4 id="事务的执行">事务的执行</h4>

<p>从名字就可以看出 <code>ActiveRecord::Transactions</code> 主要是为数据库事务提供支持，并在数据库事务的不同阶段执行不同的回调，这个 module 中的 <code>#save!</code> 方法仅在 <code>#with_transaction_returning_status</code> 的 block 中执行了 <code>super</code>：</p>

<pre><code class="language-ruby">module ActiveRecord
  module Transactions
    def save!(*) #:nodoc:
      with_transaction_returning_status { super }
    end
  end
end
</code></pre>

<p><code>#with_transaction_returning_status</code> 方法会运行外部传入的 block 通过 <code>super</code> 执行父类的 <code>#save!</code> 方法：</p>

<pre><code class="language-ruby">def with_transaction_returning_status
  status = nil
  self.class.transaction do
    add_to_transaction
    begin
      status = yield
    rescue ActiveRecord::Rollback
      clear_transaction_record_state
      status = nil
    end

    raise ActiveRecord::Rollback unless status
  end
  status
ensure
  if @transaction_state &amp;&amp; @transaction_state.committed?
    clear_transaction_record_state
  end
end
</code></pre>

<p>通过上述方法，我们将所有的 SQL 请求都包装在了一个 <code>.transaction</code> 中，开启一个新的数据库事务并在其中执行请求，在这里统一处理一些跟事务回滚以及异常相关的逻辑，同时 <code>ActiveRecord::Transactions</code> 又能为当前的模型添加一些回调的支持：</p>

<pre><code class="language-ruby">module ActiveRecord
  module Transactions
    included do
      define_callbacks :commit, :rollback,
                       :before_commit,
                       :before_commit_without_transaction_enrollment,
                       :commit_without_transaction_enrollment,
                       :rollback_without_transaction_enrollment,
                       scope: [:kind, :name]
    end
  end
end
</code></pre>

<p>开发者就能够在模型中根据需要注册回调用来监听各种数据库事务相关的事件，绝大多数的事务最终都会在 <code>ActiveRecord::ConnectionAdapters::Transaction#within_new_transaction</code> 方法中执行：</p>

<pre><code class="language-ruby">def within_new_transaction(options = {})
  @connection.lock.synchronize do
    begin
      transaction = begin_transaction options
      yield
    rescue Exception =&gt; error
      if transaction
        rollback_transaction
        after_failure_actions(transaction, error)
      end
      raise
    ensure
      unless error
        if Thread.current.status == "aborting"
          rollback_transaction if transaction
        else
          begin
            commit_transaction
          rescue Exception
            rollback_transaction(transaction) unless transaction.state.completed?
            raise
          end
        end
      end
    end
  end
end
</code></pre>

<p>上述方法虽然看起来非常复杂，但是方法的逻辑还是还是非常清晰的，如果事务没有抛出任何的异常，就可以将上述代码简化成以下的几行代码：</p>

<pre><code class="language-ruby">def within_new_transaction(options = {})
  @connection.lock.synchronize do
      begin_transaction options
      yield
      commit_transaction
    end
  end
end
</code></pre>

<p>我们可以看到，经过一系列的方法调用最后会在数据库中执行 <code>BEGIN</code>、SQL 语句和 <code>COMMIT</code> 来完成数据的持久化。</p>

<h4 id="追踪属性的重置">追踪属性的重置</h4>

<p>当 <code>ActiveRecord::Transactions#save!</code> 通过 <code>super</code> 将方法抛给上层之后，就由 <code>ActiveRecord::AttributesMethod::Dirty</code> 来处理了：</p>

<pre><code class="language-ruby">def save!(*)
  super.tap do
    changes_applied
  end
end
</code></pre>

<p>如果 <code>#save!</code> 最终执行成功，在这个阶段会将所有模型改变的标记全部清除，对包括 <code>@changed_attributes</code>、<code>@mutation_tracker</code> 在内的实例变量全部重置，为追踪下一次模型的修改做准备。</p>

<h4 id="字段的验证">字段的验证</h4>

<p>沿着整个继承链往下走，下一个被执行的模块就是 <code>ActiveRecord::Validations</code> 了，正如这么模块名字所暗示的，我们在这里会对模型中的字段进行验证：</p>

<pre><code class="language-ruby">def save!(options = {})
  perform_validations(options) ? super : raise_validation_error
end
</code></pre>

<p>上述代码使用 <code>#perform_validations</code> 方法验证模型中的全部字段，以此来保证所有的字段都符合我们的预期：</p>

<pre><code class="language-ruby">def perform_validations(options = {})
  options[:validate] == false || valid?(options[:context])
end
</code></pre>

<p>在这个方法中我们可以看到如果在调用 <code>save!</code> 方法时，传入了 <code>validate: false</code> 所有的验证就都会被跳过，我们通过 <code>#valid?</code> 来判断当前的模型是否合法，而这个方法的执行过程其实也包含两个过程：</p>

<pre><code class="language-ruby">module ActiveRecord
  module Validations
    def valid?(context = nil)
      context ||= default_validation_context
      output = super(context)
      errors.empty? &amp;&amp; output
    end
  end
end

module ActiveModel
  module Validations
    def valid?(context = nil)
      current_context, self.validation_context = validation_context, context
      errors.clear
      run_validations!
    ensure
      self.validation_context = current_context
    end
  end
end
</code></pre>

<p>由于 <code>ActiveModel::Validations</code> 是 <code>ActiveRecord::Validations</code> 的『父类』，所以在 <code>ActiveRecord::Validations</code> 执行 <code>#valid?</code> 方法时，最终会执行父类 <code>#run_validations</code> 运行全部的验证回调。</p>

<pre><code class="language-ruby">module ActiveModel
  module Validations
    def run_validations!
      _run_validate_callbacks
      errors.empty?
    end
  end
end
</code></pre>

<p>通过上述方法的实现，我们可以发现验证是否成功其实并不是通过我们在 <code>validate</code> 中传入一个返回 <code>true/false</code> 的方法决定的，而是要向当前模型的 <code>errors</code> 中添加更多的错误：</p>

<pre><code class="language-ruby">class Invoice &lt; ApplicationRecord
  validate :active_customer
 
  def active_customer
    errors.add(:customer_id, "is not active") unless customer.active?
  end
end
</code></pre>

<p>在这个过程中执行的另一个方法 <code>#_run_validate_callbacks</code> 其实是通过 <code>ActiveSupport::Callbacks</code> 提供的 <code>#define_callbacks</code> 方法动态生成的，所以我们没有办法在工程中搜索到：</p>

<pre><code class="language-ruby">def define_callbacks(*names)
  options = names.extract_options!

  names.each do |name|
    name = name.to_sym
    set_callbacks name, CallbackChain.new(name, options)
    module_eval &lt;&lt;-RUBY, __FILE__, __LINE__ + 1
      def _run_#{name}_callbacks(&amp;block)
        run_callbacks #{name.inspect}, &amp;block
      end

      def self._#{name}_callbacks
        get_callbacks(#{name.inspect})
      end

      def self._#{name}_callbacks=(value)
        set_callbacks(#{name.inspect}, value)
      end

      def _#{name}_callbacks
        __callbacks[#{name.inspect}]
      end
    RUBY
  end
end
</code></pre>

<p>在这篇文章中，我们只需要知道该 <code>#save!</code> 在合适的时机运行了正确的回调就可以了，在后面的文章（可能）中会详细介绍整个 callbacks 的具体执行流程。</p>

<h4 id="数据的持久化">数据的持久化</h4>

<p><code>#save!</code> 的调用栈最顶端就是 <code>ActiveRecord::Persistence#save!</code> 方法：</p>

<pre><code class="language-ruby">def save!(*args, &amp;block)
  create_or_update(*args, &amp;block) || raise(RecordNotSaved.new("Failed to save the record", self))
end

def create_or_update(*args, &amp;block)
  _raise_readonly_record_error if readonly?
  result = new_record? ? _create_record(&amp;block) : _update_record(*args, &amp;block)
  result != false
end
</code></pre>

<p>在这个方法中，我们执行了 <code>#create_or_update</code> 以及 <code>#_create_record</code> 两个方法来创建模型：</p>

<pre><code class="language-ruby">def _create_record(attribute_names = self.attribute_names)
  attributes_values = arel_attributes_with_values_for_create(attribute_names)
  new_id = self.class.unscoped.insert attributes_values
  self.id ||= new_id if self.class.primary_key
  @new_record = false
  yield(self) if block_given?
  id
end
</code></pre>

<p>在这个私有方法中开始执行数据的插入操作了，首先是通过 <code>ActiveRecord::AttributeMethods#arel_attributes_with_values_for_create</code> 方法获取一个用于插入数据的字典，其中包括了数据库中的表字段和对应的待插入值。</p>

<p><img src="https://img.nju520.me/2017-10-21-database-statement-insert.png" alt="database-statement-insert" /></p>

<p>而下面的 <code>.insert</code> 方法就会将这个字典转换成 SQL 语句，经过上图所示的调用栈最终到不同的数据库中执行语句并返回最新的主键。</p>

<h3 id="小结">小结</h3>

<p>从整个模型的创建过程中，我们可以看到 ActiveRecord 对于不同功能的组织非常优雅，每一个方法都非常简短并且易于阅读，通过对应的方法名和模块名我们就能够明确的知道这个东西是干什么的，对于同一个方法的不同执行逻辑也分散了不同的模块中，最终使用 module 加上 include 的方式组织起来，如果要对某个方法添加一些新的逻辑也可以通过增加更多的 module 达到目的。</p>

<p>通过对源代码的阅读，我们可以看到对于 ActiveRecord 来说，<code>#create</code> 和 <code>#save!</code> 方法的执行路径其实是差不多的，只是在细节上有一些不同之处。</p>

<p><img src="https://img.nju520.me/2017-10-21-actual-callstack-for-activerecord-base-save.png" alt="actual-callstack-for-activerecord-base-save" /></p>

<p>虽然模型或者说数据行的创建过程最终会从子类一路执行到父类的 <code>#save!</code> 方法，但是逻辑的<strong>处理顺序</strong>并不是按照从子类到父类执行的，我们可以通过上图了解不同模块的真正执行过程。</p>

<h2 id="scope-和查询的实现">Scope 和查询的实现</h2>

<p>除了模型的插入、创建和迁移模块，ActiveRecord 中还有另一个非常重要的模块，也就是 Scope 和查询；为什么同时介绍这两个看起来毫不相干的内容呢？这是因为 Scope 和查询是完全分不开的一个整体，在 ActiveRecord 的实现中，两者有着非常紧密的联系。</p>

<h3 id="activerecordrelation">ActiveRecord::Relation</h3>

<p>对 ActiveRecord 稍有了解的人都知道，在使用 ActiveRecord 进行查询时，所有的查询方法其实都会返回一个 <code>#{Model}::ActiveRecord_Relation</code> 类的对象，比如 <code>User.all</code>：</p>

<pre><code class="language-ruby">pry(main)&gt; User.all.class
=&gt; User::ActiveRecord_Relation
</code></pre>

<p>在这里使用 pry 来可以帮助我们快速理解整个过程到底都发生了什么事情：</p>

<pre><code class="language-ruby">pry(main)&gt; $ User.all

From: lib/active_record/scoping/named.rb @ line 24:
Owner: ActiveRecord::Scoping::Named::ClassMethods

def all
  if current_scope
    current_scope.clone
  else
    default_scoped
  end
end
</code></pre>

<p><code>#all</code> 方法中的注释中也写着它会返回一个 <code>ActiveRecord::Relation</code> 对象，它其实可以理解为 ActiveRecord 查询体系中的单位元，它的调用并不改变当前查询；而如果我们使用 pry 去看其他的方法例如 <code>User.where</code> 的时候：</p>

<pre><code class="language-ruby">pry(main)&gt; $ User.where

From: lib/active_record/querying.rb @ line 10:
Owner: ActiveRecord::Querying

delegate :select, :group, :order, :except, :reorder, :limit, :offset, :joins, :left_joins, :left_outer_joins, :or,
         :where, :rewhere, :preload, :eager_load, :includes, :from, :lock, :readonly, :extending,
         :having, :create_with, :distinct, :references, :none, :unscope, :merge, to: :all
</code></pre>

<p>从这里我们可以看出，真正实现为 <code>User</code> 类方法的只有 <code>.all</code>，其他的方法都会代理给 <code>all</code> 方法，在 <code>.all</code> 方法返回的对象上执行：</p>

<p><img src="https://img.nju520.me/2017-10-21-active-record-relation-delegation.png" alt="active-record-relation-delegation" /></p>

<p>所有直接在类上调用的方法都会先执行 <code>#all</code>，也就是说下面的几种写法是完全相同的：</p>

<pre><code class="language-ruby">User    .where(name: 'hacker')
User.all.where(name: 'hacker')
User.all.where(name: 'hacker').all
</code></pre>

<p>当我们了解了 <code>.where == .all + #where</code> 就可以再一次使用 pry 来查找真正被 ActiveRecord 实现的 <code>#where</code> 方法：</p>

<pre><code class="language-ruby">pry(main)&gt; $ User.all.where

From: lib/active_record/relation/query_methods.rb @ line 599:
Owner: ActiveRecord::QueryMethods

def where(opts = :chain, *rest)
  if :chain == opts
    WhereChain.new(spawn)
  elsif opts.blank?
    self
  else
    spawn.where!(opts, *rest)
  end
end
</code></pre>

<p>在分析查询的过程中，我们会选择几个常见的方法作为入口，尽可能得覆盖较多的查询相关的代码，增加我们对 ActiveRecord 的理解和认识。</p>

<h3 id="从-userall-开始">从 User.all 开始</h3>

<p>再来看一下上面看到的 <code>ActiveRecord::Relation.all</code> 方法，无论是 <code>#current_scope</code> 还是 <code>#default_scoped</code> 其实返回的都是当前的 <code>ActiveRecord</code> 对象：</p>

<pre><code class="language-ruby">def all
  if current_scope
    current_scope.clone
  else
    default_scoped
  end
end
</code></pre>

<h4 id="current_scope-和-default_scope">current_scope 和 default_scope</h4>

<p>如果当前没有 <code>#current_scope</code> 那么，就会调用 <code>#default_scoped</code> 返回响应的结果，否则就会 clone 当前对象并返回，可以简单举一个例子证明这里的猜测：</p>

<pre><code class="language-ruby">pry(main)&gt; User.current_scope
=&gt; nil
pry(main)&gt; User.all.current_scope
  User Load (0.1ms)  SELECT "users".* FROM "users"
=&gt; []
pry(main)&gt; User.all.current_scope.class
=&gt; User::ActiveRecord_Relation
</code></pre>

<p><code>.current_scope</code> 是存储在位于线程变量的 <code>ScopeRegistry</code> 中，它其实就是当前的查询语句的上下文，存储着这一次链式调用造成的全部副作用：</p>

<pre><code class="language-ruby">def current_scope(skip_inherited_scope = false)
  ScopeRegistry.value_for(:current_scope, self, skip_inherited_scope)
end
</code></pre>

<p>而 <code>.default_scoped</code> 就是在当前查询链刚开始时执行的第一个方法，因为在执行第一个查询方法之前 <code>.current_scope</code> 一定为空：</p>

<pre><code class="language-ruby">def default_scoped(scope = relation)
  build_default_scope(scope) || scope
end

def build_default_scope(base_rel = nil)
  return if abstract_class?

  if default_scopes.any?
    base_rel ||= relation
    evaluate_default_scope do
      default_scopes.inject(base_rel) do |default_scope, scope|
        scope = scope.respond_to?(:to_proc) ? scope : scope.method(:call)
        default_scope.merge(base_rel.instance_exec(&amp;scope))
      end
    end
  end
end
</code></pre>

<p>当我们在 Rails 的模型层中使用 <code>.default_scope</code> 定义一些默认的上下文时，所有的 block 都换被转换成 <code>Proc</code> 对象最终添加到 <code>default_scopes</code> 数组中：</p>

<pre><code class="language-ruby">def default_scope(scope = nil) # :doc:
  scope = Proc.new if block_given?
  # ...
  self.default_scopes += [scope]
end
</code></pre>

<p>上面提到的 <code>.build_default_scope</code> 方法其实只是在 <code>default_scopes</code> 数组不为空时，将当前的 <code>Relation</code> 对象和数组中的全部 scope 一一 <code>#merge</code> 并返回一个新的 <code>Relation</code> 对象。</p>

<h4 id="activerecordrelation-对象">ActiveRecord::Relation 对象</h4>

<p><code>.default_scoped</code> 方法的参数 <code>scope</code> 其实就有一个默认值 <code>#relation</code>，这个默认值其实就是一个 <code>Relation</code> 类的实例：</p>

<pre><code class="language-ruby">def relation
  relation = Relation.create(self, arel_table, predicate_builder)

  if finder_needs_type_condition? &amp;&amp; !ignore_default_scope?
    relation.where(type_condition).create_with(inheritance_column.to_s =&gt; sti_name)
  else
    relation
  end
end
</code></pre>

<p><code>Relation.create</code> 对象的创建过程其实比较复杂，我们只需要知道经过 ActiveRecord 一系列的疯狂操作，最终会将几个参数传入 <code>.new</code> 方法初始化一个 <code>ActiveRecord::Relation</code> 实例：</p>

<pre><code class="language-ruby">class Relation
  def initialize(klass, table, predicate_builder, values = {})
    @klass  = klass
    @table  = table
    @values = values
    @offsets = {}
    @loaded = false
    @predicate_builder = predicate_builder
  end
end
</code></pre>

<p>当执行的是 <code>#all</code>、<code>.all</code> 或者绝大多数查询方法时，都会直接将这个初始化的对象返回来接受随后的链式调用。</p>

<h3 id="where-方法">where 方法</h3>

<p>相比于 <code>#all</code>、<code>#where</code> 查询的实现就复杂多了，不像 <code>#all</code> 会返回一个默认的 <code>Relation</code> 对象，<code>#where</code> 由 <code>WhereClause</code> 以及 <code>WhereClauseFactory</code> 等类共同处理；在 <code>#where</code> 的最正常的执行路径中，它会执行 <code>#where!</code> 方法：</p>

<pre><code class="language-ruby">def where(opts = :chain, *rest)
  if :chain == opts
    WhereChain.new(spawn)
  elsif opts.blank?
    self
  else
    spawn.where!(opts, *rest)
  end
end

def where!(opts, *rest)
  opts = sanitize_forbidden_attributes(opts)
  references!(PredicateBuilder.references(opts)) if Hash === opts
  self.where_clause += where_clause_factory.build(opts, rest)
  self
end
</code></pre>

<blockquote>
  <p><code>#spawn</code> 其实就是对当前的 <code>Relation</code> 对象进行 <code>#clone</code>。</p>
</blockquote>

<p>查询方法 <code>#where!</code> 中的四行代码只有一行代码是我们需要关注的，该方法调用 <code>WhereClauseFactory#build</code> 生成一条 where 查询并存储到当前对象的 <code>where_clause</code> 中，在这个过程中并不会生成 SQL，而是会生成一个 <code>WhereClause</code> 对象，其中存储着 SQL 节点树：</p>

<pre><code class="language-ruby">pry(main)&gt; User.where(name: 'hacker').where_clause
=&gt; #&lt;ActiveRecord::Relation::WhereClause:0x007fe5a10bf2c8
 @binds=
  [#&lt;ActiveRecord::Relation::QueryAttribute:0x007fe5a10bf4f8
    @name="name",
    @original_attribute=nil,
    @type=#&lt;ActiveModel::Type::String:0x007fe59d33f2e0 @limit=nil, @precision=nil, @scale=nil&gt;,
    @value_before_type_cast="hacker"&gt;],
 @predicates=
  [#&lt;Arel::Nodes::Equality:0x007fe5a10bf368
    @left=
     #&lt;struct Arel::Attributes::Attribute
      relation=
       #&lt;Arel::Table:0x007fe59cc87830
        @name="users",
        @table_alias=nil,
        @type_caster=
         #&lt;ActiveRecord::TypeCaster::Map:0x007fe59cc87bf0
          @types=
           User(id: integer, avatar: string, nickname: string, wechat: string, name: string, gender: integer, school: string, grade: string, major: string, completed: boolean, created_at: datetime, updated_at: datetime, mobile: string, admin: boolean)&gt;&gt;,
      name="name"&gt;,
    @right=#&lt;Arel::Nodes::BindParam:0x007fe5a10bf520&gt;&gt;]&gt;
</code></pre>

<blockquote>
  <p><a href="https://github.com/rails/arel">Arel</a> 是一个 Ruby 的 SQL 抽象语法树的管理器，ActiveRecord 查询的过程都是惰性的，在真正进入数据库查询之前，查询条件都是以语法树的形式存储的。</p>
</blockquote>

<p>在这里不像展开介绍 SQL 语法树的生成过程，因为过程比较复杂，详细分析也没有太大的意义。</p>

<h3 id="order-方法">order 方法</h3>

<p>除了 <code>#where</code> 方法之外，在这里还想简单介绍一下另外一个常用的方法 <code>#order</code>：</p>

<pre><code class="language-ruby">def order(*args)
  check_if_method_has_arguments!(:order, args)
  spawn.order!(*args)
end

def order!(*args)
  preprocess_order_args(args)
  self.order_values += args
  self
end
</code></pre>

<p>该方法的调用栈与 <code>#where</code> 非常相似，在调用栈中都会执行另一个带有 <code>!</code> 的方法，也都会向自己持有的某个『属性』追加一些参数，参数的处理也有点复杂，在这里简单看一看就好：</p>

<pre><code class="language-ruby">def preprocess_order_args(order_args)
  order_args.map! do |arg|
    klass.send(:sanitize_sql_for_order, arg)
  end
  order_args.flatten!
  validate_order_args(order_args)

  references = order_args.grep(String)
  references.map! { |arg| arg =~ /^([a-zA-Z]\w*)\.(\w+)/ &amp;&amp; $1 }.compact!
  references!(references) if references.any?

  # if a symbol is given we prepend the quoted table name
  order_args.map! do |arg|
    case arg
    when Symbol
      arel_attribute(arg).asc
    when Hash
      arg.map { |field, dir|
        case field
        when Arel::Nodes::SqlLiteral
          field.send(dir.downcase)
        else
          arel_attribute(field).send(dir.downcase)
        end
      }
    else
      arg
    end
  end.flatten!
end
</code></pre>

<p>同样的，<code>#order</code> 方法的使用也会向 <code>order_values</code> 数组中添加对应的语法元素：</p>

<pre><code class="language-ruby">pry(main)&gt; User.order(name: :desc).order_values
=&gt; [#&lt;Arel::Nodes::Descending:0x007fe59ce4f190
  @expr=
   #&lt;struct Arel::Attributes::Attribute
    relation=
     #&lt;Arel::Table:0x007fe59cc87830
      @name="users",
      @table_alias=nil,
      @type_caster=
       #&lt;ActiveRecord::TypeCaster::Map:0x007fe59cc87bf0
        @types=
         User(id: integer, avatar: string, nickname: string, wechat: string, name: string, gender: integer, school: string, grade: string, major: string, completed: boolean, created_at: datetime, updated_at: datetime, mobile: string, admin: boolean)&gt;&gt;,
    name=:name&gt;&gt;]
</code></pre>

<p>在这个方法的返回值中，我们也能看到与 Arel 相关的各种节点，可以大致理解上述语法树的作用。</p>

<h3 id="语法树的存储">语法树的存储</h3>

<p>无论是 <code>#where</code> 还是 <code>#order</code> 方法，它们其实都会向当前的 <code>Relation</code> 对象中追加相应的语法树节点，而除了上述的两个方法之外 <code>#from</code>、<code>#distinct</code>、<code>#lock</code>、<code>#limit</code> 等等，几乎所有的查询方法都会改变 <code>Relation</code> 中的某个值，然而所有的值其实都是通过 <code>@values</code> 这个实例变量存储的：</p>

<p><img src="https://img.nju520.me/2017-10-21-activerecord-relation-value-methods.png" alt="activerecord-relation-value-methods" /></p>

<p><code>@values</code> 中存储的值分为三类，<code>SINGLE_VALUE</code>、<code>MULTI_VALUE</code> 和 <code>CLAUSE</code>，这三类属性会按照下面的规则存储在 <code>@values</code> 中：</p>

<pre><code class="language-ruby">Relation::VALUE_METHODS.each do |name|
  method_name = \
    case name
    when *Relation::MULTI_VALUE_METHODS then "#{name}_values"
    when *Relation::SINGLE_VALUE_METHODS then "#{name}_value"
    when *Relation::CLAUSE_METHODS then "#{name}_clause"
    end
  class_eval &lt;&lt;-CODE, __FILE__, __LINE__ + 1
    def #{method_name}                   # def includes_values
      get_value(#{name.inspect})         #   get_value(:includes)
    end                                  # end

    def #{method_name}=(value)           # def includes_values=(value)
      set_value(#{name.inspect}, value)  #   set_value(:includes, value)
    end                                  # end
  CODE
end
</code></pre>

<p>各种不同的值在最后都会按照一定的命名规则，存储在这个 <code>@values</code> 字典中：</p>

<pre><code class="language-ruby">def get_value(name)
  @values[name] || default_value_for(name)
end

def set_value(name, value)
  assert_mutability!
  @values[name] = value
end
</code></pre>

<p>如果我们直接在一个查询链中访问 <code>#values</code> 方法可以获得其中存储的所有查询条件：</p>

<pre><code class="language-ruby">pry(main)&gt; User.where(name: 'hacker').order(name: :desc).values
=&gt; {:references=&gt;[],
 :where=&gt;
  #&lt;ActiveRecord::Relation::WhereClause:0x007fe59d14d860&gt;,
 :order=&gt;
  [#&lt;Arel::Nodes::Descending:0x007fe59d14cd98&gt;]}
</code></pre>

<p>很多 ActiveRecord 的使用者其实在使用的过程中都感觉在各种链式方法调用时没有改变任何事情，所有的方法都可以任意组合进行链式调用，其实每一个方法的调用都会对 <code>@values</code> 中存储的信息进行了修改，只是 ActiveRecord 很好地将它隐藏了幕后，让我们没有感知到它的存在。</p>

<h3 id="scope-方法">scope 方法</h3>

<p>相比于 <code>.default_scope</code> 这个类方法只是改变了当前模型中的 <code>default_scopes</code> 数组，另一个方法 <code>.scope</code> 会为当前类定义一个新的类方法：</p>

<pre><code class="language-ruby">From: lib/active_record/scoping/named.rb @ line 155:
Owner: ActiveRecord::Scoping::Named::ClassMethods

def scope(name, body, &amp;block)
  extension = Module.new(&amp;block) if block

  if body.respond_to?(:to_proc)
    singleton_class.send(:define_method, name) do |*args|
      scope = all.scoping { instance_exec(*args, &amp;body) }
      scope = scope.extending(extension) if extension
      scope || all
    end
  else
    singleton_class.send(:define_method, name) do |*args|
      scope = all.scoping { body.call(*args) }
      scope = scope.extending(extension) if extension
      scope || all
    end
  end
end
</code></pre>

<p>上述方法会直接在当前类的单类上通过 <code>define_methods</code> 为当前类定义类方法，定义的方法会在上面提到的 <code>.all</code> 的返回结果上执行 <code>#scoping</code>，存储当前执行的上下文，执行传入的 block，再恢复 <code>current_scope</code>：</p>

<pre><code class="language-ruby">def scoping
  previous, klass.current_scope = klass.current_scope(true), self
  yield
ensure
  klass.current_scope = previous
end
</code></pre>

<p>在这里其实有一个可能很多人从来没用过的特性，就是在 <code>.scope</code> 方法中传入一个 block：</p>

<pre><code class="language-ruby">class User
  scope :male, -&gt; { where gender: :male } do
    def twenty
      where age: 20
    end
  end
end

pry(main)&gt; User.male.twenty
#=&gt; &lt;#User:0x007f98f3d61c38&gt;
pry(main)&gt; User.twenty
#=&gt; NoMethodError: undefined method `twenty' for #&lt;Class:0x007f98f5c7b2b8&gt;
pry(main)&gt; User.female.twenty
#=&gt; NoMethodError: undefined method `twenty' for #&lt;User::ActiveRecord_Relation:0x007f98f5d950e0&gt;
</code></pre>

<p>这个传入的 block 只会在当前 <code>Relation</code> 对象的单类上添加方法，如果我们想定义一些不想在其他作用域使用的方法就可以使用这种方式：</p>

<pre><code class="language-ruby">def extending(*modules, &amp;block)
  if modules.any? || block
    spawn.extending!(*modules, &amp;block)
  else
    self
  end
end

def extending!(*modules, &amp;block)
  modules &lt;&lt; Module.new(&amp;block) if block
  modules.flatten!
  self.extending_values += modules
  extend(*extending_values) if extending_values.any?
  self
end
</code></pre>

<p>而 <code>extending</code> 方法的实现确实与我们预期的一样，创建了新的 <code>Module</code> 对象之后，直接使用 <code>#extend</code> 将其中的方法挂载当前对象的单类上。</p>

<h3 id="小结-1">小结</h3>

<p>到这里为止，我们对 ActiveRecord 中查询的分析就已经比较全面了，从最终要的 <code>Relation</code> 对象，到常见的 <code>#all</code>、<code>#where</code> 和 <code>#order</code> 方法，到 ActiveRecord 对语法树的存储，如何与 Arel 进行协作，在最后我们也介绍了 <code>.scope</code> 方法的工作原理，对于其它方法或者功能的实现其实也都大同小异，在这里就不展开细谈了。</p>

<h2 id="模型的关系">模型的关系</h2>

<p>作为一个关系型数据库的 ORM，ActiveRecord 一定要提供对模型之间关系的支持，它为模型之间的关系建立提供了四个类方法 <code>has_many</code>、<code>has_one</code>、<code>belongs_to</code> 和 <code>has_and_belongs_to_many</code>，在文章的这一部分，我们会从上面几个方法中选择一部分介绍 ActiveRecord 是如何建立模型之间的关系的。</p>

<p><img src="https://img.nju520.me/2017-10-21-activerecord-associations.png" alt="activerecord-associations" /></p>

<h3 id="association-和继承链">Association 和继承链</h3>

<p>首先来看 <code>.has_many</code> 方法是如何实现的，我们可以通过 pry 直接找到该方法的源代码：</p>

<pre><code class="language-ruby">pry(main)&gt; $ User.has_many

From: lib/active_record/associations.rb @ line 1401:
Owner: ActiveRecord::Associations::ClassMethods

def has_many(name, scope = nil, options = {}, &amp;extension)
  reflection = Builder::HasMany.build(self, name, scope, options, &amp;extension)
  Reflection.add_reflection self, name, reflection
end
</code></pre>

<p>整个 <code>.has_many</code> 方法的实现也只有两行代码，总共涉及两个类 <code>Builder::HasMany</code> 和 <code>Reflection</code>，其中前者用于创建新的 <code>HasMany</code> 关系，后者负责将关系添加到当前类中。</p>

<p><code>HasMany</code> 类的实现其实非常简单，但是它从父类和整个继承链中继承了很多方法：</p>

<p><img src="https://img.nju520.me/2017-10-21-activerecord-hasmany-ancestors.png" alt="activerecord-hasmany-ancestors" /></p>

<p>我们暂时先忘记 <code>.has_many</code> 方法的实现，先来看一下这里涉及的两个非常重要的类都是如何工作的，首先是 <code>Association</code> 以及它的子类；在 ActiveRecord 的实现中，我们其实能够找到四种关系的 Builder，它们有着非常清晰简单的继承关系：</p>

<p><img src="https://img.nju520.me/2017-10-21-activerecord-ancestor-builders.png" alt="activerecord-ancestor-builders" /></p>

<p>在这里定义的 <code>.build</code> 方法其实实现也很清晰，它通过调用当前抽象类 <code>Association</code> 或者子类的响应方法完成一些建立关系必要的工作：</p>

<pre><code class="language-ruby">def self.build(model, name, scope, options, &amp;block)
  extension = define_extensions model, name, &amp;block
  reflection = create_reflection model, name, scope, options, extension
  define_accessors model, reflection
  define_callbacks model, reflection
  define_validations model, reflection
  reflection
end
</code></pre>

<p>其中包括创建用于操作、查询和管理当前关系扩展 Module 的 <code>.define_extensions</code> 方法，同时也会使用 <code>.create_reflection</code> 创建一个用于检查 ActiveRecord 类的关系的 <code>Reflection</code> 对象，我们会在下一节中展开介绍，在创建了 <code>Reflection</code> 后，我们会根据传入的模型和 <code>Reflection</code> 对象为当前的类，例如 <code>User</code> 定义属性存取方法、回调以及验证:</p>

<pre><code class="language-ruby">def self.define_accessors(model, reflection)
  mixin = model.generated_association_methods
  name = reflection.name
  define_readers(mixin, name)
  define_writers(mixin, name)
end

def self.define_readers(mixin, name)
  mixin.class_eval &lt;&lt;-CODE, __FILE__, __LINE__ + 1
    def #{name}(*args)
      association(:#{name}).reader(*args)
    end
  CODE
end

def self.define_writers(mixin, name)
  mixin.class_eval &lt;&lt;-CODE, __FILE__, __LINE__ + 1
    def #{name}=(value)
      association(:#{name}).writer(value)
    end
  CODE
end
</code></pre>

<p>存取方法还是通过 Ruby 的元编程能力定义的，在这里通过 <code>.class_eval</code> 方法非常轻松地就能在当前的模型中定义方法，关于回调和验证的定义在这里就不在展开介绍了。</p>

<h3 id="reflection-和继承链">Reflection 和继承链</h3>

<p><code>Reflection</code> 启用了检查 ActiveRecord 类和对象的关系和聚合的功能，它能够在 Builder 中使用为 ActiveRecord 中的类创建对应属性和方法。</p>

<p>与 <code>Association</code> 一样，ActiveRecord 中的不同关系也有不同的 <code>Reflection</code>，根据不同的关系和不同的配置，ActiveRecord 中建立了一套 Reflection 的继承体系与数据库中的不同关系一一对应：</p>

<p><img src="https://img.nju520.me/2017-10-21-activerecord-reflections.png" alt="activerecord-reflections" /></p>

<p>当我们在上面使用 <code>.has_many</code> 方法时，会通过 <code>.create_reflection</code> 创建一个 <code>HasManyReflection</code> 对象：</p>

<pre><code class="language-ruby">def self.create_reflection(model, name, scope, options, extension = nil)
  if scope.is_a?(Hash)
    options = scope
    scope   = nil
  end

  validate_options(options)
  scope = build_scope(scope, extension)
  ActiveRecord::Reflection.create(macro, name, scope, options, model)
end
</code></pre>

<p><code>Reflection#create</code> 方法是一个工厂方法，它会根据传入的 <code>macro</code> 和 <code>options</code> 中的值选择合适的类实例化：</p>

<pre><code class="language-ruby">def self.create(macro, name, scope, options, ar)
  klass = \
    case macro
    when :composed_of
      AggregateReflection
    when :has_many
      HasManyReflection
    when :has_one
      HasOneReflection
    when :belongs_to
      BelongsToReflection
    else
      raise "Unsupported Macro: #{macro}"
    end

  reflection = klass.new(name, scope, options, ar)
  options[:through] ? ThroughReflection.new(reflection) : reflection
end
</code></pre>

<p>这个创建的 <code>Reflection</code> 在很多时候都有非常重要的作用，在创建存储方法、回调和验证时，都需要将这个对象作为参数传入提供一定的支持，起到了数据源和提供 Helper 方法的作用。</p>

<p>在整个定义方法、属性以及回调的工作完成之后，会将当前的对象以 <code>name</code> 作为键存储到自己持有的一个 <code>_reflections</code> 字典中：</p>

<pre><code class="language-ruby"># class_attribute :_reflections, instance_writer: false

def self.add_reflection(ar, name, reflection)
  ar.clear_reflections_cache
  ar._reflections = ar._reflections.merge(name.to_s =&gt; reflection)
end
</code></pre>

<p>这个字典中存储着所有在当前类中使用 <code>has_many</code>、<code>has_one</code>、<code>belongs_to</code> 等方法定义的关系对应的映射。</p>

<h3 id="一对多关系">一对多关系</h3>

<p>一对多关系的这一节会分别介绍两个极其重要的方法 <code>.has_many</code> 和 <code>.belongs_to</code> 的实现；在这里，会先通过 <code>.has_many</code> 关系了解它是如何通过覆写父类方法定制自己的特性的，之后会通过 <code>.belongs_to</code> 研究 getter/setter 方法的调用栈。</p>

<p><img src="https://img.nju520.me/2017-10-21-one-to-many-association.png" alt="one-to-many-association" /></p>

<p>一对多关系在数据库的模型之间非常常见，而这两个方法在 ActiveRecord 也经常成对出现。</p>

<h4 id="has_many">has_many</h4>

<p>当我们对构建关系模块的两大支柱都已经有所了解之后，再来看这几个常用的方法就没有太多的难度了，首先来看一下一对多关系中的『多』是怎么实现的：</p>

<pre><code class="language-ruby">def has_many(name, scope = nil, options = {}, &amp;extension)
  reflection = Builder::HasMany.build(self, name, scope, options, &amp;extension)
  Reflection.add_reflection self, name, reflection
end
</code></pre>

<p>由于已经对 <code>Reflection.add_reflection</code> 方法的实现有所了解，所以这里直接看 <code>.has_many</code> 调用的 <code>Builder::HasMany.build</code> 方法的实现就可以知道这个类方法究竟做了什么，：</p>

<pre><code class="language-ruby">def self.build(model, name, scope, options, &amp;block)
  extension = define_extensions model, name, &amp;block
  reflection = create_reflection model, name, scope, options, extension
  define_accessors model, reflection
  define_callbacks model, reflection
  define_validations model, reflection
  reflection
end
</code></pre>

<p>在这里执行的 <code>.build</code> 方法与抽象类中的方法实现完全相同，子类并没有覆盖父类实现的方法，我们来找一下 <code>.define_accessors</code>、<code>.define_callbacks</code> 和 <code>.define_validations</code> 三个方法在 has_many 关系中都做了什么。</p>

<p><code>HasMany</code> 作为 has_many 关系的 Builder 类，其本身并没有实现太多的方法，只是对一些关系选项有一些自己独有的声明：</p>

<pre><code class="language-ruby">module ActiveRecord::Associations::Builder
  class HasMany &lt; CollectionAssociation
    def self.macro
      :has_many
    end

    def self.valid_options(options)
      super + [:primary_key, :dependent, :as, :through, :source, :source_type, :inverse_of, :counter_cache, :join_table, :foreign_type, :index_errors]
    end

    def self.valid_dependent_options
      [:destroy, :delete_all, :nullify, :restrict_with_error, :restrict_with_exception]
    end
  end
end
</code></pre>

<p>由于本身 has_many 关系中的读写方法都是对集合的操作，所以首先覆写了 <code>.define_writers</code> 和 <code>.define_readers</code> 两个方法生成了另外一组操作 id 的 getter/setter 方法：</p>

<pre><code class="language-ruby">def self.define_readers(mixin, name)
  super

  mixin.class_eval &lt;&lt;-CODE, __FILE__, __LINE__ + 1
    def #{name.to_s.singularize}_ids
      association(:#{name}).ids_reader
    end
  CODE
end

def self.define_writers(mixin, name)
  super

  mixin.class_eval &lt;&lt;-CODE, __FILE__, __LINE__ + 1
    def #{name.to_s.singularize}_ids=(ids)
      association(:#{name}).ids_writer(ids)
    end
  CODE
end
</code></pre>

<p>has_many 关系在 <code>CollectionAssociation</code> 和 <code>HasManyAssociation</code> 中实现的几个方法 <code>#reader</code>、<code>#writer</code>、<code>#ids_reader</code> 和 <code>#ids_writer</code> 其实还是比较复杂的，在这里就跳过不谈了。</p>

<p>而 <code>.define_callbacks</code> 和 <code>.define_extensions</code> 其实都大同小异，在作者看来没有什么值得讲的，has_many 中最重要的部分还是读写方法的实现过程，不过由于篇幅所限这里就不多说了。</p>

<h4 id="belongs_to">belongs_to</h4>

<p>在一对多关系中，经常与 has_many 对应的关系 belongs_to 其实实现和调用栈也几乎完全相同：</p>

<pre><code class="language-ruby">def belongs_to(name, scope = nil, options = {})
  reflection = Builder::BelongsTo.build(self, name, scope, options)
  Reflection.add_reflection self, name, reflection
end
</code></pre>

<p>但是与 has_many 比较大的不同是 <code>Builder::BelongsTo</code> 通过继承的父类定义了很多用于创建新关系的方法：</p>

<pre><code class="language-ruby">def self.define_accessors(model, reflection)
  super
  mixin = model.generated_association_methods
  name = reflection.name
  define_constructors(mixin, name) if reflection.constructable?
  mixin.class_eval &lt;&lt;-CODE, __FILE__, __LINE__ + 1
    def reload_#{name}
      association(:#{name}).force_reload_reader
    end
  CODE
end

def self.define_constructors(mixin, name)
  mixin.class_eval &lt;&lt;-CODE, __FILE__, __LINE__ + 1
    def build_#{name}(*args, &amp;block)
      association(:#{name}).build(*args, &amp;block)
    end
    def create_#{name}(*args, &amp;block)
      association(:#{name}).create(*args, &amp;block)
    end
    def create_#{name}!(*args, &amp;block)
      association(:#{name}).create!(*args, &amp;block)
    end
  CODE
end
</code></pre>

<p>其他的部分虽然实现上也与 has_many 有着非常大的不同，但是原理基本上完全一致，不过在这里我们可以来看一下 belongs_to 关系创建的两个方法 <code>association</code> 和 <code>association=</code> 究竟是如何对数据库进行操作的。</p>

<pre><code class="language-ruby">class Topic &lt; ActiveRecord::Base
  has_many :subtopics
end

class Subtopic &lt; ActiveRecord::Base
  belongs_to :topic
end
</code></pre>

<p>假设我们有着如上所示的两个模型，它们之间是一对多关系，我们以这对模型为例先来看一下 <code>association</code> 这个读方法的调用栈。</p>

<p><img src="https://img.nju520.me/2017-10-21-callstack-for-belongs-to-association-getter.png" alt="callstack-for-belongs-to-association-gette" /></p>

<p>通过我们对源代码和调用栈的阅读，我们可以发现其实如下的所有方法调用在大多数情况下是完全等价的，假设我们已经持有了一个 <code>Subtopic</code> 对象：</p>

<pre><code class="language-ruby">subtopic = Subtopic.first #=&gt; #&lt;Subtopic:0x007ff513f67768&gt;

subtopic.topic
subtopic.association(:topic).reader
subtopic.association(:topic).target
subtopic.association(:topic).load_target
subtopic.association(:topic).send(:find_target)
</code></pre>

<p>上述的五种方式都可以获得当前 <code>Subtopic</code> 对象的 belongs_to 关系对应的 <code>Topic</code> 数据行，而最后一个方法 <code>#find_target</code> 其实也就是真正创建、绑定到最后执行查询 SQL 的方法：</p>

<pre><code class="language-ruby">pry(main)&gt; $ subtopic.association(:topic).find_target

From: lib/active_record/associations/singular_association.rb @ line 38:
Owner: ActiveRecord::Associations::SingularAssociation

def find_target
  return scope.take if skip_statement_cache?

  conn = klass.connection
  sc = reflection.association_scope_cache(conn, owner) do
    StatementCache.create(conn) { |params|
      as = AssociationScope.create { params.bind }
      target_scope.merge(as.scope(self, conn)).limit(1)
    }
  end

  binds = AssociationScope.get_bind_values(owner, reflection.chain)
  sc.execute(binds, klass, conn) do |record|
    set_inverse_instance record
  end.first
rescue ::RangeError
  nil
end
</code></pre>

<p>我们已经对 <code>association</code> 方法的实现有了非常清楚的认知了，下面再来过一下 <code>association=</code> 方法的实现，首先还是来看一下 setter 方法的调用栈：</p>

<p><img src="https://img.nju520.me/2017-10-21-callstack-for-belongs-to-association-setter.png" alt="callstack-for-belongs-to-association-sette" /></p>

<p>相比于 getter 的调用栈，setter 方法的调用栈都复杂了很多，在研究 setter 方法实现的过程中我们一定要记住这个方法并不会改变数据库中对应的数据行，只会改变当前对应的某个属性，经过对调用栈和源代码的分析，我们可以有以下的结论：假设现在有一个 <code>Subtopic</code> 对象和一个新的 <code>Topic</code> 实例，那么下面的一系列操作其实是完全相同的：</p>

<pre><code class="language-ruby">subtopic = Subtopic.first #=&gt; #&lt;Subtopic:0x007ff513f67768&gt;
new_topic = Topic.first   #=&gt; #&lt;Topic:0x007ff514b24cb8&gt;

subtopic.topic = new_topic
subtopic.topic_id = new_topic.id
subtopic.association(:topic).writer(new_topic)
subtopic.association(:topic).replace(new_topic)
subtopic.association(:topic).replace_keys(new_topic)
subtopic.association(:topic).owner[:topic_id] = new_topic.id
subtopic[:topic_id] = new_topic.id
subtopic.write_attribute(:topic_id, new_topic.id)
</code></pre>

<p>虽然这些方法最后返回的结果可能有所不同，但是它们最终都会将 <code>subtopic</code> 对象的 <code>topic_id</code> 属性更新成 <code>topic.id</code>，上面的方法中有简单的，也有复杂的，不过都能达到相同的目的；我相信如果读者亲手创建上述的关系并使用 pry 查看源代码一定会对 getter 和 setter 的执行过程有着非常清楚的认识。</p>

<h3 id="多对多关系-habtm">多对多关系 habtm</h3>

<p>无论是 has_many 还是 belongs_to 其实都是一个 ORM 原生需要支持的关系，但是 habtm(has_and_belongs_to_many) 却是 ActiveRecord 为我们提供的一个非常方便的语法糖，哪怕是并没有 <code>.has_and_belongs_to_many</code> 这个方法，我们也能通过 <code>.has_many</code> 实现多对多关系，得到与前者完全等价的效果，只是实现的过程稍微麻烦一些。</p>

<p>在这一小节中，我们想要了解 habtm 这个语法糖是如何工作的，它是如何将现有的关系组成更复杂的 habtm 的多对多关系的；想要了解它的工作原理，我们自然要分析它的源代码：</p>

<pre><code class="language-ruby">def has_and_belongs_to_many(name, scope = nil, **options, &amp;extension)
  builder = Builder::HasAndBelongsToMany.new name, self, options
  join_model = ActiveSupport::Deprecation.silence { builder.through_model }
  const_set join_model.name, join_model
  private_constant join_model.name

  habtm_reflection = ActiveRecord::Reflection::HasAndBelongsToManyReflection.new(name, scope, options, self)
  middle_reflection = builder.middle_reflection join_model
  Builder::HasMany.define_callbacks self, middle_reflection
  Reflection.add_reflection self, middle_reflection.name, middle_reflection
  middle_reflection.parent_reflection = habtm_reflection

  # ...

  hm_options = {}
  hm_options[:through] = middle_reflection.name
  hm_options[:source] = join_model.right_reflection.name

  # ...

  ActiveSupport::Deprecation.silence { has_many name, scope, hm_options, &amp;extension }
  _reflections[name.to_s].parent_reflection = habtm_reflection
end
</code></pre>

<blockquote>
  <p>在这里，我们对该方法的源代码重新进行组织和排序，方法的作用与 v5.1.4 中的完全相同。</p>
</blockquote>

<p>上述方法在最开始先创建了一个 <code>HasAndBelongsToMany</code> 的 Builder 实例，然后在 block 中执行了这个 Builder 的 <code>#through_model</code> 方法：</p>

<pre><code class="language-ruby">def through_model
  habtm = JoinTableResolver.build lhs_model, association_name, options

  join_model = Class.new(ActiveRecord::Base) {
    class &lt;&lt; self;
      attr_accessor :left_model
      attr_accessor :name
      attr_accessor :table_name_resolver
      attr_accessor :left_reflection
      attr_accessor :right_reflection
    end

    # ...
  }

  join_model.name                = "HABTM_#{association_name.to_s.camelize}"
  join_model.table_name_resolver = habtm
  join_model.left_model          = lhs_model
  join_model.add_left_association :left_side, anonymous_class: lhs_model
  join_model.add_right_association association_name, belongs_to_options(options)
  join_model
end
</code></pre>

<p><code>#through_model</code> 方法会返回一个新的继承自 <code>ActiveRecord::Base</code> 的类，我们通过一下的例子来说明一下这里究竟做了什么，假设在我们的工程中定义了如下的两个类：</p>

<pre><code class="language-ruby">class Post &lt; ActiveRecord::Base
  has_and_belongs_to_many :tags
end

class Tag &lt; ActiveRecord::Base
  has_and_belongs_to_many :posts
end
</code></pre>

<p>它们每个类都通过 <code>.has_and_belongs_to_many</code> 创建了一个 <code>join_model</code> 类，这两个类都是在当前类的命名空间下的：</p>

<pre><code class="language-ruby">class Post::HABTM_Posts &lt; ActiveRecord::Base; end
class Tags::HABTM_Posts &lt; ActiveRecord::Base; end
</code></pre>

<p>除了在当前类的命名空间下定义两个新的类之外，<code>#through_model</code> 方法还通过 <code>#add_left_association</code> 和 <code>#add_right_association</code> 为创建的私有类添加了两个 <code>.belongs_to</code> 方法的调用：</p>

<pre><code class="language-ruby">join_model = Class.new(ActiveRecord::Base) {
  # ...

  def self.add_left_association(name, options)
    belongs_to name, required: false, **options
    self.left_reflection = _reflect_on_association(name)
  end

  def self.add_right_association(name, options)
    rhs_name = name.to_s.singularize.to_sym
    belongs_to rhs_name, required: false, **options
    self.right_reflection = _reflect_on_association(rhs_name)
  end
}
</code></pre>

<p>所以在这里，每一个 HABTM 类中都通过 <code>.belongs_to</code> 增加了两个对数据库表中对应列的映射：</p>

<pre><code class="language-ruby">class Post::HABTM_Posts &lt; ActiveRecord::Base
  belongs_to :post_id, required: false
  belongs_to :tag_id, required: false
end

class Tags::HABTM_Posts &lt; ActiveRecord::Base
  belongs_to :tag_id, required: false
  belongs_to :post_id, required: false
end
</code></pre>

<p>看到这里，你可能会认为既然有两个模型，那么应该会有两张表分别对应这两个模型，但是实际情况却不是这样。</p>

<p><img src="https://img.nju520.me/2017-10-21-habtm-association-table-name.png" alt="habtm-association-table-name" /></p>

<p>ActiveRecord 通过覆写这两个类的 <code>.table_name</code> 方法，使用一个 <code>JoinTableResolver</code> 来解决不同的模型拥有相同的数据库表的问题：</p>

<pre><code class="language-ruby">class Migration
  module JoinTable
    def join_table_name(table_1, table_2)
      ModelSchema.derive_join_table_name(table_1, table_2).to_sym
    end
  end
end

module ModelSchema
  def self.derive_join_table_name(first_table, second_table) 
    [first_table.to_s, second_table.to_s].sort.join("\0").gsub(/^(.*_)(.+)\0\1(.+)/, '\1\2_\3').tr("\0", "_")
  end
end
</code></pre>

<p>在默认的 <code>join_table</code> 规则中，两张表会按照字母顺序排序，最后通过 <code>_</code> 连接到一起，但是如果两张表有着完全相同的前缀，比如 music_artists 和 music_records 两张表，它们连接的结果就是 music_artists_records，公共的前缀会被删除，这种情况经常发生在包含命名空间的模型中，例如：<code>Music::Artist</code>。</p>

<p>当我们已经通过多对多关系的 Builder 创建了一个中间模型之后，就会建立两个 <code>Reflection</code> 对象：</p>

<pre><code class="language-ruby">habtm_reflection = ActiveRecord::Reflection::HasAndBelongsToManyReflection.new(name, scope, options, self)
middle_reflection = builder.middle_reflection join_model
Builder::HasMany.define_callbacks self, middle_reflection
Reflection.add_reflection self, middle_reflection.name, middle_reflection
middle_reflection.parent_reflection = habtm_reflection
</code></pre>

<p>其中一个对象是 <code>HasAndBelongsToManyReflection</code> 实例，表示当前的多对多关系，另一个对象是 <code>#middle_reflection</code> 方法返回的 <code>HasMany</code>，表示当前的类与 <code>join_model</code> 之间有一个一对多关系，这个关系是隐式的，不过我们可以通过下面的代码来『理解』它：</p>

<pre><code class="language-ruby">class Post &lt; ActiveRecord::Base
  # has_and_belongs_to_many :posts
  # =
  has_many :posts_tag
  # + 
  # ...
end
</code></pre>

<p>上述的代码构成了整个多对多关系的一部分，而另一部分由下面的代码来处理，当模型持有了一个跟中间模型相关的一对多关系之后，就会创建另一个以中间模型为桥梁 has_many 关系：</p>

<pre><code class="language-ruby">hm_options = {}
hm_options[:through] = middle_reflection.name
hm_options[:source] = join_model.right_reflection.name

ActiveSupport::Deprecation.silence { has_many name, scope, hm_options, &amp;extension }
</code></pre>

<p>这里还是使用 <code>Post</code> 和 <code>Tag</code> 这两个模型之间的关系举例子，通过上述代码，我们会在两个类中分别建立如下的关系：</p>

<pre><code class="language-ruby">class Post &lt; ActiveRecord::Base
  # has_many :posts_tag
  has_many :tags, through: :posts_tag, source: :tag
end

class Tag &lt; ActiveRecord::Base
  # has_many :tags_post
  has_many :post, through: :tags_post, source: :post
end
</code></pre>

<p>通过两个隐式的 has_many 关系，两个显示的 has_many 就能够通过 <code>through</code> 和 <code>source</code> 间接找到自己对应的多个数据行，而从开发者的角度来看，整个工程中只使用了一行代码 <code>has_and_belongs_to_many :models</code>，其他的工作完全都是隐式的。</p>

<p><img src="https://img.nju520.me/2017-10-21-many-to-many-associations.png" alt="many-to-many-associations" /></p>

<p>由于关系型数据库其实并没有物理上的多对多关系，只有在逻辑上才能实现多对多，所以对于每一个模型来说，它实现的都是一对多关系；只有从整体来看，通过 <code>PostsTags</code> 第三张表的引入，我们实现的才是从 <code>Post</code> 到 <code>Tag</code> 之间的多对多关系。</p>

<h3 id="小结-2">小结</h3>

<p>ActiveRecord 对关系的支持其实非常全面，从最常见的一对一、一对多关系，再到多对多关系，都有着非常优雅、简洁的实现，虽然这一小节中没能全面的介绍所有关系的实现，但是对整个模块中重要类和整体架构的介绍已经非常具体了；不得不感叹 ActiveRecord 对多对多关系方法 <code>has_and_belongs_to_many</code> 的实现非常整洁，我们在分析其实现时也非常顺畅。</p>

<h2 id="migrations-任务和执行过程">Migrations 任务和执行过程</h2>

<p>Migrations（迁移）是 ActiveRecord 提供的一种用于更改数据库 Schema 的方式，它提供了可以直接操作数据库的 DSL，这样我们就不需要自己去手写所有的 SQL 来更新数据库中的表结构了。</p>

<p><img src="https://img.nju520.me/2017-10-21-activerecord-migrations.png" alt="activerecord-migrations" /></p>

<p>每一个 Migration 都具有一个唯一的时间戳，每次进行迁移时都会在现有的数据库中执行当前 Migration 文件的 DSL 更新数据库 Schema 得到新的数据库版本。而想要理解 Migrations 是如何工作的，就需要知道 <code>#create_table</code>、<code>#add_column</code> 等 DSL 是怎么实现的。</p>

<h3 id="migration51">Migration[5.1]</h3>

<p>我在使用 ActiveRecord 提供的数据库迁移的时候一直都特别好奇 <code>Migration[5.1]</code> 后面跟着的这个 <code>[5.1]</code> 是个什么工作原理，看了源代码之后我才知道：</p>

<pre><code class="language-ruby">class Migration
  def self.[](version)
    Compatibility.find(version)
  end
end
</code></pre>

<p><code>.[]</code> 是 <code>ActiveRecord::Migration</code> 的类方法，它通过执行 <code>Compatibility.find</code> 来判断当前的代码中使用的数据库迁移版本是否与 gem 中的版本兼容：</p>

<pre><code class="language-ruby">class Current &lt; Migration
end
</code></pre>

<p><code>compatibility.rb</code> 在兼容性方面做了很多事情，保证 ActiveRecord 中的迁移都是可以向前兼容的，在这里也就不准备介绍太多了。</p>

<h3 id="从-rake-dbmigrate-开始">从 rake db:migrate 开始</h3>

<p>作者在阅读迁移部分的源代码时最开始以 <code>Migration</code> 类作为入口，结果发现这并不是一个好的选择，最终也没能找到定义 DSL 的位置，所以重新选择了 <code>rake db:migrate</code> 作为入口分析迁移的实现；通过对工程目录的分析，很快就能发现 ActiveRecord 中所有的 rake 命令都位于 <code>lib/railties/database.rake</code> 文件中，在文件中也能找到 <code>db:migrate</code> 对应的 rake 任务：</p>

<pre><code class="language-ruby">db_namespace = namespace :db do
  desc "Migrate the database (options: VERSION=x, VERBOSE=false, SCOPE=blog)."
  task migrate: [:environment, :load_config] do
    ActiveRecord::Tasks::DatabaseTasks.migrate
    db_namespace["_dump"].invoke
  end
end
</code></pre>

<p>上述代码中的 <code>DatabaseTasks</code> 类就包含在 <code>lib/active_record/tasks</code> 目录中的 <code>database_tasks.rb</code> 文件里：</p>

<pre><code class="language-ruby">lib/active_record/tasks/
├── database_tasks.rb
├── mysql_database_tasks.rb
├── postgresql_database_tasks.rb
└── sqlite_database_tasks.rb
</code></pre>

<p><code>#migrate</code> 方法就是 <code>DatabaseTasks</code> 的一个实例方法，同时 ActiveRecord 通过 <code>extend self</code> 将 <code>#migrate</code> 方法添加到了当前类的单类上，成为了当前类的类方法：</p>

<pre><code class="language-ruby">module Tasks
  module DatabaseTasks
    extend self
    
    def migrate
      raise "Empty VERSION provided" if ENV["VERSION"] &amp;&amp; ENV["VERSION"].empty?

      version = ENV["VERSION"] ? ENV["VERSION"].to_i : nil
      scope = ENV["SCOPE"]
      Migrator.migrate(migrations_paths, version) do |migration|
        scope.blank? || scope == migration.scope
      end
      ActiveRecord::Base.clear_cache!
    end
  end
end
</code></pre>

<h4 id="迁移器migrator">『迁移器』Migrator</h4>

<p>迁移任务中主要使用了 <code>Migrator.migrate</code> 方法，通过传入迁移文件的路径和期望的迁移版本对数据库进行迁移：</p>

<pre><code class="language-ruby">class Migrator#:nodoc:
  class &lt;&lt; self
    def migrate(migrations_paths, target_version = nil, &amp;block)
      case
      when target_version.nil?
        up(migrations_paths, target_version, &amp;block)
      when current_version == 0 &amp;&amp; target_version == 0
        []
      when current_version &gt; target_version
        down(migrations_paths, target_version, &amp;block)
      else
        up(migrations_paths, target_version, &amp;block)
      end
    end
  end
end
</code></pre>

<p>在默认情况下，显然我们是不会传入目标的数据库版本的，也就是 <code>target_version.nil? == true</code>，这时会执行 <code>.up</code> 方法，对数据库向『上』迁移：</p>

<pre><code class="language-ruby">def up(migrations_paths, target_version = nil)
  migrations = migrations(migrations_paths)
  migrations.select! { |m| yield m } if block_given?

  new(:up, migrations, target_version).migrate
end
</code></pre>

<h4 id="方法调用栈">方法调用栈</h4>

<p>通过 <code>.new</code> 方法 ActiveRecord 初始化了一个新的 <code>Migrator</code> 实例，然后执行了 <code>Migrator#migrate</code>，在整个迁移执行的过程中，我们有以下的方法调用栈：</p>

<p><img src="https://img.nju520.me/2017-10-21-rake-db-migrate.png" alt="rake-db-migrate" /></p>

<p>在整个迁移过程的调用栈中，我们会关注以下的四个部分，首先是 <code>Migrator#migrate_without_lock</code> 方法：</p>

<pre><code class="language-ruby">def migrate_without_lock
  if invalid_target?
    raise UnknownMigrationVersionError.new(@target_version)
  end

  result = runnable.each do |migration|
    execute_migration_in_transaction(migration, @direction)
  end

  record_environment
  result
end
</code></pre>

<p>这个方法其实并没有那么重要，但是这里调用了 <code>Migrator#runnable</code> 方法，这个无参的方法返回了所有需要运行的 <code>Migration</code> 文件，<code>Migrator#runnable</code> 是如何选择需要迁移的文件是作者比较想要了解的，也是作者认为比较重要的地方：</p>

<pre><code class="language-ruby">def runnable
  runnable = migrations[start..finish]
  if up?
    runnable.reject { |m| ran?(m) }
  else
    runnable.pop if target
    runnable.find_all { |m| ran?(m) }
  end
end

def ran?(migration)
  migrated.include?(migration.version.to_i)
end
</code></pre>

<p>通过对这个方法的阅读的分析，我们可以看到，如果迁移模式是 <code>:up</code>，那么就会选择所有未迁移的文件，也就是说在这时<strong>迁移文件的选择与创建的顺序是无关的</strong>。</p>

<h4 id="迁移的执行">迁移的执行</h4>

<p>当我们通过 <code>#runnable</code> 获得了整个待运行的迁移文件数组之后，就可以遍历所有的文件一次执行 <code>Migrator#execute_migrate_in_transaction</code> 方法了，在调用栈的最后会执行 <code>Migration#exec_migration</code>：</p>

<pre><code class="language-ruby">def exec_migration(conn, direction)
  @connection = conn
  if respond_to?(:change)
    if direction == :down
      revert { change }
    else
      change
    end
  else
    send(direction)
  end
ensure
  @connection = nil
end
</code></pre>

<p>到这里就能与我们平时在 <code>Migration</code> 中实现的 <code>#change</code>、<code>#up</code> 和 <code>#down</code> 连到一起，逻辑也走通了；上述代码的逻辑还是很清晰的，如果当前的 <code>Migratoin</code> 实现了 <code>#change</code> 方法就会根据 <code>direction</code> 选择执行 <code>#change</code> 还是 <code>#revert + #change</code>，否则就会按照迁移的方向执行对应的方法。</p>

<h3 id="migrations-的-dsl">Migrations 的 DSL</h3>

<p>在数据迁移的模块执行的 Migration 文件中包含的都是 ActiveRecord 提供的 DSL 语法，这部分语法包含两部分，一部分是 Schema 相关的 DSL <code>schema_statements.rb</code>，其中包括表格的创建和删除以及一些用于辅助 Schema 创建的 <code>#column_exists?</code> 等方法，另一部分是表定义相关的 DSL <code>schema_definitions.rb</code>，其中包括处理表结构的 <code>TableDefinition</code> 类和抽象代表一张数据库中表的 <code>Table</code> 类。</p>

<h4 id="抽象适配器">抽象适配器</h4>

<p>在整个 <code>connection_adapters</code> 的子模块中，绝大多数模块在三大 SQL 数据库，MySQL、PostgreSQL 和 sqlite3 中都有着各自的实现：</p>

<pre><code class="language-ruby">lib/active_record/connection_adapters
├── abstract
│   ├── connection_pool.rb
│   ├── database_limits.rb
│   ├── database_statements.rb
│   ├── query_cache.rb
│   ├── quoting.rb
│   ├── savepoints.rb
│   ├── schema_creation.rb
│   ├── schema_definitions.rb
│   ├── schema_dumper.rb
│   ├── schema_statements.rb
│   └── transaction.rb
├── mysql
│   ├── column.rb
│   ├── database_statements.rb
│   ├── explain_pretty_printer.rb
│   ├── quoting.rb
│   ├── schema_creation.rb
│   ├── schema_definitions.rb
│   ├── schema_dumper.rb
│   ├── schema_statements.rb
│   └── type_metadata.rb
├── postgresql
│   └── ...
├── sqlite3
│   └── ...
├── abstract_adapter.rb
├── ...
└── sqlite3_adapter.rb
</code></pre>

<p>不过这三个数据库的所有子模块都继承自 <code>AbstractAdapter</code> 下面对应的子模块，以获得一些三者共用的能力，包括数据库、Schema 的声明与管理等功能。</p>

<p><img src="https://img.nju520.me/2017-10-21-abstract-adapter-and-much-more.png" alt="abstract-adapter-and-much-more" /></p>

<p>通过 <code>AbstractAdapter</code> 抽离出的公用功能，我们可以通过新的适配器随时适配其他的 SQL 数据库。</p>

<h4 id="schema-dsl">Schema DSL</h4>

<p>数据库的 Schema DSL 部分就包含我们经常使用的 <code>#create_table</code>、<code>#rename_table</code> 以及 <code>#add_column</code> 这些需要表名才能执行的方法，在这里以最常见的 <code>#create_table</code> 为例，简单分析一下这部分代码的实现：</p>

<pre><code class="language-ruby">def create_table(table_name, comment: nil, **options)
  td = create_table_definition table_name, options[:temporary], options[:options], options[:as], comment: comment

  yield td if block_given?

  execute schema_creation.accept td
end
</code></pre>

<p>首先，在创建表时先通过 <code>#create_table_definition</code> 方法创建一个新的 <code>TableDefinition</code> 实例，然后将这个实例作为参数传入 block：</p>

<pre><code class="language-ruby">create_table :users do |t|
end
</code></pre>

<p>在 block 对这个 <code>TableDefinition</code> 对象一顿操作后，会通过 <code>SchemaCreation#accept</code> 方法获得一个用于在数据库中，能够创建表的 SQL 语句：</p>

<pre><code class="language-ruby">def accept(o)
  m = @cache[o.class] ||= "visit_#{o.class.name.split('::').last}"
  send m, o
end

def visit_TableDefinition(o)
  create_sql = "CREATE#{' TEMPORARY' if o.temporary} TABLE #{quote_table_name(o.name)} "

  statements = o.columns.map { |c| accept c }
  statements &lt;&lt; accept(o.primary_keys) if o.primary_keys

  create_sql &lt;&lt; "(#{statements.join(', ')})" if statements.present?
  add_table_options!(create_sql, table_options(o))
  create_sql &lt;&lt; " AS #{@conn.to_sql(o.as)}" if o.as
  create_sql
end
</code></pre>

<p><code>SchemaCreation</code> 类就是一个接受各种各样的 <code>TableDefinition</code>、<code>PrimaryKeyDefinition</code> 对象返回 SQL 的一个工具，可以将 <code>SchemaCreation</code> 理解为一个表结构的解释器；最后的 <code>#execute</code> 会在数据库中执行 SQL 改变数据库中的表结构。</p>

<p>在 <code>SchemaStatements</code> 中定义的其它方法的实现也都是大同小异，比如 <code>#drop_table</code> 其实都是删除数据库中的某张表：</p>

<pre><code class="language-ruby">def drop_table(table_name, options = {})
  execute "DROP TABLE#{' IF EXISTS' if options[:if_exists]} #{quote_table_name(table_name)}"
end
</code></pre>

<h4 id="表定义-dsl">表定义 DSL</h4>

<p><code>SchemaStatements</code> 中定义的方法，参数大都包含 <code>table_name</code>，而另一个类 <code>TableDefinitions</code> 就包含了直接对表操作的 DSL：</p>

<pre><code class="language-ruby">create_table :foo do |t|
  puts t.class  # =&gt; "ActiveRecord::ConnectionAdapters::TableDefinition"
end
</code></pre>

<p>当我们在 <code>#create_table</code> 中使用例如 <code>#string</code>、<code>#integer</code> 等方法时，所有的方法都会通过元编程的魔法最终执行 <code>TableDefinition#column</code> 改变表的定义：</p>

<pre><code class="language-ruby">module ColumnMethods
  [
    :bigint,
    # ...
    :integer,
    :string,
    :text,
    :time,
    :timestamp,
    :virtual,
  ].each do |column_type|
    module_eval &lt;&lt;-CODE, __FILE__, __LINE__ + 1
      def #{column_type}(*args, **options)
        args.each { |name| column(name, :#{column_type}, options) }
      end
    CODE
  end
  alias_method :numeric, :decimal
end
</code></pre>

<p><code>#column</code> 方法非常神奇，它从各处收集有关当前表的定义，最终为表中的每一个字段创建一个 <code>ColumnDefinition</code> 实例，并存储到自己持有的 <code>@columns_hash</code> 中：</p>

<pre><code class="language-ruby">def column(name, type, options = {})
  name = name.to_s
  type = type.to_sym if type
  options = options.dup

  index_options = options.delete(:index)
  index(name, index_options.is_a?(Hash) ? index_options : {}) if index_options
  @columns_hash[name] = new_column_definition(name, type, options)
  self
end

def new_column_definition(name, type, **options)
  type = aliased_types(type.to_s, type)
  options[:primary_key] ||= type == :primary_key
  options[:null] = false if options[:primary_key]
  create_column_definition(name, type, options)
end

def create_column_definition(name, type, options)
  ColumnDefinition.new(name, type, options)
end
</code></pre>

<p>除了 <code>ColumnDefinition</code> 之外，在 ActiveRecord 中还存在 <code>PrimaryKeyDefinition</code>、<code>IndexDefinition</code> 等等类和结构体用于表示数据库中的某一种元素。</p>

<p>表结构在最后会被 <code>SchemaCreation</code> 类的 <code>#accept</code> 方法展开，最后在数据库中执行。</p>

<h3 id="小结-3">小结</h3>

<p>到这里整个 Migrations 部分的实现就已经阅读分析完了，整个『模块』包含两个部分，一部分是 rake 任务执行 DSL 代码的过程，另一部分是 DSL 的实现，两部分的结合最终构成了整个 Migrations 模块的全部内容。</p>

<p>ActiveRecord 对于 Migration 迁移机制的设计确实很好的解决数据库中的表结构不断变更的问题，同时因为所有的 Migration 文件都在版本控制中管理，我们也能够随时还原数据库中的表结构。</p>

<h2 id="总结">总结</h2>

<p>文章对 ActiveRecord 中涉及的很多问题都进行了分析和介绍，包括模型的创建、查询以及关系，还包括数据库表迁移的实现，本来想将文中的几个部分分开进行介绍，但是写着写着就懒得分开了，如果对文章的内容有疑问，请在博客下面的评论系统中留言。</p>

<blockquote>

</blockquote>


  ]]></description>
</item>

<item>
  <title>如何从 MongoDB 迁移到 MySQL</title>
  <link>//mongodb-to-mysql</link>
  <author>nju520</author>
  <pubDate>2017-10-10T00:00:00+08:00</pubDate>
  <guid>//mongodb-to-mysql</guid>
  <description><![CDATA[
  <p>最近的一个多月时间其实都在做数据库的迁移工作，我目前在开发的项目其实在上古时代是使用 MySQL 作为主要数据库的，后来由于一些业务上的原因从 MySQL 迁移到了 MongoDB，使用了几个月的时间后，由于数据库服务非常不稳定，再加上无人看管，同时 MongoDB 本身就是无 Schema 的数据库，最后导致数据库的脏数据问题非常严重。目前团队的成员没有较为丰富的 Rails 开发经验，所以还是希望使用 ActiveRecord 加上 Migration 的方式对数据进行一些强限制，保证数据库中数据的合法。</p>

<p><img src="https://img.nju520.me/2017-10-10-mysql-and-mongodb.png" alt="mysql-and-mongodb" /></p>

<p>文中会介绍作者在迁移数据库的过程中遇到的一些问题，并为各位读者提供需要<strong>停机</strong>迁移数据库的可行方案，如果需要不停机迁移数据库还是需要别的方案来解决，在这里提供的方案用于百万数据量的 MongoDB，预计的停机时间在两小时左右，如果数据量在千万级别以上，过长的停机时间可能是无法接受的，应该设计不停机的迁移方案；无论如何，作者希望这篇文章能够给想要做数据库迁移的开发者带来一些思路，少走一些坑。</p>

<h2 id="从关系到文档">从关系到文档</h2>

<p>虽然这篇文章的重点是从 MongoDB 迁移到 MySQL，但是作者还是想简单提一下从 MySQL 到 MongoDB 的迁移，如果我们仅仅是将 MySQL 中的全部数据导入到 MongoDB 中其实是一间比较简单的事情，其中最重要的原因就是 <strong>MySQL 支持的数据类型是 MongoDB 的子集</strong>：</p>

<p><img src="https://img.nju520.me/2017-10-10-mongodb-mysql-datatype-relation.png" alt="mongodb-mysql-datatype-relation" /></p>

<p>在迁移的过程中可以将 MySQL 中的全部数据以 csv 的格式导出，然后再将所有 csv 格式的数据使用 <code>mongoimport</code> 全部导入到 MongoDB 中：</p>

<pre><code class="language-shell">$ mysqldump -u&lt;username&gt; -p&lt;password&gt; \
    -T &lt;output_directory&gt; \
    --fields-terminated-by ',' \
    --fields-enclosed-by '\"' \
    --fields-escaped-by '\' \
    --no-create-info &lt;database_name&gt;

$ mongoimport --db &lt;database_name&gt; --collection &lt;collection_name&gt; \
    --type csv \
    --file &lt;data.csv&gt; \
    --headerline
</code></pre>

<p>虽然整个过程看起来只需要两个命令非常简单，但是等到你真要去做的时候你会遇到非常多的问题，作者没有过从 MySQL 或者其他关系型数据库迁移到 MongoDB 的经验，但是 Google 上相关的资料特别多，所以这总是一个有无数前人踩过坑的问题，而前人的经验也能够帮助我们节省很多时间。</p>

<p><img src="https://img.nju520.me/2017-10-24-mysql-to-mongodb.png" alt="mysql-to-mongodb" /></p>

<blockquote>
  <p>使用 csv 的方式导出数据在绝大多数的情况都不会出现问题，但是如果数据库中的某些文档中存储的是富文本，那么虽然在导出数据时不会出现问题，最终导入时可能出现一些比较奇怪的错误。</p>
</blockquote>

<h2 id="从文档到关系">从文档到关系</h2>

<p>相比于从 MySQL 到 MongoDB 的迁移，反向的迁移就麻烦了不止一倍，这主要是因为 MongoDB 中的很多数据类型和集合之间的关系在 MySQL 中都并不存在，比如嵌入式的数据结构、数组和哈希等集合类型、多对多关系的实现，很多的问题都不是仅仅能通过数据上的迁移解决的，我们需要在对数据进行迁移之前先对部分数据结构进行重构，本文中的后半部分会介绍需要处理的数据结构和逻辑。</p>

<p><img src="https://img.nju520.me/2017-10-10-mongodb-mysql-problems-to-be-solved.png" alt="mongodb-mysql-problems-to-be-solved" /></p>

<p>当我们准备将数据库彻底迁移到 MySQL 之前，需要做一些准备工作，将最后迁移所需要的工作尽可能地减少，保证停机的时间不会太长，准备工作的目标就是尽量消灭工程中复杂的数据结构。</p>

<h3 id="数据的预处理">数据的预处理</h3>

<p>在进行迁移之前要做很多准备工作，第一件事情是要把所有嵌入的数据结构改成非嵌入式的数据结构：</p>

<p><img src="https://img.nju520.me/2017-10-10-embedded-reference-documents.png" alt="embedded-reference-documents" /></p>

<p>也就是把所有 <code>embeds_many</code> 和 <code>embeds_one</code> 的关系都改成 <code>has_many</code> 和 <code>has_one</code>，同时将 <code>embedded_in</code> 都替换成 <code>belongs_to</code>，同时我们需要将工程中对应的测试都改成这种引用的关系，然而只改变代码中的关系并没有真正改变 MongoDB 中的数据。</p>

<pre><code class="language-ruby">def embeds_many_to_has_many(parent, child)
  child_key_name = child.to_s.underscore.pluralize
  parent.collection.find({}).each do |parent_document|
    next unless parent_document[child_key_name]
    parent_document[child_key_name].each do |child_document|
      new_child = child_document.merge "#{parent.to_s.underscore}_id": parent_document['_id']
      child.collection.insert_one new_child
    end
  end
  parent.all.unset(child_key_name.to_sym)
end

embeds_many_to_has_many(Person, Address)
</code></pre>

<p>我们可以使用上述的代码将关系为嵌入的模型都转换成引用，拍平所有复杂的数据关系，这段代码的运行时间与嵌入关系中的两个模型的数量有关，需要注意的是，MongoDB 中嵌入模型的数据可能因为某些原因出现相同的 <code>_id</code> 在插入时会发生冲突导致崩溃，你可以对 <code>insert_one</code> 使用 <code>resuce</code> 来保证这段代码的运行不会因为上述原因而停止。</p>

<p><img src="https://img.nju520.me/2017-10-10-embedded-to-reference.png" alt="embedded-to-reference" /></p>

<p>通过这段代码我们就可以轻松将原有的嵌入关系全部展开变成引用的关系，将嵌入的关系变成引用除了做这两个改变之外，不需要做其他的事情，无论是数据的查询还是模型的创建都不需要改变代码的实现，不过记得为子模型中父模型的外键<strong>添加索引</strong>，否则会导致父模型在获取自己持有的全部子模型时造成<strong>全表扫描</strong>：</p>

<pre><code class="language-ruby">class Comment
  include Mongoid::Document
  index post_id: 1
  belongs_to :post
end
</code></pre>

<p>在处理了 MongoDB 中独有的嵌入式关系之后，我们就需要解决一些复杂的集合类型了，比如数组和哈希，如果我们使用 MySQL5.7 或者 PostgreSQL 的话，其实并不需要对他们进行处理，因为最新版本的 MySQL 和 PostgreSQL 已经提供了对 JSON 的支持，不过作者还是将项目中的数组和哈希都变成了常见的数据结构。</p>

<p>在这个可选的过程中，其实并没有什么标准答案，我们可以根据需要将不同的数据转换成不同的数据结构：</p>

<p><img src="https://img.nju520.me/2017-10-10-array-to-string-or-relation.png" alt="array-to-string-or-relation" /></p>

<p>比如，将数组变成字符串或者一对多关系，将哈希变成当前文档的键值对等等，如何处理这些集合数据其实都要看我们的业务逻辑，在改变这些字段的同时尽量为上层提供一个与原来直接 <code>.tags</code> 或者 <code>.categories</code> 结果相同的 API：</p>

<pre><code class="language-ruby">class Post
  ...
  def tag_titles
    tags.map(&amp;:title)
  end
  
  def split_categories
    categories.split(',')
  end
end
</code></pre>

<p>这一步其实也是可选的，上述代码只是为了减少其他地方的修改负担，当然如果你想使用 MySQL5.7 或者 PostgreSQL 数据库对 JSON 的支持也没有什么太大的问题，只是在查询集合字段时有一些不方便。</p>

<h3 id="mongoid-的小兄弟们">Mongoid 的『小兄弟』们</h3>

<p>在使用 Mongoid 进行开发期间难免会用到一些相关插件，比如 <a href="https://github.com/thetron/mongoid-enum">mongoid-enum</a>、<a href="https://github.com/mongoid/mongoid-slug">mongoid-slug</a> 和 <a href="https://github.com/mongoid/mongoid-history">mongoid-history</a> 等，这些插件的实现与 ActiveRecord 中具有相同功能的插件在实现上有很大的不同。</p>

<p>对于有些插件，比如 mongoid-slug 只是在引入插件的模型的文档中插入了 <code>_slugs</code> 字段，我们只需要在进行数据迁移忽略这些添加的字段并将所有的 <code>#slug</code> 方法改成 <code>#id</code>，不需要在预处理的过程中做其它的改变。而枚举的实现在 Mongoid 的插件和 ActiveRecord 中就截然不同了：</p>

<p><img src="https://img.nju520.me/2017-10-10-mongodb-mysql-enum.png" alt="mongodb-mysql-enu" /></p>

<p>mongoid-enum 使用字符串和 <code>_status</code> 来保存枚举类型的字段，而 ActiveRecord 使用整数和 <code>status</code> 表示枚举类型，两者在底层数据结构的存储上有一些不同，我们会在之后的迁移脚本中解决这个问题。</p>

<p><img src="https://img.nju520.me/2017-10-10-mongoid-activerecord-enum.png" alt="mongoid-activerecord-enum" /></p>

<p>如果在项目中使用了很多 Mongoid 的插件，由于其实现不同，我们也只能根据不同的插件的具体实现来决定如何对其进行迁移，如果使用了一些支持特殊功能的插件可能很难在 ActiveRecord 中找到对应的支持，在迁移时可以考虑暂时将部分不重要的功能移除。</p>

<h3 id="主键与-uuid">主键与 UUID</h3>

<p>我们希望从 MongoDB 迁移到 MySQL 的另一个重要原因就是 MongoDB 每一个文档的主键实在是太过冗长，一个 32 字节的 <code>_id</code> 无法给我们提供特别多的信息，只能增加我们的阅读障碍，再加上项目中并没有部署 MongoDB 集群，所以没能享受到用默认的 UUID 生成机制带来的好处。</p>

<p><img src="https://img.nju520.me/2017-10-10-mongodb-mysql-id.png" alt="mongodb-mysql-id" /></p>

<p>我们不仅没有享受到 UUID 带来的优点，它还在迁移 MySQL 的过程中为我们带来了很大的麻烦，一方面是因为 ActiveRecord 的默认主键是整数，不支持 32 字节长度的 UUID，如果我们想要不改变 MongoDB 的 UUID，直接迁移到 MySQL 中使用其实也没有什么问题，只是我们要将默认的整数类型的主键变成字符串类型，同时要使用一个 UUID 生成器来保证所有的主键都是根据时间递增的并且不会冲突。</p>

<p>如果准备使用 UUID 加生成器的方式，其实会省去很多迁移的时间，不过看起来确实不是特别的优雅，如何选择还是要权衡和评估，但是如果我们选择了使用 <code>integer</code> 类型的自增主键时，就需要做很多额外的工作了，首先是为所有的表添加 <code>uuid</code> 字段，同时为所有的外键例如 <code>post_id</code> 创建对应的 <code>post_uuid</code> 字段，通过 <code>uuid</code> 将两者关联起来：</p>

<p><img src="https://img.nju520.me/2017-10-10-mysql-before-migrations.png" alt="mysql-before-migrations" /></p>

<p>在数据的迁移过程中，我们会将原有的 <code>_id</code> 映射到 <code>uuid</code> 中，<code>post_id</code> 映射到 <code>post_uuid</code> 上，我们通过保持 <code>uuid</code> 和 <code>post_uuid</code> 之间的关系保证模型之间的关系没有丢失，在迁移数据的过程中 <code>id</code> 和 <code>post_id</code> 是完全不存在任何联系的。</p>

<p>当我们按照 <code>_id</code> 的顺序遍历整个文档，将文档中的数据被插入到表中时，MySQL 会为所有的数据行自动生成的递增的主键 <code>id</code>，而 <code>post_id</code> 在这时都为空。</p>

<p><img src="https://img.nju520.me/2017-10-10-mysql-after-migrations.png" alt="mysql-after-migrations" /></p>

<p>在全部的数据都被插入到 MySQL 之后，我们通过 <code>#find_by_uuid</code> 查询的方式将 <code>uuid</code> 和 <code>post_uuid</code> 中的关系迁移到 <code>id</code> 和 <code>post_id</code> 中，并将与 <code>uuid</code> 相关的字段全部删除，这样我们能够保证模型之间的关系不会消失，并且数据行的相对位置与迁移前完全一致。</p>

<h3 id="代码的迁移">代码的迁移</h3>

<p>Mongoid 在使用时都是通过 <code>include</code> 将相关方法加载到当前模型中的，而 ActiveRecord 是通过继承 <code>ActiveRecord::Base</code> 的方式使用的，完成了对数据的预处理，我们就可以对现有模型层的代码进行修改了。</p>

<p>首先当然是更改模型的『父类』，把所有的 <code>Mongoid::Document</code> 都改成 <code>ActiveRecord::Base</code>，然后创建类对应的 Migration 迁移文件：</p>

<pre><code class="language-ruby"># app/models/post.rb
class Post &lt; ActiveRecord::Base
  validate_presence_of :title, :content
end

# db/migrate/20170908075625_create_posts.rb
class CreatePosts &lt; ActiveRecord::Migration[5.1]
  def change
    create_table :posts do |t|
      t.string :title, null: false
      t.text :content, null: false
      t.string :uuid, null: false

      t.timestamps null: false
    end
    
    add_index :posts, :uuid, unique: true
  end
end
</code></pre>

<blockquote>
  <p>注意：要为每一张表添加类型为字符串的 <code>uuid</code> 字段，同时为 <code>uuid</code> 建立唯一索引，以加快通过 <code>uuid</code> 建立不同数据模型之间关系的速度。</p>
</blockquote>

<p>除了建立数据库的迁移文件并修改基类，我们还需要修改一些 <code>include</code> 的模块和 Mongoid 中独有的查询，比如使用 <code>gte</code> 或者 <code>lte</code> 的日期查询和使用正则进行模式匹配的查询，这些查询在 ActiveRecord 中的使用方式与 Mongoid 中完全不同，我们需要通过手写 SQL 来解决这些问题。</p>

<p><img src="https://img.nju520.me/2017-10-10-mongoid-to-activerecord-model-and-query.png" alt="mongoid-to-activerecord-model-and-query" /></p>

<p>除此之外，我们也需要处理一些复杂的模型关系，比如 Mongoid 中的 <code>inverse_of</code> 在 ActiveRecord 中叫做  <code>foreign_key</code> 等等，这些修改其实都并不复杂，只是如果想要将这部分的代码全部处理掉，就需要对业务逻辑进行详细地测试以保证不会有遗留的问题，这也就对我们项目的测试覆盖率有着比较高的要求了，不过我相信绝大多数的 Rails 工程都有着非常好的测试覆盖率，能够保证这一部分代码和逻辑能够顺利迁移，但是如果项目中完全没有测试或者测试覆盖率很低，就只能人肉进行测试或者自求多福了，或者<strong>就别做迁移了，多写点测试再考虑这些重构的事情吧</strong>。</p>

<h3 id="数据的迁移">数据的迁移</h3>

<p>为每一个模型创建对应的迁移文件并建表其实一个不得不做的体力活，虽然有一些工作我们没法省略，但是我们可以考虑使用自动化的方式为所有的模型添加 <code>uuid</code> 字段和索引，同时也为类似 <code>post_id</code> 的字段添加相应的 <code>post_uuid</code> 列：</p>

<pre><code class="language-ruby">class AddUuidColumns &lt; ActiveRecord::Migration[5.1]
  def change
    Rails.application.eager_load!
    ActiveRecord::Base.descendants.map do |klass|
      # add `uuid` column and create unique index on `uuid`.
      add_column klass.table_name, :uuid, :string, unique: true
      add_index klass.table_name, unique: true
      
      # add `xxx_uuid` columns, ex: `post_uuid`, `comment_uuid` and etc.
      uuids = klass.attribute_names
        .select { |attr| attr.include? '_id' }
        .map    { |attr| attr.gsub '_id', '_uuid' }
      next unless uuids.present?
      uuids.each do |uuid|
        add_column klass.table_name, uuid, :string
      end
    end
  end
end
</code></pre>

<p>在添加 <code>uuid</code> 列并建立好索引之后，我们就可以开始对数据库进行迁移了，如果我们决定在迁移的过程中改变原有数据的主键，那么我们会将迁移分成两个步骤，数据的迁移和关系的重建，前者仅指将 MongoDB 中的所有数据全部迁移到 MySQL 中对应的表中，并将所有的 <code>_id</code> 转换成 <code>uuid</code>、<code>xx_id</code> 转换成 <code>xx_uuid</code>，而后者就是前面提到的：通过 <code>uuid</code> 和 <code>xx_uuid</code> 的关联重新建立模型之间的关系并在最后删除所有的 <code>uuid</code> 字段。</p>

<p>我们可以使用如下的代码对数据进行迁移，这段代码从 MongoDB 中遍历某个集合 Collection 中的全部数据，然后将文档作为参数传入 block，然后再分别通过 <code>DatabaseTransformer#delete_obsolete_columns</code> 和 <code>DatabaseTransformer#update_rename_columns</code> 方法删除部分已有的列、更新一些数据列最后将所有的 <code>id</code> 列都变成 <code>uuid</code>：</p>

<pre><code class="language-ruby">module DatabaseTransformer
  def import(collection_name, *obsolete_columns, **rename_columns)
    collection = Mongoid::Clients.default.collections.select do |c|
      c.namespace == "#{database}.#{collection_name.to_s.pluralize}"
    end.first

    unless collection.present?
      STDOUT.puts "#{collection_name.to_s.yellow}: skipped"
      STDOUT.puts
      return
    end

    constant = collection_name.to_s.singularize.camelcase.constantize
    reset_callbacks constant

    DatabaseTransformer.profiling do
      collection_count = collection.find.count
      collection.find.each_with_index do |document, index|
        document = yield document if block_given?
        delete_obsolete_columns document, obsolete_columns
        update_rename_columns document, rename_columns
        update_id_columns document

        insert_record constant, document
        STDOUT.puts "#{index}/#{collection_count}\n" if (index % 1000).zero?
      end
    end
  end
end
</code></pre>

<p>当完成了对文档的各种操作之后，该方法会直接调用 <code>DatabaseTransformer#insert_record</code> 将数据插入 MySQL 对应的表中；我们可以直接使用如下的代码将某个 Collection 中的全部文档迁移到 MySQL 中：</p>

<pre><code class="language-ruby">transformer = DatabaseTransformer.new 'hacker_production'
transformer.import :post, :_slugs, name: :title, _status: :status
</code></pre>

<p>上述代码会在迁移时将集合每一个文档的 <code>_slugs</code> 字段全部忽略，同时将 <code>name</code> 重命名成 <code>title</code>、<code>_status</code> 重命名成 <code>status</code>，虽然作为枚举类型的字段 mongoid-enum 和 ActiveRecord 的枚举类型完全不同，但是在这里可以直接插入也没有什么问题，ActiveRecord 的模型在创建时会自己处理字符串和整数之间的转换：</p>

<pre><code class="language-ruby">def insert_record(constant, params)
  model = constant.new params
  model.save! validate: false
rescue Exception =&gt; exception
  STDERR.puts "Import Error: #{exception}"
  raise exception
end
</code></pre>

<p>为了加快数据的插入速度，同时避免所有由于插入操作带来的副作用，我们会在数据迁移期间重置所有的回调：</p>

<pre><code class="language-ruby">def reset_callbacks(constant)
  %i(create save update).each do |callback|
    constant.reset_callbacks callback
  end
end
</code></pre>

<p>这段代码的作用仅在这个脚本运行的过程中才会生效，不会对工程中的其他地方造成任何的影响；同时，该脚本会在每 1000 个模型插入成功后向标准输出打印当前进度，帮助我们快速发现问题和预估迁移的时间。</p>

<blockquote>
  <p>你可以在 <a href="https://gist.github.com/nju520/10476fe67a10128a37ba27a4c6967d07">database_transformer.rb</a> 找到完整的数据迁移代码。</p>
</blockquote>

<p>将所有的数据全部插入到 MySQL 的表之后，模型之间还没有任何显式的关系，我们还需要将通过 <code>uuid</code> 连接的模型转换成使用 <code>id</code> 的方式，对象之间的关系才能通过点语法直接访问，关系的建立其实非常简单，我们获得当前类所有结尾为 <code>_uuid</code> 的属性，然后遍历所有的数据行，根据 <code>uuid</code> 的值和 <code>post_uuid</code> 属性中的 “post” 部分获取到表名，最终得到对应的关联模型，在这里我们也处理了类似多态的特殊情况：</p>

<pre><code class="language-ruby">module RelationBuilder
  def build_relations(class_name, polymorphic_associations = [], rename_associations = {})
    uuids = class_name.attribute_names.select { |name| name.end_with? '_uuid' }

    unless uuids.present?
      STDOUT.puts "#{class_name.to_s.yellow}: skipped"
      STDOUT.puts
      return
    end

    reset_callbacks class_name

    RelationBuilder.profiling do
      models_count = class_name.count
      class_name.unscoped.all.each_with_index do |model, index|
        update_params = uuids.map do |uuid|
          original_association_name = uuid[0...-5]

          association_model = association_model(
            original_association_name,
            model[uuid],
            polymorphic_associations,
            rename_associations
          )

          [original_association_name.to_s, association_model]
        end.compact

        begin
          Hash[update_params].each do |key, value|
            model.send "#{key}=", value
          end
          model.save! validate: false
        rescue Exception =&gt; e
          STDERR.puts e
          raise e
        end

        STDOUT.puts "#{index}/#{models_count}\n" if (counter % 1000).zero?
      end
    end
  end
end
</code></pre>

<p>在查找到对应的数据行之后就非常简单了，我们调用对应的 <code>post=</code> 等方法更新外键最后直接将外键的值保存到数据库中，与数据的迁移过程一样，我们在这段代码的执行过程中也会打印出当前的进度。</p>

<p>在初始化 <code>RelationBuilder</code> 时，如果我们传入了 <code>constants</code>，那么在调用 <code>RelationBuilder#build!</code> 时就会重建其中的全部关系，但是如果没有传入就会默认加载 ActiveRecord 中所有的子类，并去掉其中包含 <code>::</code> 的模型，也就是 ActiveRecord 中使用 <code>has_and_belongs_to_many</code> 创建的中间类，我们会在下一节中介绍如何单独处理多对多关系：</p>

<pre><code class="language-ruby">def initialize(constants = [])
  if constants.present?
    @constants = constants
  else
    Rails.application.eager_load!
    @constants = ActiveRecord::Base.descendants
        .reject { |constant| constant.to_s.include?('::') }
  end
end
</code></pre>

<blockquote>
  <p>跟关系重建相关的代码可以在 <a href="https://gist.github.com/nju520/c0798fb1272f483a176fa67741a3f1ee">relation_builder.rb</a> 找到完整的用于关系迁移的代码。</p>
</blockquote>

<pre><code class="language-ruby">builder = RelationBuilder.new([Post, Comment])
builder.build!
</code></pre>

<p>通过这数据迁移和关系重建两个步骤就已经可以解决绝大部分的数据迁移问题了，但是由于 MongoDB 和 ActiveRecord 中对于多对多关系的处理比较特殊，所以我们需要单独进行解决，如果所有的迁移问题到这里都已经解决了，那么我们就可以使用下面的迁移文件将数据库中与 <code>uuid</code> 有关的全部列都删除了：</p>

<pre><code class="language-ruby">class RemoveAllUuidColumns &lt; ActiveRecord::Migration[5.1]
  def change
    Rails.application.eager_load!
    ActiveRecord::Base.descendants.map do |klass|
      attrs = klass.attribute_names.select { |n| n.include? 'uuid' }
      next unless attrs.present?
      remove_columns klass.table_name, *attrs
    end
  end
end
</code></pre>

<p>到这里位置整个迁移的过程就基本完成了，接下来就是跟整个迁移过程中有关的其他事项，例如：对多对关系、测试的重要性等话题。</p>

<h3 id="多对多关系的处理">多对多关系的处理</h3>

<p>多对多关系在数据的迁移过程中其实稍微有一些复杂，在 Mongoid 中使用 <code>has_and_belongs_to_many</code> 会在相关的文档下添加一个 <code>tag_ids</code> 或者 <code>post_ids</code> 数组：</p>

<pre><code class="language-ruby"># The post document.
{
  "_id" : ObjectId("4d3ed089fb60ab534684b7e9"),
  "tag_ids" : [
    ObjectId("4d3ed089fb60ab534684b7f2"), 
    ObjectId("4d3ed089fb60ab53468831f1")
  ],
  "title": "xxx",
  "content": "xxx"
}
</code></pre>

<p>而 ActiveRecord 中会建立一张单独的表，表的名称是两张表名按照字母表顺序的拼接，如果是 <code>Post</code> 和 <code>Tag</code>，对应的多对多表就是 <code>posts_tags</code>，除了创建多对多表，<code>has_and_belongs_to_many</code> 还会创建两个 <code>ActiveRecord::Base</code> 的子类 <code>Tag::HABTM_Posts</code> 和 <code>Post::HABTM_Tags</code>，我们可以使用下面的代码简单实验一下：</p>

<pre><code class="language-ruby">require 'active_record'

class Tag &lt; ActiveRecord::Base; end
class Post &lt; ActiveRecord::Base
  has_and_belongs_to_many :tags
end
class Tag &lt; ActiveRecord::Base
  has_and_belongs_to_many :posts
end
puts ActiveRecord::Base.descendants
# =&gt; [Tag, Post, Post::HABTM_Tags, Tag::HABTM_Posts]
</code></pre>

<p>上述代码打印出了两个 <code>has_and_belongs_to_many</code> 生成的类 <code>Tag::HABTM_Posts</code> 和 <code>Post::HABTM_Tags</code>，它们有着完全相同的表 <code>posts_tags</code>，处理多对多关系时，我们只需要在使用 <code>DatabaseTransformer</code> 导入表中的所有的数据之后，再通过遍历 <code>posts_tags</code> 表中的数据更新多对多的关系表就可以了：</p>

<pre><code class="language-ruby">class PostsTag &lt; ActiveRecord::Base; end

# migrate data from mongodb to mysql.
transformer = DatabaseTransformer.new 'hacker_production'
transformer.import :posts_tags

# establish association between posts and tags.
PostsTag.unscoped.all.each do |model|
  post = Post.find_by_uuid model.post_uuid
  tag = Tag.find_by_uuid model.tag_uuid
  next unless post.present? &amp;&amp; tag.present?
  model.update_columns post_id: post.id, tag_id: tag.id
end
</code></pre>

<p>所有使用 <code>has_and_belongs_to_many</code> 的多对多关系都需要通过上述代码进行迁移，这一步需要在删除数据库中的所有 <code>uuid</code> 字段之前完成。</p>

<h3 id="测试的重要性">测试的重要性</h3>

<p>在真正对线上的服务进行停机迁移之前，我们其实需要对数据库已有的数据进行部分和全量测试，在部分测试阶段，我们可以在本地准备一个数据量为生产环境数据量 1/10 或者 1/100 的 MongoDB 数据库，通过在本地模拟 MongoDB 和 MySQL 的环境进行预迁移，确保我们能够尽快地发现迁移脚本中的错误。</p>

<p><img src="https://img.nju520.me/2017-10-10-mongodb-pre-migration.png" alt="mongodb-pre-migration" /></p>

<p>准备测试数据库的办法是通过关系删除一些主要模型的数据行，在删除时可以通过 MongoDB 中的 <code>dependent: :destroy</code> 删除相关的模型，这样可以尽可能的保证数据的一致性和完整性，但是在对线上数据库进行迁移之前，我们依然需要对 MongoDB 中的全部数据进行全量的迁移测试，这样可以发现一些更加隐蔽的问题，保证真正上线时可以出现更少的状况。</p>

<p>数据库的迁移其实也属于重构，在进行 MongoDB 的数据库迁移之前一定要保证项目有着完善的测试体系和测试用例，这样才能让我们在项目重构之后，确定不会出现我们难以预料的问题，整个项目才是可控的，如果工程中没有足够的测试甚至没有测试，那么就不要再说重构这件事情了 – <strong>单元测试是重构的基础</strong>。</p>

<h2 id="总结">总结</h2>

<p>如何从 MongoDB 迁移到 MySQL 其实是一个工程问题，我们需要在整个过程中不断寻找可能出错的问题，将一个比较复杂的任务进行拆分，在真正做迁移之前尽可能地减少迁移对服务可用性以及稳定性带来的影响。</p>

<p><img src="https://img.nju520.me/2017-10-10-mysql-and-mongodb-work-together.png" alt="mysql-and-mongodb-work-together" /></p>

<p>除此之外，MongoDB 和 MySQL 之间的选择也不一定是非此即彼，我们将项目中的大部分数据都迁移到了 MySQL 中，但是将一部分用于计算和分析的数据留在了 MongoDB，这样就可以保证 MongoDB 宕机之后仍然不会影响项目的主要任务，同时，MySQL 的备份和恢复速度也会因为数据库变小而非常迅速。</p>

<p>最后一点，测试真的很重要，如果没有测试，没有人能够做到在<strong>修改大量的业务代码的过程中不丢失任何的业务逻辑</strong>，甚至如果没有测试，很多业务逻辑可能在开发的那一天就已经丢失了。</p>

<p>如果对文章的内容有疑问或者有 MongoDB 迁移相关的问题，可以在评论中留言。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://www.quora.com/How-do-I-migrate-data-from-a-MongoDB-to-MySQL-database-Can-it-be-done-in-a-real-time-scenario-What-are-the-pros-and-cons-for-each-migration-Which-one-do-you-advice-What-is-your-experience-Any-reference-DB-expert-who-can-do-it">How do I migrate data from a MongoDB to MySQL database? · Quora</a></li>
</ul>

  ]]></description>
</item>

<item>
  <title>浅谈数据库并发控制 - 锁和 MVCC</title>
  <link>//database-concurrency-control</link>
  <author>nju520</author>
  <pubDate>2017-10-01T00:00:00+08:00</pubDate>
  <guid>//database-concurrency-control</guid>
  <description><![CDATA[
  <p>在学习几年编程之后，你会发现所有的问题都没有简单、快捷的解决方案，很多问题都需要权衡和妥协，而本文介绍的就是数据库在并发性能和可串行化之间做的权衡和妥协 - 并发控制机制。</p>

<p><img src="https://img.nju520.me/2017-10-02-tradeoff-between-performance-and-serializability.png" alt="tradeoff-between-performance-and-serializability" /></p>

<p>如果数据库中的所有事务都是串行执行的，那么它非常容易成为整个应用的性能瓶颈，虽然说没法水平扩展的节点在最后都会成为瓶颈，但是串行执行事务的数据库会加速这一过程；而并发（Concurrency）使一切事情的发生都有了可能，它能够解决一定的性能问题，但是它会带来更多诡异的错误。</p>

<p>引入了并发事务之后，如果不对事务的执行进行控制就会出现各种各样的问题，你可能没有享受到并发带来的性能提升就已经被各种奇怪的问题折磨的欲仙欲死了。</p>

<h2 id="概述">概述</h2>

<p>如何控制并发是数据库领域中非常重要的问题之一，不过到今天为止事务并发的控制已经有了很多成熟的解决方案，而这些方案的原理就是这篇文章想要介绍的内容，文章中会介绍最为常见的三种并发控制机制：</p>

<p><img src="https://img.nju520.me/2017-10-02-pessimistic-optimistic-multiversion-conccurency-control.png" alt="pessimistic-optimistic-multiversion-conccurency-control" /></p>

<p>分别是悲观并发控制、乐观并发控制和多版本并发控制，其中悲观并发控制其实是最常见的并发控制机制，也就是锁；而乐观并发控制其实也有另一个名字：乐观锁，乐观锁其实并不是一种真实存在的锁，我们会在文章后面的部分中具体介绍；最后就是多版本并发控制（MVCC）了，与前两者对立的命名不同，MVCC 可以与前两者中的任意一种机制结合使用，以提高数据库的读性能。</p>

<p>既然这篇文章介绍了不同的并发控制机制，那么一定会涉及到不同事务的并发，我们会通过示意图的方式分析各种机制是如何工作的。</p>

<h2 id="悲观并发控制">悲观并发控制</h2>

<p>控制不同的事务对同一份数据的获取是保证数据库的一致性的最根本方法，如果我们能够让事务在同一时间对同一资源有着独占的能力，那么就可以保证操作同一资源的不同事务不会相互影响。</p>

<p><img src="https://img.nju520.me/2017-10-02-pessimistic-conccurency-control.png" alt="pessimistic-conccurency-control" /></p>

<p>最简单的、应用最广的方法就是使用锁来解决，当事务需要对资源进行操作时需要先获得资源对应的锁，保证其他事务不会访问该资源后，在对资源进行各种操作；在悲观并发控制中，数据库程序对于数据被修改持悲观的态度，在数据处理的过程中都会被锁定，以此来解决竞争的问题。</p>

<h3 id="读写锁">读写锁</h3>

<p>为了最大化数据库事务的并发能力，数据库中的锁被设计为两种模式，分别是共享锁和互斥锁。当一个事务获得共享锁之后，它只可以进行读操作，所以共享锁也叫读锁；而当一个事务获得一行数据的互斥锁时，就可以对该行数据进行读和写操作，所以互斥锁也叫写锁。</p>

<p><img src="https://img.nju520.me/2017-10-02-Shared-Exclusive-Lock.png" alt="Shared-Exclusive-Lock" /></p>

<p>共享锁和互斥锁除了限制事务能够执行的读写操作之外，它们之间还有『共享』和『互斥』的关系，也就是多个事务可以同时获得某一行数据的共享锁，但是互斥锁与共享锁和其他的互斥锁并不兼容，我们可以很自然地理解这么设计的原因：多个事务同时写入同一数据难免会发生各种诡异的问题。</p>

<p><img src="https://img.nju520.me/2017-10-02-lock-and-wait.png" alt="lock-and-wait" /></p>

<p>如果当前事务没有办法获取该行数据对应的锁时就会陷入等待的状态，直到其他事务将当前数据对应的锁释放才可以获得锁并执行相应的操作。</p>

<h3 id="两阶段锁协议">两阶段锁协议</h3>

<p>两阶段锁协议（2PL）是一种能够保证事务可串行化的协议，它将事务的获取锁和释放锁划分成了增长（Growing）和缩减（Shrinking）两个不同的阶段。</p>

<p><img src="https://img.nju520.me/2017-10-02-growing-to-shrinking.png" alt="growing-to-shrinking" /></p>

<p>在增长阶段，一个事务可以获得锁但是不能释放锁；而在缩减阶段事务只可以释放锁，并不能获得新的锁，如果只看 2PL 的定义，那么到这里就已经介绍完了，但是它还有两个变种：</p>

<ol>
  <li><strong>Strict 2PL</strong>：事务持有的<strong>互斥</strong>锁必须在提交后再释放；</li>
  <li><strong>Rigorous 2PL</strong>：事务持有的<strong>所有</strong>锁必须在提交后释放；</li>
</ol>

<p><img src="https://img.nju520.me/2017-10-02-two-phase-locking.png" alt="two-phase-locking" /></p>

<p>虽然锁的使用能够为我们解决不同事务之间由于并发执行造成的问题，但是两阶段锁的使用却引入了另一个严重的问题，死锁；不同的事务等待对方已经锁定的资源就会造成死锁，我们在这里举一个简单的例子：</p>

<p><img src="https://img.nju520.me/2017-10-02-deadlock.png" alt="deadlock" /></p>

<p>两个事务在刚开始时分别获取了 hacker 和 beacon 资源上面的锁，然后再请求对方已经获得的锁时就会发生死锁，双方都没有办法等到锁的释放，如果没有死锁的处理机制就会无限等待下去，两个事务都没有办法完成。</p>

<h3 id="死锁的处理">死锁的处理</h3>

<p>死锁在多线程编程中是经常遇到的事情，一旦涉及多个线程对资源进行争夺就需要考虑当前的几个线程或者事务是否会造成死锁；解决死锁大体来看有两种办法，一种是从源头杜绝死锁的产生和出现，另一种是允许系统进入死锁的状态，但是在系统出现死锁时能够及时发现并且进行恢复。</p>

<p><img src="https://img.nju520.me/2017-10-02-deadlock-handling.png" alt="deadlock-handling" /></p>

<h4 id="预防死锁">预防死锁</h4>

<p>有两种方式可以帮助我们预防死锁的出现，一种是保证事务之间的等待不会出现环，也就是事务之间的等待图应该是一张<strong>有向无环图</strong>，没有循环等待的情况或者保证一个事务中想要获得的所有资源都在事务开始时以原子的方式被锁定，所有的资源要么被锁定要么都不被锁定。</p>

<p>但是这种方式有两个问题，在事务一开始时很难判断哪些资源是需要锁定的，同时因为一些很晚才会用到的数据被提前锁定，数据的利用率与事务的并发率也非常的低。一种解决的办法就是按照一定的顺序为所有的数据行加锁，同时与 2PL 协议结合，在加锁阶段保证所有的数据行都是从小到大依次进行加锁的，不过这种方式依然需要事务提前知道将要加锁的数据集。</p>

<p>另一种预防死锁的方法就是使用抢占加事务回滚的方式预防死锁，当事务开始执行时会先获得一个时间戳，数据库程序会根据事务的时间戳决定事务应该等待还是回滚，在这时也有两种机制供我们选择，一种是 wait-die 机制：</p>

<p><img src="https://img.nju520.me/2017-10-02-deadlock-prevention-wait-die.png" alt="deadlock-prevention-wait-die" /></p>

<p>当执行事务的时间戳小于另一事务时，即事务 A 先于 B 开始，那么它就会等待另一个事务释放对应资源的锁，否则就会保持当前的时间戳并回滚。</p>

<p>另一种机制叫做 wound-wait，这是一种抢占的解决方案，它和 wait-die 机制的结果完全相反，当前事务如果先于另一事务执行并请求了另一事务的资源，那么另一事务会立刻回滚，将资源让给先执行的事务，否则就会等待其他事务释放资源：</p>

<p><img src="https://img.nju520.me/2017-10-02-deadlock-prevention-wound-wait.png" alt="deadlock-prevention-wound-wait" /></p>

<p>两种方法都会造成不必要的事务回滚，由此会带来一定的性能损失，更简单的解决死锁的方式就是使用超时时间，但是超时时间的设定是需要仔细考虑的，否则会造成耗时较长的事务无法正常执行，或者无法及时发现需要解决的死锁，所以它的使用还是有一定的局限性。</p>

<h3 id="死锁检测和恢复">死锁检测和恢复</h3>

<p>如果数据库程序无法通过协议从原理上保证死锁不会发生，那么就需要在死锁发生时及时检测到并从死锁状态恢复到正常状态保证数据库程序可以正常工作。在使用检测和恢复的方式解决死锁时，数据库程序需要维护数据和事务之间的引用信息，同时也需要提供一个用于判断当前数据库是否进入死锁状态的算法，最后需要在死锁发生时提供合适的策略及时恢复。</p>

<p>在上一节中我们其实提到死锁的检测可以通过一个有向的等待图来进行判断，如果一个事务依赖于另一个事务正在处理的数据，那么当前事务就会等待另一个事务的结束，这也就是整个等待图中的一条边：</p>

<p><img src="https://img.nju520.me/2017-10-02-deadlock-wait-for-graph.png" alt="deadlock-wait-for-graph" /></p>

<p>如上图所示，如果在这个有向图中出现了环，就说明当前数据库进入了死锁的状态 <code>TransB -&gt; TransE -&gt; TransF -&gt; TransD -&gt; TransB</code>，在这时就需要死锁恢复机制接入了。</p>

<p>如何从死锁中恢复其实非常简单，最常见的解决办法就是选择整个环中一个事务进行回滚，以打破整个等待图中的环，在整个恢复的过程中有三个事情需要考虑：</p>

<p><img src="https://img.nju520.me/2017-10-02-deadlock-recovery.png" alt="deadlock-recovery" /></p>

<p>每次出现死锁时其实都会有多个事务被波及，而选择其中哪一个任务进行回滚是必须要做的事情，在选择牺牲品（Victim）时的黄金原则就是<strong>最小化代价</strong>，所以我们需要综合考虑事务已经计算的时间、使用的数据行以及涉及的事务等因素；当我们选择了牺牲品之后就可以开始回滚了，回滚其实有两种选择一种是全部回滚，另一种是部分回滚，部分回滚会回滚到事务之前的一个检查点上，如果没有检查点那自然没有办法进行部分回滚。</p>

<blockquote>
  <p>在死锁恢复的过程中，其实还可能出现某些任务在多次死锁时都被选择成为牺牲品，一直都不会成功执行，造成饥饿（Starvation），我们需要保证事务会在有穷的时间内执行，所以要在选择牺牲品时将时间戳加入考虑的范围。</p>
</blockquote>

<h3 id="锁的粒度">锁的粒度</h3>

<p>到目前为止我们都没有对不同粒度的锁进行讨论，一直以来我们都讨论的都是数据行锁，但是在有些时候我们希望将多个节点看做一个数据单元，使用锁直接将这个数据单元、表甚至数据库锁定起来。这个目标的实现需要我们在数据库中定义不同粒度的锁：</p>

<p><img src="https://img.nju520.me/2017-10-02-granularity-hierarchy.png" alt="granularity-hierarchy" /></p>

<p>当我们拥有了不同粒度的锁之后，如果某个事务想要锁定整个数据库或者整张表时只需要简单的锁住对应的节点就会在当前节点加上显示（explicit）锁，在所有的子节点上加隐式（implicit）锁；虽然这种不同粒度的锁能够解决父节点被加锁时，子节点不能被加锁的问题，但是我们没有办法在子节点被加锁时，立刻确定父节点不能被加锁。</p>

<p>在这时我们就需要引入<em>意向锁</em>来解决这个问题了，当需要给子节点加锁时，先给所有的父节点加对应的意向锁，意向锁之间是完全不会互斥的，只是用来帮助父节点快速判断是否可以对该节点进行加锁：</p>

<p><img src="https://img.nju520.me/2017-10-02-lock-type-compatibility-matrix.png" alt="lock-type-compatibility-matrix" /></p>

<p>这里是一张引入了两种意向锁，<em>意向共享锁</em>和<em>意向互斥锁</em>之后所有的锁之间的兼容关系；到这里，我们通过不同粒度的锁和意向锁加快了数据库的吞吐量。</p>

<h2 id="乐观并发控制">乐观并发控制</h2>

<p>除了悲观并发控制机制 - 锁之外，我们其实还有其他的并发控制机制，<em>乐观并发控制</em>（Optimistic Concurrency Control）。乐观并发控制也叫乐观锁，但是它并不是真正的锁，很多人都会误以为乐观锁是一种真正的锁，然而它只是一种并发控制的思想。</p>

<p><img src="https://img.nju520.me/2017-10-02-pessimistic-and-optimistic.png" alt="pessimistic-and-optimisti" /></p>

<p>在这一节中，我们将会先介绍<em>基于时间戳的并发控制机制</em>，然后在这个协议的基础上进行扩展，实现乐观的并发控制机制。</p>

<h3 id="基于时间戳的协议">基于时间戳的协议</h3>

<p>锁协议按照不同事务对同一数据项请求的时间依次执行，因为后面执行的事务想要获取的数据已将被前面的事务加锁，只能等待锁的释放，所以基于锁的协议执行事务的顺序与获得锁的顺序有关。在这里想要介绍的基于时间戳的协议能够在事务执行之前先决定事务的执行顺序。</p>

<p>每一个事务都会具有一个全局唯一的时间戳，它即可以使用系统的时钟时间，也可以使用计数器，只要能够保证所有的时间戳都是唯一并且是随时间递增的就可以。</p>

<p><img src="https://img.nju520.me/2017-10-02-timestamp-ordering-protocol.png" alt="timestamp-ordering-protocol" /></p>

<p>基于时间戳的协议能够保证事务并行执行的顺序与事务按照时间戳串行执行的效果完全相同；每一个数据项都有两个时间戳，读时间戳和写时间戳，分别代表了当前成功执行对应操作的事务的时间戳。</p>

<p>该协议能够保证所有冲突的读写操作都能按照时间戳的大小串行执行，在执行对应的操作时不需要关注其他的事务只需要关心数据项对应时间戳的值就可以了：</p>

<p><img src="https://img.nju520.me/2017-10-02-timestamp-ordering-protocol-process.png" alt="timestamp-ordering-protocol-process" /></p>

<p>无论是读操作还是写操作都会从左到右依次比较读写时间戳的值，如果小于当前值就会直接被拒绝然后回滚，数据库系统会给回滚的事务添加一个新的时间戳并重新执行这个事务。</p>

<h3 id="基于验证的协议">基于验证的协议</h3>

<p><em>乐观并发控制</em>其实本质上就是基于验证的协议，因为在多数的应用中只读的事务占了绝大多数，事务之间因为写操作造成冲突的可能非常小，也就是说大多数的事务在不需要并发控制机制也能运行的非常好，也可以保证数据库的一致性；而并发控制机制其实向整个数据库系统添加了很多的开销，我们其实可以通过别的策略降低这部分开销。</p>

<p>而验证协议就是我们找到的解决办法，它根据事务的只读或者更新将所有事务的执行分为两到三个阶段：</p>

<p><img src="https://img.nju520.me/2017-10-02-validation-based-protocol.png" alt="validation-based-protoco" /></p>

<p>在读阶段，数据库会执行事务中的<strong>全部读操作和写操作</strong>，并将所有写后的值存入临时变量中，并不会真正更新数据库中的内容；在这时候会进入下一个阶段，数据库程序会检查当前的改动是否合法，也就是是否有其他事务在 RAED PHASE 期间更新了数据，如果通过测试那么直接就进入 WRITE PHASE 将所有存在临时变量中的改动全部写入数据库，没有通过测试的事务会直接被终止。</p>

<p>为了保证乐观并发控制能够正常运行，我们需要知道一个事务不同阶段的发生时间，包括事务开始时间、验证阶段的开始时间以及写阶段的结束时间；通过这三个时间戳，我们可以保证任意冲突的事务不会同时写入数据库，一旦由一个事务完成了验证阶段就会立即写入，其他读取了相同数据的事务就会回滚重新执行。</p>

<p>作为乐观的并发控制机制，它会假定所有的事务在最终都会通过验证阶段并且执行成功，而锁机制和基于时间戳排序的协议是悲观的，因为它们会在发生冲突时强制事务进行等待或者回滚，哪怕有不需要锁也能够保证事务之间不会冲突的可能。</p>

<h2 id="多版本并发控制">多版本并发控制</h2>

<p>到目前为止我们介绍的并发控制机制其实都是通过延迟或者终止相应的事务来解决事务之间的竞争条件（Race condition）来保证事务的可串行化；虽然前面的两种并发控制机制确实能够从根本上解决并发事务的可串行化的问题，但是在实际环境中数据库的事务大都是只读的，读请求是写请求的很多倍，如果写请求和读请求之前没有并发控制机制，那么最坏的情况也是读请求读到了已经写入的数据，这对很多应用完全是可以接受的。</p>

<p><img src="https://img.nju520.me/2017-10-02-multiversion-scheme.png" alt="multiversion-scheme" /></p>

<p>在这种大前提下，数据库系统引入了另一种并发控制机制 - <em>多版本并发控制</em>（Multiversion Concurrency Control），每一个写操作都会创建一个新版本的数据，读操作会从有限多个版本的数据中挑选一个最合适的结果直接返回；在这时，读写操作之间的冲突就不再需要被关注，而管理和快速挑选数据的版本就成了 MVCC 需要解决的主要问题。</p>

<p>MVCC 并不是一个与乐观和悲观并发控制对立的东西，它能够与两者很好的结合以增加事务的并发量，在目前最流行的 SQL 数据库 MySQL 和 PostgreSQL 中都对 MVCC 进行了实现；但是由于它们分别实现了悲观锁和乐观锁，所以 MVCC 实现的方式也不同。</p>

<h3 id="mysql-与-mvcc">MySQL 与 MVCC</h3>

<p>MySQL 中实现的多版本两阶段锁协议（Multiversion 2PL）将 MVCC 和 2PL 的优点结合了起来，每一个版本的数据行都具有一个唯一的时间戳，当有读事务请求时，数据库程序会直接从多个版本的数据项中具有最大时间戳的返回。</p>

<p><img src="https://img.nju520.me/2017-10-02-multiversion-2pl-read.png" alt="multiversion-2pl-read" /></p>

<p>更新操作就稍微有些复杂了，事务会先读取最新版本的数据计算出数据更新后的结果，然后创建一个新版本的数据，新数据的时间戳是目前数据行的最大版本 <code>＋1</code>：</p>

<p><img src="https://img.nju520.me/2017-10-02-multiversion-2pl-write.png" alt="multiversion-2pl-write" /></p>

<p>数据版本的删除也是根据时间戳来选择的，MySQL 会将版本最低的数据定时从数据库中清除以保证不会出现大量的遗留内容。</p>

<h3 id="postgresql-与-mvcc">PostgreSQL 与 MVCC</h3>

<p>与 MySQL 中使用悲观并发控制不同，PostgreSQL 中都是使用乐观并发控制的，这也就导致了 MVCC 在于乐观锁结合时的实现上有一些不同，最终实现的叫做多版本时间戳排序协议（Multiversion Timestamp Ordering），在这个协议中，所有的的事务在执行之前都会被分配一个唯一的时间戳，每一个数据项都有读写两个时间戳：</p>

<p><img src="https://img.nju520.me/2017-10-02-dataitem-with-timestamps.png" alt="dataitem-with-timestamps" /></p>

<p>当 PostgreSQL 的事务发出了一个读请求，数据库直接将最新版本的数据返回，不会被任何操作阻塞，而写操作在执行时，事务的时间戳一定要大或者等于数据行的读时间戳，否则就会被回滚。</p>

<p>这种 MVCC 的实现保证了读事务永远都不会失败并且不需要等待锁的释放，对于读请求远远多于写请求的应用程序，乐观锁加 MVCC 对数据库的性能有着非常大的提升；虽然这种协议能够针对一些实际情况做出一些明显的性能提升，但是也会导致两个问题，一个是每一次读操作都会更新读时间戳造成两次的磁盘写入，第二是事务之间的冲突是通过回滚解决的，所以如果冲突的可能性非常高或者回滚代价巨大，数据库的读写性能还不如使用传统的锁等待方式。</p>

<h2 id="总结">总结</h2>

<p>数据库的并发控制机制到今天已经有了非常成熟、完善的解决方案，我们并不需要自己去设计一套新的协议来处理不同事务之间的冲突问题，从数据库的并发控制机制中学习到的相关知识，无论是锁还是乐观并发控制在其他的领域或者应用中都被广泛使用，所以了解、熟悉不同的并发控制机制的原理是很有必要的。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://hwbnju.com/database-concurrency-control.html">浅谈数据库并发控制 - 锁和 MVCC · 雄关漫道真如铁</a></li>
  <li><a href="https://www.ibm.com/support/knowledgecenter/en/SSPK3V_7.0.0/com.ibm.swg.im.soliddb.sql.doc/doc/pessimistic.vs.optimistic.concurrency.control.html">PESSIMISTIC vs. OPTIMISTIC concurrency control</a></li>
  <li><a href="https://devcenter.heroku.com/articles/postgresql-concurrency">PostgreSQL Concurrency with MVCC</a></li>
  <li><a href="https://www.enterprisedb.com/well-known-databases-use-different-approaches-mvcc">Well-known Databases Use Different Approaches for MVCC</a></li>
  <li><a href="https://www.cs.unc.edu/~dewan/242/s01/notes/trans/node3.html">Serializability</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Race_condition">Race condition</a></li>
</ul>


  ]]></description>
</item>

<item>
  <title>MySQL 索引性能分析概要</title>
  <link>//sql-index-performance</link>
  <author>nju520</author>
  <pubDate>2017-09-16T00:00:00+08:00</pubDate>
  <guid>//sql-index-performance</guid>
  <description><![CDATA[
  <p>上一篇文章 <a href="http://nju520.me/sql-index-intro.html">MySQL 索引设计概要</a> 介绍了影响索引设计的几大因素，包括过滤因子、索引片的宽窄与大小以及匹配列和过滤列。在文章的后半部分介绍了 <a href="https://www.amazon.cn/图书/dp/B00ZH27RH0">数据库索引设计与优化</a> 一书中，理想的三星索引的设计流程和套路，到目前为止虽然我们掌握了单表索引的设计方法，但是却没有分析预估索引耗时的能力。</p>

<p><img src="https://img.nju520.me/2017-09-16-Proactive-Index-Design.jpg-1000width" alt="Proactive-Index-Design" /></p>

<p>在本文中，我们将介绍书中提到的两种分析索引性能的方法：基本问题法（BQ）和快速估算上限法（QUBE），这两种方法能够帮助我们快速分析、估算索引的性能，及时发现问题。</p>

<h2 id="基本问题法">基本问题法</h2>

<p>当我们需要考虑对现有的 SELECT 查询进行分析时，哪怕没有足够的时间，也应该使用基本问题法对查询进行评估，评估的内容非常简单：现有的索引或者即将添加的索引是否包含了 WHERE 中使用的全部列，也就是对于当前查询来说，是否有一个索引是半宽索引。</p>

<p><img src="https://img.nju520.me/2017-09-16-Semifat-Index-and-Fat-Index.jpg-1000width" alt="Semifat-Index-and-Fat-Index" /></p>

<p>在上一篇文章中，我们介绍过宽索引和窄索引，窄索引 (username) 其实就叫做半宽索引，其中包含了 WHERE 中的全部的列 username，当前索引的对于该查询只有一颗星，它虽然避免了无效的回表查询造成的随机 IO，但是如果当前的索引的性能仍然无法满足需要，就可以添加 age 将该索引变成宽索引 (username, age) 以此来避免回表访问造成的性能影响；对于上图中的简单查询，索引 (username, age) 其实已经是一个三星索引了，但是对于包含 ORDER BY 或者更加复杂的查询，(username, age) 可能就只是二星索引：</p>

<p><img src="https://img.nju520.me/2017-09-16-Complicated-Query-with-Order-By.jpg-1000width" alt="Complicated-Query-with-Order-By" /></p>

<p>在这时如果该索引仍然不能满足性能的需要，就可以考虑按照上一篇文章 <a href="http://nju520.me/sql-index-intro.html">MySQL 索引设计概要</a> 中提供的索引设计方法重新设计了。</p>

<blockquote>
  <p>虽然基本问题法能够快速解决一些由于索引造成的问题，但是它并不能保证足够的性能，当表中有 (city, username, age) 索引，谓词为 <code>WHERE username="nju520" AND age="21"</code> 时，使用基本问题法并不能得出正确的结果。</p>
</blockquote>

<h2 id="快速估算上限法">快速估算上限法</h2>

<p>基本问题法非常简单，它能够最短的时间内帮助我们评估一个查询的性能，但是它并不能准确地反映一个索引相关的性能问题，而快速估算上限法就是一种更加准确、复杂的方法了；其目的在于在程序开发期间就能将访问路径缓慢的问题暴露出来，这个估算方法的输出就是本地响应时间（Local Response Time）：</p>

<p><img src="https://img.nju520.me/2017-09-16-QUBE-LRT.jpg-1000width" alt="QUBE-LRT" /></p>

<p>本地响应时间就是查询在数据库服务器中的耗时，不包括任何的网络延迟和多层环境的通信时间，仅包括执行查询任务的耗时。</p>

<h3 id="响应时间">响应时间</h3>

<p>本地响应时间等于服务时间和排队时间的总和，一次查询请求需要在数据库中等待 CPU 以及磁盘的响应，也可能会因为其他事务正在对同样的数据进行读写，导致当前查询需要等待锁的获取，不过组成响应时间中的主要部分还是磁盘的服务时间：</p>

<p><img src="https://img.nju520.me/2017-09-16-Local-Response-Time.jpg-1000width" alt="Local-Response-Time" /></p>

<p>QUBE 在计算的过程中会忽略除了磁盘排队时间的其他排队时间，这样能够简化整个评估流程，而磁盘的服务时间主要还是包括同步读写以及异步读几个部分：</p>

<p><img src="https://img.nju520.me/2017-09-16-Disk-Service-Time.jpg-1000width" alt="Disk-Service-Time" /></p>

<p>在排除了上述多个部分的内容，我们得到了一个非常简单的估算过程，整个估算时间的输入仅为随机读和顺序读以及数据获取的三个输入，而它们也是影响查询的主要因素：</p>

<p><img src="https://img.nju520.me/2017-09-16-Local-Response-Time-Calculation.jpg-1000width" alt="Local-Response-Time-Calculation" /></p>

<p>其中数据获取的过程在比较不同的索引对同一查询的影响是不需要考虑的，因为同一查询使用不同的索引也会得到相同的结果集，获取的数据也是完全相同的。</p>

<h3 id="访问">访问</h3>

<p>当 MySQL 读取一个索引行或者一个表行时，就会发生一次访问，当使用全表扫描或者扫描索引片时，读取的第一个行就是随机访问，随机访问需要磁盘进行寻道和旋转，所以其代价巨大，而接下来顺序读取的所有行都是通过顺序访问读取的，代价只有随机访问的千分之一。</p>

<p>如果大量的顺序读取索引行和表行，在原理上可能会造成一些额外的零星的随机访问，不过这对于整个查询的估算来说其实并不重要；在计算本地响应时间时，仍然会把它们当做顺序访问进行估算。</p>

<h3 id="示例">示例</h3>

<p>在这里，我们简单地举一个例子来展示如何计算查询在使用某个索引时所需要的本地响应时间，假设我们有一张 <code>users</code> 表，其中有一千万条数据：</p>

<p><img src="https://img.nju520.me/2017-09-16-User-Table.jpg-1000width" alt="User-Table" /></p>

<p>在该 <code>users</code> 表中除了主键索引之外，还具有以下 (username, city)、(username, age) 和 (username) 几个辅助索引，当我们使用如下所示的查询时：</p>

<p><img src="https://img.nju520.me/2017-09-16-Filter-Factor.jpg-1000width" alt="Filter-Facto" /></p>

<p>两个查询条件分别有着 0.05% 和 12% 的过滤因子，该查询可以直接使用已有的辅助索引 (username, city)，接下来我们根据表中的总行数和过滤因子开始估算这一步骤 SQL 的执行时间：</p>

<p><img src="https://img.nju520.me/2017-09-16-Index-Slice-Scan.jpg-1000width" alt="Index-Slice-Scan" /></p>

<p>该查询在开始时会命中 (username, city) 索引，扫描符合条件的索引片，该索引总共会访问 10,000,000 * 0.05% * 12% = 600 条数据，其中包括 1 次的随机访问和 599 次的顺序访问，因为该索引中的列并不能满足查询的需要，所以对于每一个索引行都会产生一次表的随机访问，以获取剩余列 age 的信息：</p>

<p><img src="https://img.nju520.me/2017-09-16-Index-Table-Touch.jpg-1000width" alt="Index-Table-Touch" /></p>

<p>在这个过程中总共产生了 600 次随机访问，最后取回结果集的过程中也会有 600 次 FETCH 操作，从总体上来看这一次 SQL 查询共进行了 <strong>601 次随机访问</strong>、599 次顺序访问和 600 次 FETCH，根据上一节中的公式我们可以得到这个查询的用时约为 6075.99ms 也就是 6s 左右，这个时间对于绝大多数应用都是无法接受的。</p>

<p><img src="https://img.nju520.me/2017-09-16-SQL-Query-Time.jpg-1000width" alt="SQL-Query-Time" /></p>

<p>在整个查询的过程中，回表查询的 600 次随机访问成为了这个超级慢的查询的主要贡献，为了解决这个问题，我们只需要添加一个 (username, city, age) 索引或者在已有的 (username, city) 后添加新的 age 列就可以避免 600 次的随机访问：</p>

<p><img src="https://img.nju520.me/2017-09-16-SQL-Query-Time-After-Optimization.jpg-1000width" alt="SQL-Query-Time-After-Optimization" /></p>

<p>(username, city, age) 索引对于该查询其实就是一个三星索引了，有关索引设计的内容可以阅读上一篇文章 <a href="http://nju520.me/sql-index-intro.html">MySQL 索引设计概要</a> 如果读者有充足的时间依然强烈推荐 <a href="https://www.amazon.cn/图书/dp/B00ZH27RH0">数据库索引设计与优化</a> 这本书。</p>

<h2 id="总结">总结</h2>

<p>这篇文章是这一年来写的最短的一篇文章了，本来想详细介绍一下 <a href="https://www.amazon.cn/图书/dp/B00ZH27RH0">数据库索引设计与优化</a> 书中对于索引性能分析的预估方法，仔细想了一下这部分的内容实在太多，例子也非常丰富，只通过一篇文章很难完整地介绍其中的全部内容，所以只选择了其中的一部分知识点简单介绍，这也是这篇文章叫概要的原因。</p>

<p>如果对文章的内容有疑问，可以在评论中留言。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://www.amazon.cn/图书/dp/B00ZH27RH0">数据库索引设计与优化</a></li>
</ul>

  ]]></description>
</item>

<item>
  <title>MySQL 索引设计概要</title>
  <link>//sql-index-intro</link>
  <author>nju520</author>
  <pubDate>2017-09-11T00:00:00+08:00</pubDate>
  <guid>//sql-index-intro</guid>
  <description><![CDATA[
  <p>在关系型数据库中设计索引其实并不是复杂的事情，很多开发者都觉得设计索引能够提升数据库的性能，相关的知识一定非常复杂。</p>

<p><img src="https://img.nju520.me/2017-09-11-Index-and-Performance.jpg-1000width" alt="Index-and-Performance" /></p>

<p>然而这种想法是不正确的，索引其实并不是一个多么高深莫测的东西，只要我们掌握一定的方法，理解索引的实现就能在不需要 DBA 的情况下设计出高效的索引。</p>

<p>本文会介绍 <a href="https://www.amazon.cn/图书/dp/B00ZH27RH0">数据库索引设计与优化</a> 中设计索引的一些方法，让各位读者能够快速的在现有的工程中设计出合适的索引。</p>

<h2 id="磁盘-io">磁盘 IO</h2>

<p>一个数据库必须保证其中存储的所有数据都是可以随时读写的，同时因为 MySQL 中所有的数据其实都是以文件的形式存储在磁盘上的，而从磁盘上<strong>随机访问</strong>对应的数据非常耗时，所以数据库程序和操作系统提供了缓冲池和内存以提高数据的访问速度。</p>

<p><img src="https://img.nju520.me/2017-09-11-Disk-IO.jpg-1000width" alt="Disk-IO" /></p>

<p>除此之外，我们还需要知道数据库对数据的读取并不是以行为单位进行的，无论是读取一行还是多行，都会将该行或者多行所在的页全部加载进来，然后再读取对应的数据记录；也就是说，读取所耗费的时间与行数无关，只与页数有关。</p>

<p><img src="https://img.nju520.me/2017-09-11-Page-DatabaseBufferPool.jpg-1000width" alt="Page-DatabaseBufferPool" /></p>

<p>在 MySQL 中，页的大小一般为 16KB，不过也可能是 8KB、32KB 或者其他值，这跟 MySQL 的存储引擎对数据的存储方式有很大的关系，文中不会展开介绍，不过<strong>索引或行记录是否在缓存池中极大的影响了访问索引或者数据的成本</strong>。</p>

<h3 id="随机读取">随机读取</h3>

<p>数据库等待一个页从磁盘读取到缓存池的所需要的成本巨大的，无论我们是想要读取一个页面上的多条数据还是一条数据，都需要消耗<strong>约</strong> 10ms 左右的时间：</p>

<p><img src="https://img.nju520.me/2017-09-11-Disk-Random-IO.jpg-1000width" alt="Disk-Random-IO" /></p>

<p>10ms 的时间在计算领域其实是一个非常巨大的成本，假设我们使用脚本向装了 SSD 的磁盘上顺序写入字节，那么在 10ms 内可以写入大概 3MB 左右的内容，但是数据库程序在 10ms 之内只能将一页的数据加载到数据库缓冲池中，从这里可以看出随机读取的代价是巨大的。</p>

<p><img src="https://img.nju520.me/2017-09-11-Disk-IO-Total-Time.jpg-1000width" alt="Disk-IO-Total-Time" /></p>

<p>这 10ms 的一次随机读取是按照每秒 50 次的读取计算得到的，其中等待时间为 3ms、磁盘的实际繁忙时间约为 6ms，最终数据页从磁盘传输到缓冲池的时间为 1ms 左右，在对查询进行估算时并不需要准确的知道随机读取的时间，只需要知道估算出的 10ms 就可以了。</p>

<h3 id="内存读取">内存读取</h3>

<p>如果在数据库的<strong>缓存池</strong>中没有找到对应的数据页，那么会去内存中寻找对应的页面：</p>

<p><img src="https://img.nju520.me/2017-09-11-Read-from-Memory.jpg-1000width" alt="Read-from-Memory" /></p>

<p>当对应的页面存在于内存时，数据库程序就会使用内存中的页，这能够将数据的读取时间降低一个数量级，将 10ms 降低到 1ms；MySQL 在执行读操作时，会先从数据库的缓冲区中读取，如果不存在与缓冲区中就会尝试从内存中加载页面，如果前面的两个步骤都失败了，最后就只能执行随机 IO 从磁盘中获取对应的数据页。</p>

<h3 id="顺序读取">顺序读取</h3>

<p>从磁盘读取数据并不是都要付出很大的代价，当数据库管理程序一次性从磁盘中<strong>顺序</strong>读取大量的数据时，读取的速度会异常的快，大概在 40MB/s 左右。</p>

<p><img src="https://img.nju520.me/2017-09-11-Sequential-Reads-from-Disk.jpg-1000width" alt="Sequential-Reads-from-Disk" /></p>

<p>如果一个页面的大小为 4KB，那么 1s 的时间就可以读取 10000 个页，读取一个页面所花费的平均时间就是 0.1ms，相比随机读取的 10ms 已经降低了两个数量级，甚至比内存中读取数据还要快。</p>

<p><img src="https://img.nju520.me/2017-09-11-Random-to-Sequential.jpg-1000width" alt="Random-to-Sequentia" /></p>

<p>数据页面的顺序读取有两个非常重要的优势：</p>

<ol>
  <li>同时读取多个界面意味着总时间的消耗会大幅度减少，磁盘的吞吐量可以达到 40MB/s；</li>
  <li>数据库管理程序会对一些即将使用的界面进行预读，以减少查询请求的等待和响应时间；</li>
</ol>

<h3 id="小结">小结</h3>

<p>数据库查询操作的时间大都消耗在从磁盘或者内存中读取数据的过程，由于随机 IO 的代价巨大，如何在一次数据库查询中减少随机 IO 的次数往往能够大幅度的降低查询所耗费的时间提高磁盘的吞吐量。</p>

<h2 id="查询过程">查询过程</h2>

<p>在上一节中，文章从数据页加载的角度介绍了磁盘 IO 对 MySQL 查询的影响，而在这一节中将介绍 MySQL 查询的执行过程中以及数据库中的数据的特征对最终查询性能的影响。</p>

<h3 id="索引片index-slices">索引片（Index Slices）</h3>

<p>索引片其实就是 SQL 查询在执行过程中扫描的一个索引片段，在这个范围中的索引将被顺序扫描，根据索引片包含的列数不同，<a href="https://www.amazon.cn/图书/dp/B00ZH27RH0">数据库索引设计与优化</a> 书中对将索引分为宽索引和窄索引：</p>

<p><img src="https://img.nju520.me/2017-09-11-Thin-Index-and-Fat-Index.jpg-1000width" alt="Thin-Index-and-Fat-Index" /></p>

<blockquote>
  <p>主键列 <code>id</code> 在所有的 MySQL 索引中都是一定会存在的。</p>
</blockquote>

<p>对于查询 <code>SELECT id, username, age FROM users WHERE username="hacker"</code> 来说，(id, username) 就是一个窄索引，因为该索引没有包含存在于 SQL 查询中的 age 列，而 (id, username, age) 就是该查询的一个宽索引了，它<strong>包含这个查询中所需要的全部数据列</strong>。</p>

<p>宽索引能够避免二次的随机 IO，而窄索引就需要在对索引进行顺序读取之后再根据主键 id 从主键索引中查找对应的数据：</p>

<p><img src="https://img.nju520.me/2017-09-11-Thin-Index-and-Clustered-Index.jpg-1000width" alt="Thin-Index-and-Clustered-Index" /></p>

<p>对于窄索引，每一个在索引中匹配到的记录行最终都需要执行另外的随机读取从聚集索引中获得剩余的数据，如果结果集非常大，那么就会导致随机读取的次数过多进而影响性能。</p>

<h3 id="过滤因子">过滤因子</h3>

<p>从上一小节对索引片的介绍，我们可以看到影响 SQL 查询的除了查询本身还与数据库表中的数据特征有关，如果使用的是窄索引那么对表的随机访问就不可避免，在这时如何让索引片变『薄』就是我们需要做的了。</p>

<p>一个 SQL 查询扫描的索引片大小其实是由过滤因子决定的，也就是满足查询条件的记录行数所占的比例：</p>

<p><img src="https://img.nju520.me/2017-09-11-Filter-Factor.jpg-1000width" alt="Filter-Facto" /></p>

<p>对于 users 表来说，sex=”male” 就不是一个好的过滤因子，它会选择整张表中一半的数据，所以<strong>在一般情况下</strong>我们最好不要使用 sex 列作为整个索引的第一列；而 name=”hacker” 的使用就可以得到一个比较好的过滤因子了，它的使用能过滤整个数据表中 99.9% 的数据；当然我们也可以将这三个过滤进行组合，创建一个新的索引 (name, age, sex) 并同时使用这三列作为过滤条件：</p>

<p><img src="https://img.nju520.me/2017-09-11-Combined-Filter-Factor.jpg-1000width" alt="Combined-Filter-Facto" /></p>

<blockquote>
  <p>当三个过滤条件都是等值谓词时，几个索引列的顺序其实是无所谓的，索引列的顺序不会影响同一个 SQL 语句对索引的选择，也就是索引 (name, age, sex) 和 (age, sex, name) 对于上图中的条件来说是完全一样的，这两个索引在执行查询时都有着完全相同的效果。</p>
</blockquote>

<p>组合条件的过滤因子就可以达到十万分之 6 了，如果整张表中有 10w 行数据，也只需要在扫描薄索引片后进行 6 次随机读取，这种直接使用乘积来计算组合条件的过滤因子其实有一个比较重要的问题：列与列之间不应该有太强的相关性，如果不同的列之间有相关性，那么得到的结果就会比直接乘积得出的结果大一些，比如：所在的城市和邮政编码就有非常强的相关性，两者的过滤因子直接相乘其实与实际的过滤因子会有很大的偏差，不过这在多数情况下都不是太大的问题。</p>

<p>对于一张表中的同一个列，不同的值也会有不同的过滤因子，这也就造成了同一列的不同值最终的查询性能也会有很大差别：</p>

<p><img src="https://img.nju520.me/2017-09-11-Same-Columns-Filter-Factor.jpg-1000width" alt="Same-Columns-Filter-Facto" /></p>

<p>当我们评估一个索引是否合适时，需要考虑极端情况下查询语句的性能，比如 0% 或者 50% 等；最差的输入往往意味着最差的性能，在平均情况下表现良好的 SQL 语句在极端的输入下可能就完全无法正常工作，这也是在设计索引时需要注意的问题。</p>

<p>总而言之，需要扫描的索引片的大小对查询性能的影响至关重要，而扫描的索引记录的数量，就是总行数与组合条件的过滤因子的乘积，索引片的大小最终也决定了从表中读取数据所需要的时间。</p>

<h3 id="匹配列与过滤列">匹配列与过滤列</h3>

<p>假设在 users 表中有 name、age 和 (name, sex, age) 三个辅助索引；当 WHERE 条件中存在类似 age = 21 或者 name = “hacker” 这种<strong>等值谓词</strong>时，它们都会成为匹配列（Matching Column）用于选择索引树中的数据行，但是当我们使用以下查询时：</p>

<pre><code class="language-sql">SELECT * FROM users
WHERE name = "hacker" AND sex = "male" AND age &gt; 20;
</code></pre>

<p>虽然我们有 (name, sex, age) 索引包含了上述查询条件中的全部列，但是在这里只有 name 和 sex 两列才是匹配列，MySQL 在执行上述查询时，会选择 name 和 sex 作为匹配列，扫描所有满足条件的数据行，然后将 age 当做过滤列（Filtering Column）：</p>

<p><img src="https://img.nju520.me/2017-09-11-Match-Columns-Filter-Columns.jpg-1000width" alt="Match-Columns-Filter-Columns" /></p>

<p>过滤列虽然不能够减少索引片的大小，但是能够减少从表中随机读取数据的次数，所以在索引中也扮演着非常重要的角色。</p>

<h2 id="索引的设计">索引的设计</h2>

<p>作者相信文章前面的内容已经为索引的设计提供了充足的理论基础和知识，从总体来看如何减少随机读取的次数是设计索引时需要重视的最重要的问题，在这一节中，我们将介绍 <a href="https://www.amazon.cn/图书/dp/B00ZH27RH0">数据库索引设计与优化</a> 一书中归纳出的设计最佳索引的方法。</p>

<h3 id="三星索引">三星索引</h3>

<p>三星索引是对于一个查询语句可能的最好索引，如果一个查询语句的索引是三星索引，那么它只需要进行<strong>一次磁盘的随机读及一个窄索引片的顺序扫描</strong>就可以得到全部的结果集；因此其查询的响应时间比普通的索引会少几个数量级；根据书中对三星索引的定义，我们可以理解为主键索引对于 <code>WHERE id = 1</code> 就是一个特殊的三星索引，我们只需要对主键索引树进行一次索引访问并且顺序读取一条数据记录查询就结束了。</p>

<p><img src="https://img.nju520.me/2017-09-11-Three-Star-Index.jpg-1000width" alt="Three-Star-Index" /></p>

<p>为了满足三星索引中的三颗星，我们分别需要做以下几件事情：</p>

<ol>
  <li>第一颗星需要取出所有等值谓词中的列，作为索引开头的最开始的列（任意顺序）；</li>
  <li>第二颗星需要将 ORDER BY 列加入索引中；</li>
  <li>第三颗星需要将查询语句剩余的列全部加入到索引中；</li>
</ol>

<blockquote>
  <p>三星索引的概念和星级的给定来源于 <a href="https://www.amazon.cn/图书/dp/B00ZH27RH0">数据库索引设计与优化</a> 书中第四章三星索引一节。</p>
</blockquote>

<p>如果对于一个查询语句我们依照上述的三个条件进行设计，那么就可以得到该查询的三星索引，这三颗星中的最后一颗星往往都是最容易获得的，满足第三颗星的索引也就是上面提到的宽索引，能够避免大量的随机 IO，如果我们遵循这个顺序为一个 SQL 查询设计索引那么我们就可以得到一个完美的索引了；这三颗星的获得其实也没有表面上这么简单，每一颗星都有自己的意义：</p>

<p><img src="https://img.nju520.me/2017-09-11-Behind-Three-Star-Index.jpg-1000width" alt="Behind-Three-Star-Index" /></p>

<ol>
  <li>第一颗星不只是将等值谓词的列加入索引，它的作用是减少索引片的大小以减少需要扫描的数据行；</li>
  <li>第二颗星用于避免排序，减少磁盘 IO 和内存的使用；</li>
  <li>第三颗星用于避免每一个索引对应的数据行都需要进行一次随机 IO 从聚集索引中读取剩余的数据；</li>
</ol>

<p>在实际场景中，问题往往没有这么简单，我们虽然可以总能够通过宽索引避免大量的随机访问，但是在一些复杂的查询中我们无法同时获得第一颗星和第二颗星。</p>

<pre><code class="language-sql">SELECT id, name, age FROM users
WHERE age BETWEEN 18 AND 21
  AND city = "Beijing"
ORDER BY name;
</code></pre>

<p>在上述查询中，我们总可以通过增加索引中的列以获得第三颗星，但是如果我们想要获得第一颗星就需要最小化索引片的大小，这时索引的前缀必须为 (city, age)，在这时再想获得第三颗星就不可能了，哪怕在 age 的后面添加索引列 name，也会因为 name 在范围索引列 age 后面必须进行一次排序操作，最终得到的索引就是 (city, age, name, id)：</p>

<p><img src="https://img.nju520.me/2017-09-11-Different-Stars-Index.jpg-1000width" alt="Different-Stars-Index" /></p>

<p>如果我们需要在内存中避免排序的话，就需要交换 age 和 name 的位置了，在这时就可以得到索引 (city, name, age, id)，当一个 SQL 查询中<strong>同时拥有范围谓词和 ORDER BY 时</strong>，无论如何我们都是没有办法获得一个三星索引的，我们能够做的就是在这两者之间做出选择，是牺牲第一颗星还是第二颗星。</p>

<p>总而言之，在设计单表的索引时，首先把查询中所有的<strong>等值谓词全部取出</strong>以任意顺序放在索引最前面，在这时，如果索引中同时存在范围索引和 ORDER BY 就需要权衡利弊了，希望最小化扫描的索引片厚度时，应该将<strong>过滤因子最小的范围索引列</strong>加入索引，如果希望避免排序就选择 <strong>ORDER BY 中的全部列</strong>，在这之后就只需要将查询中<strong>剩余的全部列</strong>加入索引了，通过这种固定的方法和逻辑就可以最快地获得一个查询语句的二星或者三星索引了。</p>

<h2 id="总结">总结</h2>

<p>在单表上对索引进行设计其实还是非常容易的，只需要遵循固定的套路就能设计出一个理想的三星索引，在这里强烈推荐 <a href="https://www.amazon.cn/图书/dp/B00ZH27RH0">数据库索引设计与优化</a> 这本书籍，其中包含了大量与索引设计与优化的相关内容；在之后的文章中读者也会分析介绍书中提供的几种估算方法，来帮助我们通过预估问题设计出更高效的索引。</p>

<p>如果对文章内容的有疑问，可以在博客下面评论留言。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://www.amazon.cn/图书/dp/B00ZH27RH0">数据库索引设计与优化</a></li>
  <li><a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-file-space.html">File Space Management</a></li>
  <li><a href="https://www.youtube.com/watch?v=9eMWG3fwiEU">Inside of Hard Drive - YouTube</a></li>
  <li><a href="https://www.youtube.com/watch?v=4iaxOUYalJU">Hard Disk Working - How does a hard disk work - Hard Drive - YouTube</a></li>
</ul>

  ]]></description>
</item>

<item>
  <title>『浅入浅出』MongoDB 和 WiredTiger</title>
  <link>//mongodb-wiredtiger</link>
  <author>nju520</author>
  <pubDate>2017-09-06T00:00:00+08:00</pubDate>
  <guid>//mongodb-wiredtiger</guid>
  <description><![CDATA[
  <p>MongoDB 是目前主流的 NoSQL 数据库之一，与关系型数据库和其它的 NoSQL 不同，MongoDB 使用了面向文档的数据存储方式，将数据以类似 JSON 的方式存储在磁盘上，因为项目上的一些历史遗留问题，作者在最近的工作中也不得不经常与 MongoDB 打交道，这也是这篇文章出现的原因。</p>

<p><img src="https://img.nju520.me/2017-09-06-logo.png-1000width" alt="logo" /></p>

<p>虽然在之前也对 MongoDB 有所了解，但是真正在项目中大规模使用还是第一次，使用过程中也暴露了大量的问题，不过在这里，我们主要对 MongoDB 中的一些重要概念的原理进行介绍，也会与 MySQL 这种传统的关系型数据库做一个对比，让读者自行判断它们之间的优势和劣势。</p>

<h2 id="概述">概述</h2>

<p>MongoDB 虽然也是数据库，但是它与传统的 RDBMS 相比有着巨大的不同，很多开发者都认为或者被灌输了一种思想，MongoDB 这种无 Scheme 的数据库相比 RDBMS 有着巨大的性能提升，这个判断其实是一种误解；因为数据库的性能不止与数据库本身的设计有关系，还与开发者对表结构和索引的设计、存储引擎的选择和业务有着巨大的关系，如果认为<strong>仅进行了数据库的替换就能得到数量级的性能提升</strong>，那还是太年轻了。</p>

<p><img src="https://img.nju520.me/2017-09-06-its-not-always-simple-banner.jpg-1000width" alt="its-not-always-simple-banner" /></p>

<h3 id="架构">架构</h3>

<p>现有流行的数据库其实都有着非常相似的架构，MongoDB 其实就与 MySQL 中的架构相差不多，底层都使用了『可插拔』的存储引擎以满足用户的不同需要。</p>

<p><img src="https://img.nju520.me/2017-09-06-MongoDB-Architecture.jpg-1000width" alt="MongoDB-Architecture" /></p>

<p>用户可以根据表中的数据特征选择不同的存储引擎，它们可以在同一个 MongoDB 的实例中使用；在最新版本的 MongoDB 中使用了 WiredTiger 作为默认的存储引擎，WiredTiger 提供了不同粒度的并发控制和压缩机制，能够为不同种类的应用提供了最好的性能和存储效率。</p>

<p>在不同的存储引擎上层的就是 MongoDB 的数据模型和查询语言了，与关系型数据库不同，由于 MongoDB 对数据的存储与 RDBMS 有较大的差异，所以它创建了一套不同的查询语言；虽然 MongoDB 查询语言非常强大，支持的功能也很多，同时也是可编程的，不过其中包含的内容非常繁杂、API 设计也不是非常优雅，所以还是需要一些学习成本的，对于长时间使用 MySQL 的开发者肯定会有些不习惯。</p>

<pre><code class="language-javascript">db.collection.updateMany(
   &lt;filter&gt;,
   &lt;update&gt;,
   {
     upsert: &lt;boolean&gt;,
     writeConcern: &lt;document&gt;,
     collation: &lt;document&gt;
   }
)
</code></pre>

<p>查询语言的复杂是因为 MongoDB 支持了很多的数据类型，同时每一条数据记录也就是文档有着非常复杂的结构，这点是从设计上就没有办法避免的，所以还需要使用 MongoDB 的开发者花一些时间去学习各种各样的 API。</p>

<h3 id="rdbms-与-mongodb">RDBMS 与 MongoDB</h3>

<p>MongoDB 使用面向文档的的数据模型，导致很多概念都与 RDBMS 有一些差别，虽然从总体上来看两者都有相对应的概念，不过概念之间细微的差别其实也会影响我们对 MongoDB 的理解：</p>

<p><img src="https://img.nju520.me/2017-09-06-Translating-Between-RDBMS-and-MongoDB.jpg-1000width" alt="Translating-Between-RDBMS-and-MongoDB" /></p>

<p>传统的 RDBMS 其实使用 <code>Table</code> 的格式将数据逻辑地存储在一张二维的表中，其中不包括任何复杂的数据结构，但是由于 MongoDB 支持嵌入文档、数组和哈希等多种复杂数据结构的使用，所以它最终将所有的数据以 <a href="http://bsonspec.org">BSON</a> 的数据格式存储起来。</p>

<p>RDBMS 和 MongoDB 中的概念都有着相互对应的关系，数据库、表、行和索引的概念在两中数据库中都非常相似，唯独最后的 <code>JOIN</code> 和 <code>Embedded Document</code> 或者 <code>Reference</code> 有着巨大的差别。这一点差别其实也影响了在使用 MongoDB 时对集合（Collection）Schema 的设计，如果我们在 MongoDB 中遵循了与 RDBMS 中相同的思想对 Collection 进行设计，那么就不可避免的使用很多的 “JOIN” 语句，而 MongoDB 是不支持 “JOIN” 的，在应用内做这种查询的性能非常非常差，在这时使用嵌入式的文档其实就可以解决这种问题了，嵌入式的文档虽然可能会造成很多的数据冗余导致我们在更新时会很痛苦，但是查询时确实非常迅速。</p>

<pre><code class="language-javascript">{
  _id: &lt;ObjectId1&gt;,
  name: "nju520",
  books: [
    {
      _id: &lt;ObjectId2&gt;,
      name: "MongoDB: The Definitive Guide"
    },
    {
      _id: &lt;ObjectId3&gt;,
      name: "High Performance MySQL"
    }
  ]
}
</code></pre>

<p>在 MongoDB 的使用时，我们一定要忘记很多 RDBMS 中对于表设计的规则，同时想清楚 MongoDB 的优势，仔细思考如何对表进行设计才能利用 MongoDB 提供的诸多特性提升查询的效率。</p>

<h2 id="数据模型">数据模型</h2>

<p>MongoDB 与 RDBMS 之间最大的不同，就是数据模型的设计有着非常明显的差异，数据模型的不同决定了它有着非常不同的特性，存储在 MongoDB 中的数据有着非常灵活的 Schema，我们不需要像 RDBMS 一样，在插入数据之前就决定并且定义表中的数据结构，MongoDB 的结合不对 Collection 的数据结构进行任何限制，但是在实际使用中，同一个 Collection 中的大多数文档都具有类似的结构。</p>

<p><img src="https://img.nju520.me/2017-09-06-Different-Data-Structure.jpg-1000width" alt="Different-Data-Structure" /></p>

<p>在为 MongoDB 应用设计数据模型时，如何表示数据模型之间的关系其实是需要开发者需要仔细考虑的，MongoDB 为表示文档之间的关系提供了两种不同的方法：引用和嵌入。</p>

<h3 id="标准化数据模型">标准化数据模型</h3>

<p>引用（Reference）在 MongoDB 中被称为标准化的数据模型，它与 MySQL 的外键非常相似，每一个文档都可以通过一个 <code>xx_id</code> 的字段『链接』到其他的文档：</p>

<p><img src="https://img.nju520.me/2017-09-06-Reference-MongoDB.jpg-1000width" alt="Reference-MongoDB" /></p>

<p>但是 MongoDB 中的这种引用不像 MySQL 中可以直接通过 JOIN 进行查找，我们需要使用额外的查询找到该引用对应的模型，这虽然提供了更多的灵活性，不过由于增加了客户端和 MongoDB 之间的交互次数（Round-Trip）也会导致查询变慢，甚至非常严重的性能问题。</p>

<p>MongoDB 中的引用并不会对引用对应的数据模型是否真正存在做出任何的约束，所以如果在应用层级没有对文档之间的关系有所约束，那么就可能会出现引用了指向不存在的文档的问题：</p>

<p><img src="https://img.nju520.me/2017-09-06-Not-Found-Document.jpg-1000width" alt="Not-Found-Document" /></p>

<p>虽然引用有着比较严重的性能问题并且在数据库层面没有对模型是否被删除加上限制，不过它提供的一些特点是嵌入式的文档无法给予了，当我们需要表示多对多关系或者更加庞大的数据集时，就可以考虑使用标准化的数据模型 — 引用了。</p>

<h3 id="嵌入式数据模型">嵌入式数据模型</h3>

<p>除了与 MySQL 中非常相似的引用，MongoDB 由于其独特的数据存储方式，还提供了嵌入式的数据模型，嵌入式的数据模型也被认为是不标准的数据模型：</p>

<p><img src="https://img.nju520.me/2017-09-06-Embedded-Data-Models-MongoDB.jpg-1000width" alt="Embedded-Data-Models-MongoDB" /></p>

<p>因为 MongoDB 使用 BSON 的数据格式对数据进行存储，而嵌入式数据模型中的子文档其实就是父文档中的另一个值，只是其中存储的是一个对象：</p>

<pre><code class="language-javascript">{
  _id: &lt;ObjectId1&gt;,
  username: "nju520",
  age: 20,
  contact: [
    {
      _id: &lt;ObjectId2&gt;,
      email: "i@nju520.me"
    }
  ]
}
</code></pre>

<p>嵌入式的数据模型允许我们将有相同的关系的信息存储在同一个数据记录中，这样应用就可以更快地对相关的数据进行查询和更新了；当我们的数据模型中有『包含』这样的关系或者模型经常需要与其他模型一起出现（查询）时，比如文章和评论，那么就可以考虑使用嵌入式的关系对数据模型进行设计。</p>

<p>总而言之，嵌入的使用让我们在更少的请求中获得更多的相关数据，能够为读操作提供更高的性能，也为在同一个写请求中同时更新相关数据提供了支持。</p>

<blockquote>
  <p>MongoDB 底层的 WiredTiger 存储引擎能够保证对于同一个文档的操作都是原子的，任意一个写操作都不能原子性地影响多个文档或者多个集合。</p>
</blockquote>

<h2 id="主键和索引">主键和索引</h2>

<p>在这一节中，我们将主要介绍 MongoDB 中不同类型的索引，当然也包括每个文档中非常重要的字段 <code>_id</code>，可以<strong>理解</strong>为 MongoDB 的『主键』，除此之外还会介绍单字段索引、复合索引以及多键索引等类型的索引。</p>

<p>MongoDB 中索引的概念其实与 MySQL 中的索引相差不多，无论是底层的数据结构还是基本的索引类型都几乎完全相同，两者之间的区别就在于因为 MongoDB 支持了不同类型的数据结构，所以也理所应当地提供了更多的索引种类。</p>

<p><img src="https://img.nju520.me/2017-09-06-MongoDB-Indexes.jpg-1000width" alt="MongoDB-Indexes" /></p>

<h3 id="默认索引">默认索引</h3>

<p>MySQL 中的每一个数据行都具有一个主键，数据库中的数据都是按照以主键作为键物理地存储在文件中的；除了用于数据的存储，主键由于其特性也能够加速数据库的查询语句。</p>

<p>而 MongoDB 中所有的文档也都有一个唯一的 <code>_id</code> 字段，在默认情况下所有的文档都使用一个长 12 字节的 <code>ObjectId</code> 作为默认索引：</p>

<p><img src="https://img.nju520.me/2017-09-06-MongoDB-ObjectId.jpg-1000width" alt="MongoDB-ObjectId" /></p>

<p>前四位代表当前 <code>_id</code> 生成时的 Unix 时间戳，在这之后是三位的机器标识符和两位的处理器标识符，最后是一个三位的计数器，初始值就是一个随机数；通过这种方式代替递增的 <code>id</code> 能够解决分布式的 MongoDB 生成唯一标识符的问题，同时可以在一定程度上保证 <code>id</code> 的的增长是递增的。</p>

<h3 id="单字段索引single-field">单字段索引（Single Field）</h3>

<p>除了 MongoDB 提供的默认 <code>_id</code> 字段之外，我们还可以建立其它的单键索引，而且其中不止支持顺序的索引，还支持对索引倒排：</p>

<pre><code class="language-javasciprt">db.users.createIndex( { age: -1 } )
</code></pre>

<p>MySQL8.0 之前的索引都只能是正序排列的，在 8.0 之后才引入了逆序的索引，单一字段索引可以说是 MySQL 中的辅助（Secondary）索引的一个子集，它只是对除了 <code>_id</code> 外的任意单一字段建立起正序或者逆序的索引树。</p>

<p><img src="https://img.nju520.me/2017-09-06-Single-Field-Index.jpg-1000width" alt="Single-Field-Index" /></p>

<h3 id="复合索引compound">复合索引（Compound）</h3>

<p>除了单一字段索引这种非常简单的索引类型之外，MongoDB 还支持多个不同字段组成的复合索引（Compound Index），由于 MongoDB 中支持对同一字段的正逆序排列，所以相比于 MySQL 中的辅助索引就会出现更多的情况：</p>

<pre><code class="language-javascript">db.users.createIndex( { username: 1, age: -1 } )
db.users.createIndex( { username: 1, age: 1 } )
</code></pre>

<p>上面的两个索引是完全不同的，在磁盘上的 B+ 树其实也按照了完全不同的顺序进行存储，虽然 <code>username</code> 字段都是升序排列的，但是对于 <code>age</code> 来说，两个索引的处理是完全相反的：</p>

<p><img src="https://img.nju520.me/2017-09-06-Compound-Index.jpg-1000width" alt="Compound-Index" /></p>

<p>这也就造成了在使用查询语句对集合中数据进行查找时，如果约定了正逆序，那么其实是会使用不同的索引的，所以在索引创建时一定要考虑好使用的场景，避免创建无用的索引。</p>

<h3 id="多键索引multikey">多键索引（Multikey）</h3>

<p>由于 MongoDB 支持了类似数组的数据结构，所以也提供了名为多键索引的功能，可以将数组中的每一个元素进行索引，索引的创建其实与单字段索引没有太多的区别：</p>

<pre><code class="language-javascript">db.collection.createIndex( { address: 1 } )
</code></pre>

<p>如果一个字段是值是数组，那么在使用上述代码时会自动为这个字段创建一个多键索引，能够加速对数组中元素的查找。</p>

<h3 id="文本索引text">文本索引（Text）</h3>

<p>文本索引是 MongoDB 为我们提供的另一个比较实用的功能，不过在这里也只是对这种类型的索引提一下，也不打算深入去谈谈这东西的性能如何，如果真的要做全文索引的话，还是推荐使用 Elasticsearch 这种更专业的东西来做，而不是使用 MongoDB 提供的这项功能。</p>

<h2 id="存储">存储</h2>

<p>如何存储数据就是一个比较重要的问题，在前面我们已经提到了 MongoDB 与 MySQL 一样都提供了插件化的存储引擎支持，作为 MongoDB 的主要组件之一，存储引擎全权负责了 MongoDB 对数据的管理。</p>

<p><img src="https://img.nju520.me/2017-09-06-Multiple-Storage-Engines.jpg-1000width" alt="Multiple-Storage-Engines" /></p>

<h3 id="wiredtiger">WiredTiger</h3>

<p>MongoDB3.2 之后 WiredTiger 就是默认的存储引擎了，如果对各个存储引擎并不了解，那么还是不要改变 MongoDB 的默认存储引擎；它有着非常多的优点，比如拥有效率非常高的缓存机制：</p>

<p><img src="https://img.nju520.me/2017-09-06-WiredTiger-Cache.jpg-1000width" alt="WiredTiger-Cache" /></p>

<p>WiredTiger 还支持在内存中和磁盘上对索引进行压缩，在压缩时也使用了前缀压缩的方式以减少 RAM 的使用，在后面的文章中我们会详细介绍和分析 WiredTiger 存储引擎是如何对各种数据进行存储的。</p>

<h3 id="journaling">Journaling</h3>

<p>为了在数据库宕机保证 MongoDB 中数据的持久性，MongoDB 使用了 Write Ahead Logging 向磁盘上的 journal 文件预先进行写入；除了 journal 日志，MongoDB 还使用检查点（Checkpoint）来保证数据的一致性，当数据库发生宕机时，我们就需要 Checkpoint 和 journal 文件协作完成数据的恢复工作：</p>

<ol>
  <li>在数据文件中查找上一个检查点的标识符；</li>
  <li>在 journal 文件中查找标识符对应的记录；</li>
  <li>重做对应记录之后的全部操作；</li>
</ol>

<p>MongoDB 会每隔 60s 或者在 journal 数据的写入达到 2GB 时设置一次检查点，当然我们也可以通过在写入时传入 <code>j: true</code> 的参数强制 journal 文件的同步。</p>

<p><img src="https://img.nju520.me/2017-09-06-Checkpoints-Conditions.jpg-1000width" alt="Checkpoints-Conditions" /></p>

<p>这篇文章并不会介绍 Journal 文件的格式以及相关的内容，作者可能会在之后介绍分析 WiredTiger 的文章中简单分析其存储格式以及一些其它特性。</p>

<h2 id="总结">总结</h2>

<p>这篇文章中只是对 MongoDB 的一些基本特性以及数据模型做了简单的介绍，虽然『无限』扩展是 MongoDB 非常重要的特性，但是由于篇幅所限，我们并没有介绍任何跟 MongoDB 集群相关的信息，不过会在之后的文章中专门介绍多实例的 MongoDB 是如何协同工作的。</p>

<p>在这里，我想说的是，如果各位读者接收到了类似 MongoDB 比 MySQL 性能好很多的断言，但是在使用 MongoDB 的过程中仍然遵循以往 RDBMS 对数据库的设计方式，那么我相信性能在最终也不会有太大的提升，反而可能会不升反降；只有真正理解 MongoDB 的数据模型，并且根据业务的需要进行设计才能很好地利用类似嵌入式文档等特性并提升 MongoDB 的性能。</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://www.mongodb.com/mongodb-architecture">MongoDB Architecture</a></li>
  <li><a href="https://www.mongodb.com/blog/post/thinking-documents-part-1?jmp=docs">Thinking in Documents: Part 1</a></li>
  <li><a href="https://db-engines.com/en/ranking">DB-Engines Ranking</a></li>
  <li><a href="https://docs.mongodb.com/manual/core/data-modeling-introduction/">Data Modeling Introduction</a></li>
  <li><a href="https://www.mongodb.com/blog/post/building-applications-with-mongodbs-pluggable-storage-engines-part-1?jmp=docs">Building Applications with MongoDB’s Pluggable Storage Engines: Part 1</a></li>
</ul>

  ]]></description>
</item>

<item>
  <title>『浅入深出』MySQL 中事务的实现</title>
  <link>//mysql-transaction</link>
  <author>nju520</author>
  <pubDate>2017-08-20T00:00:00+08:00</pubDate>
  <guid>//mysql-transaction</guid>
  <description><![CDATA[
  <p>在关系型数据库中，事务的重要性不言而喻，只要对数据库稍有了解的人都知道事务具有 ACID 四个基本属性，而我们不知道的可能就是数据库是如何实现这四个属性的；在这篇文章中，我们将对事务的实现进行分析，尝试理解数据库是如何实现事务的，当然我们也会在文章中简单对 MySQL 中对 ACID 的实现进行简单的介绍。</p>

<p><img src="https://img.nju520.me/2017-08-20-Transaction-Basics.jpg-1000width" alt="Transaction-Basics" /></p>

<p>事务其实就是<strong>并发控制的基本单位</strong>；相信我们都知道，事务是一个序列操作，其中的操作要么都执行，要么都不执行，它是一个不可分割的工作单位；数据库事务的 ACID 四大特性是事务的基础，了解了 ACID 是如何实现的，我们也就清除了事务的实现，接下来我们将依次介绍数据库是如何实现这四个特性的。</p>

<h2 id="原子性">原子性</h2>

<p>在学习事务时，经常有人会告诉你，事务就是一系列的操作，要么全部都执行，要都不执行，这其实就是对事务原子性的刻画；虽然事务具有原子性，但是原子性并不是只与事务有关系，它的身影在很多地方都会出现。</p>

<p><img src="https://img.nju520.me/2017-08-20-Atomic-Operation.jpg-1000width" alt="Atomic-Operation" /></p>

<p>由于操作并不具有原子性，并且可以再分为多个操作，当这些操作出现错误或抛出异常时，整个操作就可能不会继续执行下去，而已经进行的操作造成的副作用就可能造成数据更新的丢失或者错误。</p>

<p>事务其实和一个操作没有什么太大的区别，它是一系列的数据库操作（可以理解为 SQL）的集合，如果事务不具备原子性，那么就没办法保证同一个事务中的所有操作都被执行或者未被执行了，整个数据库系统就既不可用也不可信。</p>

<h3 id="回滚日志">回滚日志</h3>

<p>想要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行<strong>回滚</strong>，而在 MySQL 中，恢复机制是通过<em>回滚日志</em>（undo log）实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后在对数据库中的对应行进行写入。</p>

<p><img src="https://img.nju520.me/2017-08-20-Transaction-Undo-Log.jpg-1000width" alt="Transaction-Undo-Log" /></p>

<p>这个过程其实非常好理解，为了能够在发生错误时撤销之前的全部操作，肯定是需要将之前的操作都记录下来的，这样在发生错误时才可以回滚。</p>

<p>回滚日志除了能够在发生错误或者用户执行 <code>ROLLBACK</code> 时提供回滚相关的信息，它还能够在整个系统发生崩溃、数据库进程直接被杀死后，当用户再次启动数据库进程时，还能够立刻通过查询回滚日志将之前未完成的事务进行回滚，这也就需要回滚日志必须先于数据持久化到磁盘上，是我们需要先写日志后写数据库的主要原因。</p>

<p>回滚日志并不能将数据库物理地恢复到执行语句或者事务之前的样子；它是逻辑日志，当回滚日志被使用时，它只会按照日志<strong>逻辑地</strong>将数据库中的修改撤销掉看，可以<strong>理解</strong>为，我们在事务中使用的每一条 <code>INSERT</code> 都对应了一条 <code>DELETE</code>，每一条 <code>UPDATE</code> 也都对应一条相反的 <code>UPDATE</code> 语句。</p>

<p><img src="https://img.nju520.me/2017-08-20-Logical-Undo-Log.jpg-1000width" alt="Logical-Undo-Log" /></p>

<p>在这里，我们并不会介绍回滚日志的格式以及它是如何被管理的，本文重点关注在它到底是一个什么样的东西，究竟解决了、如何解决了什么样的问题，如果想要了解具体实现细节的读者，相信网络上关于回滚日志的文章一定不少。</p>

<h3 id="事务的状态">事务的状态</h3>

<p>因为事务具有原子性，所以从远处看的话，事务就是密不可分的一个整体，事务的状态也只有三种：Active、Commited 和 Failed，事务要不就在执行中，要不然就是成功或者失败的状态：</p>

<p><img src="https://img.nju520.me/2017-08-20-Atomitc-Transaction-State.jpg-1000width" alt="Atomitc-Transaction-State" /></p>

<p>但是如果放大来看，我们会发现事务不再是原子的，其中包括了很多中间状态，比如部分提交，事务的状态图也变得越来越复杂。</p>

<p><img src="https://img.nju520.me/2017-08-20-Nonatomitc-Transaction-State.jpg-1000width" alt="Nonatomitc-Transaction-State" /></p>

<blockquote>
  <p>事务的状态图以及状态的描述取自 <a href="https://www.amazon.com/Database-System-Concepts-Computer-Science/dp/0073523321">Database System Concepts</a> 一书中第 14 章的内容。</p>
</blockquote>

<ul>
  <li>Active：事务的初始状态，表示事务正在执行；</li>
  <li>Partially Commited：在最后一条语句执行之后；</li>
  <li>Failed：发现事务无法正常执行之后；</li>
  <li>Aborted：事务被回滚并且数据库恢复到了事务进行之前的状态之后；</li>
  <li>Commited：成功执行整个事务；</li>
</ul>

<p>虽然在发生错误时，整个数据库的状态可以恢复，但是如果我们在事务中执行了诸如：向标准输出打印日志、向外界发出邮件、没有通过数据库修改了磁盘上的内容甚至在事务执行期间发生了转账汇款，那么这些操作作为可见的外部输出都是没有办法回滚的；这些问题都是由应用开发者解决和负责的，在绝大多数情况下，我们都需要在整个事务提交后，再触发类似的无法回滚的操作。</p>

<p><img src="https://img.nju520.me/2017-08-20-Shutdown-After-Commited.jpg-1000width" alt="Shutdown-After-Commited" /></p>

<p>以订票为例，哪怕我们在整个事务结束之后，才向第三方发起请求，由于向第三方请求并获取结果是一个需要较长事件的操作，如果在事务刚刚提交时，数据库或者服务器发生了崩溃，那么我们就非常有可能丢失发起请求这一过程，这就造成了非常严重的问题；而这一点就不是数据库所能保证的，开发者需要在适当的时候查看请求是否被发起、结果是成功还是失败。</p>

<h3 id="并行事务的原子性">并行事务的原子性</h3>

<p>到目前为止，所有的事务都只是串行执行的，一直都没有考虑过并行执行的问题；然而在实际工作中，并行执行的事务才是常态，然而并行任务下，却可能出现非常复杂的问题：</p>

<p><img src="https://img.nju520.me/2017-08-20-Nonrecoverable-Schedule.jpg-1000width" alt="Nonrecoverable-Schedule" /></p>

<p>当 Transaction1 在执行的过程中对 <code>id = 1</code> 的用户进行了读写，但是没有将修改的内容进行提交或者回滚，在这时 Transaction2 对同样的数据进行了读操作并提交了事务；也就是说 Transaction2 是依赖于 Transaction1 的，当 Transaction1 由于一些错误需要回滚时，因为要保证事务的原子性，需要对 Transaction2 进行回滚，但是由于我们已经提交了 Transaction2，所以我们已经没有办法进行回滚操作，在这种问题下我们就发生了问题，<a href="https://www.amazon.com/Database-System-Concepts-Computer-Science/dp/0073523321">Database System Concepts</a> 一书中将这种现象称为<em>不可恢复安排</em>（Nonrecoverable Schedule），那什么情况下是可以恢复的呢？</p>

<blockquote>
  <p>A recoverable schedule is one where, for each pair of transactions Ti and Tj such that Tj reads a data item previously written by Ti , the commit operation of Ti appears before the commit operation of Tj .</p>
</blockquote>

<p>简单理解一下，如果 Transaction2 依赖于事务 Transaction1，那么事务 Transaction1 必须在 Transaction2 提交之前完成提交的操作：</p>

<p><img src="https://img.nju520.me/2017-08-20-Recoverable-Schedule.jpg-1000width" alt="Recoverable-Schedule" /></p>

<p>然而这样还不算完，当事务的数量逐渐增多时，整个恢复流程也会变得越来越复杂，如果我们想要从事务发生的错误中恢复，也不是一件那么容易的事情。</p>

<p><img src="https://img.nju520.me/2017-08-20-Cascading-Rollback.jpg-1000width" alt="Cascading-Rollback" /></p>

<p>在上图所示的一次事件中，Transaction2 依赖于 Transaction1，而 Transaction3 又依赖于 Transaction1，当 Transaction1 由于执行出现问题发生回滚时，为了保证事务的原子性，就会将 Transaction2 和 Transaction3 中的工作全部回滚，这种情况也叫做<em>级联回滚</em>（Cascading Rollback），级联回滚的发生会导致大量的工作需要撤回，是我们难以接受的，不过如果想要达到<strong>绝对的</strong>原子性，这件事情又是不得不去处理的，我们会在文章的后面具体介绍如何处理并行事务的原子性。</p>

<h2 id="持久性">持久性</h2>

<p>既然是数据库，那么一定对数据的持久存储有着非常强烈的需求，如果数据被写入到数据库中，那么数据一定能够被安全存储在磁盘上；而事务的持久性就体现在，一旦事务被提交，那么数据一定会被写入到数据库中并持久存储起来。</p>

<p><img src="https://img.nju520.me/2017-08-20-Compensating-Transaction.jpg-1000width" alt="Compensating-Transaction" /></p>

<p>当事务已经被提交之后，就无法再次回滚了，唯一能够撤回已经提交的事务的方式就是创建一个相反的事务对原操作进行『补偿』，这也是事务持久性的体现之一。</p>

<h3 id="重做日志">重做日志</h3>

<p>与原子性一样，事务的持久性也是通过日志来实现的，MySQL 使用重做日志（redo log）实现事务的持久性，重做日志由两部分组成，一是内存中的重做日志缓冲区，因为重做日志缓冲区在内存中，所以它是易失的，另一个就是在磁盘上的重做日志文件，它是持久的。</p>

<p><img src="https://img.nju520.me/2017-08-20-Redo-Logging.jpg-1000width" alt="Redo-Logging" /></p>

<p>当我们在一个事务中尝试对数据进行修改时，它会先将数据从磁盘读入内存，并更新内存中缓存的数据，然后生成一条重做日志并写入重做日志缓存，当事务真正提交时，MySQL 会将重做日志缓存中的内容刷新到重做日志文件，再将内存中的数据更新到磁盘上，图中的第 4、5 步就是在事务提交时执行的。</p>

<p>在 InnoDB 中，重做日志都是以 512 字节的块的形式进行存储的，同时因为块的大小与磁盘扇区大小相同，所以重做日志的写入可以保证原子性，不会由于机器断电导致重做日志仅写入一半并留下脏数据。</p>

<p>除了所有对数据库的修改会产生重做日志，因为回滚日志也是需要持久存储的，它们也会创建对应的重做日志，在发生错误后，数据库重启时会从重做日志中找出未被更新到数据库磁盘中的日志重新执行以满足事务的持久性。</p>

<h3 id="回滚日志和重做日志">回滚日志和重做日志</h3>

<p>到现在为止我们了解了 MySQL 中的两种日志，回滚日志（undo log）和重做日志（redo log）；在数据库系统中，事务的原子性和持久性是由事务日志（transaction log）保证的，在实现时也就是上面提到的两种日志，前者用于对事务的影响进行撤销，后者在错误处理时对已经提交的事务进行重做，它们能保证两点：</p>

<ol>
  <li>发生错误或者需要回滚的事务能够成功回滚（原子性）；</li>
  <li>在事务提交后，数据没来得及写会磁盘就宕机时，在下次重新启动后能够成功恢复数据（持久性）；</li>
</ol>

<p>在数据库中，这两种日志经常都是一起工作的，我们<strong>可以</strong>将它们整体看做一条事务日志，其中包含了事务的 ID、修改的行元素以及修改前后的值。</p>

<p><img src="https://img.nju520.me/2017-08-20-Transaction-Log.jpg-1000width" alt="Transaction-Log" /></p>

<p>一条事务日志同时包含了修改前后的值，能够非常简单的进行回滚和重做两种操作，在这里我们也不会对重做和回滚日志展开进行介绍，可能会在之后的文章谈一谈数据库系统的恢复机制时提到两种日志的使用。</p>

<h2 id="隔离性">隔离性</h2>

<p>其实作者在之前的文章 <a href="http://nju520.me/mysql-innodb.html">『浅入浅出』MySQL 和 InnoDB</a> 就已经介绍过数据库事务的隔离性，不过为了保证文章的独立性和完整性，我们还会对事务的隔离性进行介绍，介绍的内容可能稍微有所不同。</p>

<p>事务的隔离性是数据库处理数据的几大基础之一，如果没有数据库的事务之间没有隔离性，就会发生在 <a href="#并行事务的原子性">并行事务的原子性</a> 一节中提到的级联回滚等问题，造成性能上的巨大损失。如果所有的事务的执行顺序都是线性的，那么对于事务的管理容易得多，但是允许事务的并行执行却能能够提升吞吐量和资源利用率，并且可以减少每个事务的等待时间。</p>

<p><img src="https://img.nju520.me/2017-08-20-Reasons-for-Allowing-Concurrency.jpg-1000width" alt="Reasons-for-Allowing-Concurrency" /></p>

<p>当多个事务同时并发执行时，事务的隔离性可能就会被违反，虽然单个事务的执行可能没有任何错误，但是从总体来看就会造成数据库的一致性出现问题，而串行虽然能够允许开发者忽略并行造成的影响，能够很好地维护数据库的一致性，但是却会影响事务执行的性能。</p>

<h3 id="事务的隔离级别">事务的隔离级别</h3>

<p>所以说数据库的隔离性和一致性其实是一个需要开发者去权衡的问题，为数据库提供什么样的隔离性层级也就决定了数据库的性能以及可以达到什么样的一致性；在 SQL 标准中定义了四种数据库的事务的隔离级别：<code>READ UNCOMMITED</code>、<code>READ COMMITED</code>、<code>REPEATABLE READ</code> 和 <code>SERIALIZABLE</code>；每个事务的隔离级别其实都比上一级多解决了一个问题：</p>

<ul>
  <li><code>RAED UNCOMMITED</code>：使用查询语句不会加锁，可能会读到未提交的行（Dirty Read）；</li>
  <li><code>READ COMMITED</code>：只对记录加记录锁，而不会在记录之间加间隙锁，所以允许新的记录插入到被锁定记录的附近，所以再多次使用查询语句时，可能得到不同的结果（Non-Repeatable Read）；</li>
  <li><code>REPEATABLE READ</code>：多次读取同一范围的数据会返回第一次查询的快照，不会返回不同的数据行，但是可能发生幻读（Phantom Read）；</li>
  <li><code>SERIALIZABLE</code>：InnoDB 隐式地将全部的查询语句加上共享锁，解决了幻读的问题；</li>
</ul>

<p>以上的所有的事务隔离级别都不允许脏写入（Dirty Write），也就是当前事务更新了另一个事务已经更新但是还未提交的数据，大部分的数据库中都使用了 READ COMMITED 作为默认的事务隔离级别，但是 MySQL 使用了 REPEATABLE READ 作为默认配置；从 RAED UNCOMMITED 到 SERIALIZABLE，随着事务隔离级别变得越来越严格，数据库对于并发执行事务的性能也逐渐下降。</p>

<p><img src="https://img.nju520.me/2017-08-20-Isolation-Performance.jpg-1000width" alt="Isolation-Performance" /></p>

<p>对于数据库的使用者，从理论上说，并不需要知道事务的隔离级别是如何实现的，我们只需要知道这个隔离级别解决了什么样的问题，但是不同数据库对于不同隔离级别的是实现细节在很多时候都会让我们遇到意料之外的坑。</p>

<p>如果读者不了解脏读、不可重复读和幻读究竟是什么，可以阅读之前的文章 <a href="http://nju520.me/mysql-innodb.html">『浅入浅出』MySQL 和 InnoDB</a>，在这里我们仅放一张图来展示各个隔离层级对这几个问题的解决情况。</p>

<p><img src="https://img.nju520.me/2017-08-20-Transaction-Isolation-Matrix.jpg-1000width" alt="Transaction-Isolation-Matrix" /></p>

<h3 id="隔离级别的实现">隔离级别的实现</h3>

<p>数据库对于隔离级别的实现就是使用<strong>并发控制机制</strong>对在同一时间执行的事务进行控制，限制不同的事务对于同一资源的访问和更新，而最重要也最常见的并发控制机制，在这里我们将简单介绍三种最重要的并发控制器机制的工作原理。</p>

<h4 id="锁">锁</h4>

<p>锁是一种最为常见的并发控制机制，在一个事务中，我们并不会将整个数据库都加锁，而是只会锁住那些需要访问的数据项， MySQL 和常见数据库中的锁都分为两种，共享锁（Shared）和互斥锁（Exclusive），前者也叫读锁，后者叫写锁。</p>

<p><img src="https://img.nju520.me/2017-08-20-Shared-Exclusive-Lock.jpg-1000width" alt="Shared-Exclusive-Lock" /></p>

<p>读锁保证了读操作可以并发执行，相互不会影响，而写锁保证了在更新数据库数据时不会有其他的事务访问或者更改同一条记录造成不可预知的问题。</p>

<h4 id="时间戳">时间戳</h4>

<p>除了锁，另一种实现事务的隔离性的方式就是通过时间戳，使用这种方式实现事务的数据库，例如 PostgreSQL 会为每一条记录保留两个字段；<em>读时间戳</em>中报错了所有访问该记录的事务中的最大时间戳，而记录行的<em>写时间戳</em>中保存了将记录改到当前值的事务的时间戳。</p>

<p><img src="https://img.nju520.me/2017-08-20-Timestamps-Record.jpg-1000width" alt="Timestamps-Record" /></p>

<p>使用时间戳实现事务的隔离性时，往往都会使用乐观锁，先对数据进行修改，在写回时再去判断当前值，也就是时间戳是否改变过，如果没有改变过，就写入，否则，生成一个新的时间戳并再次更新数据，乐观锁其实并不是真正的锁机制，它只是一种思想，在这里并不会对它进行展开介绍。</p>

<h4 id="多版本和快照隔离">多版本和快照隔离</h4>

<p>通过维护多个版本的数据，数据库可以允许事务在数据被其他事务更新时对旧版本的数据进行读取，很多数据库都对这一机制进行了实现；因为所有的读操作不再需要等待写锁的释放，所以能够显著地提升读的性能，MySQL 和 PostgreSQL 都对这一机制进行自己的实现，也就是 MVCC，虽然各自实现的方式有所不同，MySQL 就通过文章中提到的回滚日志实现了 MVCC，保证事务并行执行时能够不等待互斥锁的释放直接获取数据。</p>

<h3 id="隔离性与原子性">隔离性与原子性</h3>

<p>在这里就需要简单提一下在在原子性一节中遇到的级联回滚等问题了，如果一个事务对数据进行了写入，这时就会获取一个互斥锁，其他的事务就想要获得改行数据的读锁就必须等待写锁的释放，自然就不会发生级联回滚等问题了。</p>

<p><img src="https://img.nju520.me/2017-08-20-Shared-Lock-and-Atomicity.jpg-1000width" alt="Shared-Lock-and-Atomicity" /></p>

<p>不过在大多数的数据库，比如 MySQL 中都使用了 MVCC 等特性，也就是正常的读方法是不需要获取锁的，在想要对读取的数据进行更新时需要使用 <code>SELECT ... FOR UPDATE</code> 尝试获取对应行的互斥锁，以保证不同事务可以正常工作。</p>

<h2 id="一致性">一致性</h2>

<p>作者认为数据库的一致性是一个非常让人迷惑的概念，原因是数据库领域其实包含两个一致性，一个是 ACID 中的一致性、另一个是 CAP 定义中的一致性。</p>

<p><img src="https://img.nju520.me/2017-08-20-ACID-And-CAP.jpg-1000width" alt="ACID-And-CAP" /></p>

<p>这两个数据库的一致性说的<strong>完全不是</strong>一个事情，很多很多人都对这两者的概念有非常深的误解，当我们在讨论数据库的一致性时，一定要清楚上下文的语义是什么，尽量明确的问出我们要讨论的到底是 ACID 中的一致性还是 CAP 中的一致性。</p>

<h3 id="acid">ACID</h3>

<p>数据库对于 ACID 中的一致性的定义是这样的：如果一个事务原子地在一个一致地数据库中独立运行，那么在它执行之后，数据库的状态一定是一致的。对于这个概念，它的第一层意思就是对于数据完整性的约束，包括主键约束、引用约束以及一些约束检查等等，在事务的执行的前后以及过程中不会违背对数据完整性的约束，所有对数据库写入的操作都应该是合法的，并不能产生不合法的数据状态。</p>

<blockquote>
  <p>A transaction must preserve database consistency - if a transaction is run atomically in isolation starting from a consistent database, the database must again be consistent at the end of the transaction.</p>
</blockquote>

<p>我们可以将事务理解成一个函数，它接受一个外界的 SQL 输入和一个一致的数据库，它一定会返回一个一致的数据库。</p>

<p><img src="https://img.nju520.me/2017-08-20-Transaction-Consistency.jpg-1000width" alt="Transaction-Consistency" /></p>

<p>而第二层意思其实是指逻辑上的对于开发者的要求，我们要在代码中写出正确的事务逻辑，比如银行转账，事务中的逻辑不可能只扣钱或者只加钱，这是应用层面上对于数据库一致性的要求。</p>

<blockquote>
  <p>Ensuring consistency for an individual transaction is the responsibility of the application programmer who codes the transaction. - <a href="https://www.amazon.com/Database-System-Concepts-Computer-Science/dp/0073523321">Database System Concepts</a></p>
</blockquote>

<p>数据库 ACID 中的一致性对事务的要求不止包含对数据完整性以及合法性的检查，还包含应用层面逻辑的正确。</p>

<p>CAP 定理中的数据一致性，其实是说分布式系统中的各个节点中对于同一数据的拷贝有着相同的值；而 ACID 中的一致性是指数据库的规则，如果 schema 中规定了一个值必须是唯一的，那么一致的系统必须确保在所有的操作中，该值都是唯一的，由此来看 CAP 和 ACID 对于一致性的定义有着根本性的区别。</p>

<h2 id="总结">总结</h2>

<p>事务的 ACID 四大基本特性是保证数据库能够运行的基石，但是完全保证数据库的 ACID，尤其是隔离性会对性能有比较大影响，在实际的使用中我们也会根据业务的需求对隔离性进行调整，除了隔离性，数据库的原子性和持久性相信都是比较好理解的特性，前者保证数据库的事务要么全部执行、要么全部不执行，后者保证了对数据库的写入都是持久存储的、非易失的，而一致性不仅是数据库对本身数据的完整性的要求，同时也对开发者提出了要求 - 写出逻辑正确并且合理的事务。</p>

<p>最后，也是最重要的，当别人在将一致性的时候，一定要搞清楚他的上下文，如果对文章的内容有疑问，可以在评论中留言。</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://www.amazon.com/Database-System-Concepts-Computer-Science/dp/0073523321">Database System Concepts</a></li>
  <li><a href="https://zh.wikipedia.org/wiki/数据库事务">数据库事务</a></li>
  <li><a href="https://vladmihalcea.com/2017/03/01/how-does-mvcc-multi-version-concurrency-control-work/">How does MVCC (Multi-Version Concurrency Control) work</a></li>
  <li><a href="https://vladmihalcea.com/2017/02/14/how-does-a-relational-database-work/">How does a relational database work</a></li>
  <li><a href="http://www.mathcs.emory.edu/~cheung/Courses/377/Syllabus/10-Transactions/redo-log.html">Implementing Transaction Processing using Redo Logs</a></li>
  <li><a href="http://www.mathcs.emory.edu/~cheung/Courses/377/Syllabus/10-Transactions/undo-log.html">Implementing Transaction Processing using Undo Logs</a></li>
  <li><a href="http://cs.ulb.ac.be/public/_media/teaching/infoh417/05_-_logging-sol-slides.pdf">Undo/Redo Logging Rules</a></li>
  <li><a href="https://www.qiancheng.me/post/coding/mysql-001">MySQL 解密：InnoDB 存储引擎重做日志漫游</a></li>
  <li><a href="http://www.jdon.com/46956">ACID 中 C 与 CAP 定理中 C 的区别</a></li>
  <li><a href="https://www.voltdb.com/blog/2015/10/22/disambiguating-acid-cap/">Disambiguating ACID and CAP</a></li>
</ul>

  ]]></description>
</item>

<item>
  <title>浅析 Bigtable 和 LevelDB 的实现</title>
  <link>//bigtable-leveldb</link>
  <author>nju520</author>
  <pubDate>2017-08-12T00:00:00+08:00</pubDate>
  <guid>//bigtable-leveldb</guid>
  <description><![CDATA[
  <p>在 2006 年的 OSDI 上，Google 发布了名为 <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a> 的论文，其中描述了一个用于管理结构化数据的分布式存储系统  - Bigtable 的数据模型、接口以及实现等内容。</p>

<p><img src="https://img.nju520.me/2017-08-12-leveldb-logo.png-1000width" alt="leveldb-logo" /></p>

<p>本文会先对 Bigtable 一文中描述的分布式存储系统进行简单的描述，然后对 Google 开源的 KV 存储数据库 <a href="https://github.com/google/leveldb">LevelDB</a> 进行分析；LevelDB 可以理解为单点的 Bigtable 的系统，虽然其中没有 Bigtable 中与 tablet 管理以及一些分布式相关的逻辑，不过我们可以通过对 LevelDB 源代码的阅读增加对 Bigtable 的理解。</p>

<h2 id="bigtable">Bigtable</h2>

<p>Bigtable 是一个用于管理<strong>结构化数据</strong>的分布式存储系统，它有非常优秀的扩展性，可以同时处理上千台机器中的 PB 级别的数据；Google 中的很多项目，包括 Web 索引都使用 Bigtable 来存储海量的数据；Bigtable 的论文中声称它实现了四个目标：</p>

<p><img src="https://img.nju520.me/2017-08-12-Goals-of-Bigtable.jpg-1000width" alt="Goals-of-Bigtable" /></p>

<p>在作者看来这些目标看看就好，其实并没有什么太大的意义，所有的项目都会对外宣称它们达到了高性能、高可用性等等特性，我们需要关注的是 Bigtable 到底是如何实现的。</p>

<h3 id="数据模型">数据模型</h3>

<p>Bigtable 与数据库在很多方面都非常相似，但是它提供了与数据库不同的接口，它并没有支持全部的关系型数据模型，反而使用了简单的数据模型，使数据可以被更灵活的控制和管理。</p>

<p>在实现中，Bigtable 其实就是一个稀疏的、分布式的、多维持久有序哈希。</p>

<blockquote>
  <p>A Bigtable is a sparse, distributed, persistent multi-dimensional sorted map.</p>
</blockquote>

<p>它的定义其实也就决定了其数据模型非常简单并且易于实现，我们使用 <code>row</code>、<code>column</code> 和 <code>timestamp</code> 三个字段作为这个哈希的键，值就是一个字节数组，也可以理解为字符串。</p>

<p><img src="https://img.nju520.me/2017-08-12-Bigtable-DataModel-Row-Column-Timestamp-Value.jpg-1000width" alt="Bigtable-DataModel-Row-Column-Timestamp-Value" /></p>

<p>这里最重要的就是 <code>row</code> 的值，它的长度最大可以为 64KB，对于同一 <code>row</code> 下数据的读写都可以看做是原子的；因为 Bigtable 是按照 <code>row</code> 的值使用字典顺序进行排序的，每一段 <code>row</code> 的范围都会被 Bigtable 进行分区，并交给一个 tablet 进行处理。</p>

<h3 id="实现">实现</h3>

<p>在这一节中，我们将介绍 Bigtable 论文对于其本身实现的描述，其中包含很多内容：tablet 的组织形式、tablet 的管理、读写请求的处理以及数据的压缩等几个部分。</p>

<h4 id="tablet-的组织形式">tablet 的组织形式</h4>

<p>我们使用类似 B+ 树的三层结构来存储 tablet 的位置信息，第一层是一个单独的 <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf">Chubby</a> 文件，其中保存了根 tablet 的位置。</p>

<blockquote>
  <p>Chubby 是一个分布式锁服务，我们可能会在后面的文章中介绍它。</p>
</blockquote>

<p><img src="https://img.nju520.me/2017-08-12-Tablet-Location-Hierarchy.jpg-1000width" alt="Tablet-Location-Hierarchy" /></p>

<p>每一个 METADATA tablet 包括根节点上的 tablet 都存储了 tablet 的位置和该 tablet 中 key 的最小值和最大值；每一个 METADATA 行大约在内存中存储了 1KB 的数据，如果每一个 METADATA tablet 的大小都为 128MB，那么整个三层结构可以存储 2^61 字节的数据。</p>

<h4 id="tablet-的管理">tablet 的管理</h4>

<p>既然在整个 Bigtable 中有着海量的 tablet 服务器以及数据的分片 tablet，那么 Bigtable 是如何管理海量的数据呢？Bigtable 与很多的分布式系统一样，使用一个主服务器将 tablet 分派给不同的服务器节点。</p>

<p><img src="https://img.nju520.me/2017-08-12-Master-Manage-Tablet-Servers-And-Tablets.jpg-1000width" alt="Master-Manage-Tablet-Servers-And-Tablets" /></p>

<p>为了减轻主服务器的负载，所有的客户端仅仅通过 Master 获取 tablet 服务器的位置信息，它并不会在每次读写时都请求 Master 节点，而是直接与 tablet 服务器相连，同时客户端本身也会保存一份 tablet 服务器位置的缓存以减少与 Master 通信的次数和频率。</p>

<h4 id="读写请求的处理">读写请求的处理</h4>

<p>从读写请求的处理，我们其实可以看出整个 Bigtable 中的各个部分是如何协作的，包括日志、memtable 以及 SSTable 文件。</p>

<p><img src="https://img.nju520.me/2017-08-12-Tablet-Serving.jpg-1000width" alt="Tablet-Serving" /></p>

<p>当有客户端向 tablet 服务器发送写操作时，它会先向 tablet 服务器中的日志追加一条记录，在日志成功追加之后再向 memtable 中插入该条记录；这与现在大多的数据库的实现完全相同，通过顺序写向日志追加记录，然后再向数据库随机写，因为随机写的耗时远远大于追加内容，如果直接进行随机写，可能由于发生设备故障造成数据丢失。</p>

<p>当 tablet 服务器接收到读操作时，它会在 memtable 和 SSTable 上进行合并查找，因为 memtable 和 SSTable 中对于键值的存储都是字典顺序的，所以整个读操作的执行会非常快。</p>

<h4 id="表的压缩">表的压缩</h4>

<p>随着写操作的进行，memtable 会随着事件的推移逐渐增大，当 memtable 的大小超过一定的阈值时，就会将当前的 memtable 冻结，并且创建一个新的 memtable，被冻结的 memtable 会被转换为一个 SSTable 并且写入到 GFS 系统中，这种压缩方式也被称作 <em>Minor Compaction</em>。</p>

<p><img src="https://img.nju520.me/2017-08-12-Minor-Compaction.jpg-1000width" alt="Minor-Compaction" /></p>

<p>每一个 Minor Compaction 都能够创建一个新的 SSTable，它能够有效地降低内存的占用并且降低服务进程异常退出后，过大的日志导致的过长的恢复时间。既然有用于压缩 memtable 中数据的 Minor Compaction，那么就一定有一个对应的 Major Compaction 操作。</p>

<p><img src="https://img.nju520.me/2017-08-12-Major-Compaction.jpg-1000width" alt="Major-Compaction" /></p>

<p>Bigtable 会在<strong>后台周期性</strong>地进行 <em>Major Compaction</em>，将 memtable 中的数据和一部分的 SSTable 作为输入，将其中的键值进行归并排序，生成新的 SSTable 并移除原有的 memtable 和 SSTable，新生成的 SSTable 中包含前两者的全部数据和信息，并且将其中一部分标记未删除的信息彻底清除。</p>

<h4 id="小结">小结</h4>

<p>到这里为止，对于 Google 的 Bigtable 论文的介绍就差不多完成了，当然本文只介绍了其中的一部分内容，关于压缩算法的实现细节、缓存以及提交日志的实现等问题我们都没有涉及，想要了解更多相关信息的读者，这里强烈推荐去看一遍 Bigtable 这篇论文的原文 <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a> 以增强对其实现的理解。</p>

<h2 id="leveldb">LevelDB</h2>

<p>文章前面对于 Bigtable 的介绍其实都是对 <a href="https://github.com/google/leveldb">LevelDB</a> 这部分内容所做的铺垫，当然这并不是说前面的内容就不重要，LevelDB 是对 Bigtable 论文中描述的键值存储系统的单机版的实现，它提供了一个极其高速的键值存储系统，并且由 Bigtable 的作者 <a href="https://research.google.com/pubs/jeff.html">Jeff Dean</a> 和 <a href="https://research.google.com/pubs/SanjayGhemawat.html">Sanjay Ghemawat</a> 共同完成，可以说高度复刻了 Bigtable 论文中对于其实现的描述。</p>

<p>因为 Bigtable 只是一篇论文，同时又因为其实现依赖于 Google 的一些不开源的基础服务：GFS、Chubby 等等，我们很难接触到它的源代码，不过我们可以通过 LevelDB 更好地了解这篇论文中提到的诸多内容和思量。</p>

<h3 id="概述">概述</h3>

<p>LevelDB 作为一个键值存储的『仓库』，它提供了一组非常简单的增删改查接口：</p>

<pre><code class="language-cpp">class DB {
 public:
  virtual Status Put(const WriteOptions&amp; options, const Slice&amp; key, const Slice&amp; value) = 0;
  virtual Status Delete(const WriteOptions&amp; options, const Slice&amp; key) = 0;
  virtual Status Write(const WriteOptions&amp; options, WriteBatch* updates) = 0;
  virtual Status Get(const ReadOptions&amp; options, const Slice&amp; key, std::string* value) = 0;
}
</code></pre>

<blockquote>
  <p><code>Put</code> 方法在内部最终会调用 <code>Write</code> 方法，只是在上层为调用者提供了两个不同的选择。</p>
</blockquote>

<p><code>Get</code> 和 <code>Put</code> 是 LevelDB 为上层提供的用于读写的接口，如果我们能够对读写的过程有一个非常清晰的认知，那么理解 LevelDB 的实现就不是那么困难了。</p>

<p>在这一节中，我们将先通过对读写操作的分析了解整个工程中的一些实现，并在遇到问题和新的概念时进行解释，我们会在这个过程中一步一步介绍 LevelDB 中一些重要模块的实现以达到掌握它的原理的目标。</p>

<h3 id="从写操作开始">从写操作开始</h3>

<p>首先来看 <code>Get</code> 和 <code>Put</code> 两者中的写方法：</p>

<pre><code class="language-cpp">Status DB::Put(const WriteOptions&amp; opt, const Slice&amp; key, const Slice&amp; value) {
  WriteBatch batch;
  batch.Put(key, value);
  return Write(opt, &amp;batch);
}

Status DBImpl::Write(const WriteOptions&amp; options, WriteBatch* my_batch) {
    ...
}
</code></pre>

<p>正如上面所介绍的，<code>DB::Put</code> 方法将传入的参数封装成了一个 <code>WritaBatch</code>，然后仍然会执行 <code>DBImpl::Write</code> 方法向数据库中写入数据；写入方法 <code>DBImpl::Write</code> 其实是一个是非常复杂的过程，包含了很多对上下文状态的判断，我们先来看一个写操作的整体逻辑：</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Put.jpg-1000width" alt="LevelDB-Put" /></p>

<p>从总体上看，LevelDB 在对数据库执行写操作时，会有三个步骤：</p>

<ol>
  <li>调用 <code>MakeRoomForWrite</code> 方法为即将进行的写入提供足够的空间；
    <ul>
      <li>在这个过程中，由于 memtable 中空间的不足可能会冻结当前的 memtable，发生 Minor Compaction 并创建一个新的 <code>MemTable</code> 对象；</li>
      <li>在某些条件满足时，也可能发生 Major Compaction，对数据库中的 SSTable 进行压缩；</li>
    </ul>
  </li>
  <li>通过 <code>AddRecord</code> 方法向日志中追加一条写操作的记录；</li>
  <li>再向日志成功写入记录后，我们使用 <code>InsertInto</code> 直接插入 memtable 中，完成整个写操作的流程；</li>
</ol>

<p>在这里，我们并不会提供 LevelDB 对于 <code>Put</code> 方法实现的全部代码，只会展示一份精简后的代码，帮助我们大致了解一下整个写操作的流程：</p>

<pre><code class="language-cpp">Status DBImpl::Write(const WriteOptions&amp; options, WriteBatch* my_batch) {
  Writer w(&amp;mutex_);
  w.batch = my_batch;

  MakeRoomForWrite(my_batch == NULL);

  uint64_t last_sequence = versions_-&gt;LastSequence();
  Writer* last_writer = &amp;w;
  WriteBatch* updates = BuildBatchGroup(&amp;last_writer);
  WriteBatchInternal::SetSequence(updates, last_sequence + 1);
  last_sequence += WriteBatchInternal::Count(updates);

  log_-&gt;AddRecord(WriteBatchInternal::Contents(updates));
  WriteBatchInternal::InsertInto(updates, mem_);

  versions_-&gt;SetLastSequence(last_sequence);
  return Status::OK();
}
</code></pre>

<h4 id="不可变的-memtable">不可变的 memtable</h4>

<p>在写操作的实现代码 <code>DBImpl::Put</code> 中，写操作的准备过程 <code>MakeRoomForWrite</code> 是我们需要注意的一个方法：</p>

<pre><code class="language-cpp">Status DBImpl::MakeRoomForWrite(bool force) {
  uint64_t new_log_number = versions_-&gt;NewFileNumber();
  WritableFile* lfile = NULL;
  env_-&gt;NewWritableFile(LogFileName(dbname_, new_log_number), &amp;lfile);

  delete log_;
  delete logfile_;
  logfile_ = lfile;
  logfile_number_ = new_log_number;
  log_ = new log::Writer(lfile);
  imm_ = mem_;
  has_imm_.Release_Store(imm_);
  mem_ = new MemTable(internal_comparator_);
  mem_-&gt;Ref();
  MaybeScheduleCompaction();
  return Status::OK();
}
</code></pre>

<p>当 LevelDB 中的 memtable 已经被数据填满导致内存已经快不够用的时候，我们会开始对 memtable 中的数据进行冻结并创建一个新的 <code>MemTable</code> 对象。</p>

<p><img src="https://img.nju520.me/2017-08-12-Immutable-MemTable.jpg-1000width" alt="Immutable-MemTable" /></p>

<p>你可以看到，与 Bigtable 中论文不同的是，LevelDB 中引入了一个不可变的 memtable 结构 imm，它的结构与 memtable 完全相同，只是其中的所有数据都是不可变的。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Serving.jpg-1000width" alt="LevelDB-Serving" /></p>

<p>在切换到新的 memtable 之后，还可能会执行 <code>MaybeScheduleCompaction</code> 来触发一次 Minor Compaction 将 imm 中数据固化成数据库中的 SSTable；imm 的引入能够解决由于 memtable 中数据过大导致压缩时不可写入数据的问题。</p>

<p>引入 imm 后，如果 memtable 中的数据过多，我们可以直接将 memtable 指针赋值给 imm，然后创建一个新的 MemTable 实例，这样就可以继续接受外界的写操作，不再需要等待 Minor Compaction 的结束了。</p>

<h4 id="日志记录的格式">日志记录的格式</h4>

<p>作为一个持久存储的 KV 数据库，LevelDB 一定要有日志模块以支持错误发生时恢复数据，我们想要深入了解 LevelDB 的实现，那么日志的格式是一定绕不开的问题；这里并不打算展示用于追加日志的方法 <code>AddRecord</code> 的实现，因为方法中只是实现了对表头和字符串的拼接。</p>

<p>日志在 LevelDB 是以块的形式存储的，每一个块的长度都是 32KB，<strong>固定的块长度</strong>也就决定了日志可能存放在块中的任意位置，LevelDB 中通过引入一位 <code>RecordType</code> 来表示当前记录在块中的位置：</p>

<pre><code class="language-cpp">enum RecordType {
  // Zero is reserved for preallocated files
  kZeroType = 0,
  kFullType = 1,
  // For fragments
  kFirstType = 2,
  kMiddleType = 3,
  kLastType = 4
};
</code></pre>

<p>日志记录的类型存储在该条记录的头部，其中还存储了 4 字节日志的 CRC 校验、记录的长度等信息：</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-log-format-and-recordtype.jpg-1000width" alt="LevelDB-log-format-and-recordtype" /></p>

<p>上图中一共包含 4 个块，其中存储着 6 条日志记录，我们可以通过 <code>RecordType</code> 对每一条日志记录或者日志记录的一部分进行标记，并在日志需要使用时通过该信息重新构造出这条日志记录。</p>

<pre><code class="language-cpp">virtual Status Sync() {
  Status s = SyncDirIfManifest();
  if (fflush_unlocked(file_) != 0 ||
      fdatasync(fileno(file_)) != 0) {
    s = Status::IOError(filename_, strerror(errno));
  }
  return s;
}
</code></pre>

<p>因为向日志中写新记录都是顺序写的，所以它写入的速度非常快，当在内存中写入完成时，也会直接将缓冲区的这部分的内容 <code>fflush</code> 到磁盘上，实现对记录的持久化，用于之后的错误恢复等操作。</p>

<h4 id="记录的插入">记录的插入</h4>

<p>当一条数据的记录写入日志时，这条记录仍然无法被查询，只有当该数据写入 memtable 后才可以被查询，而这也是这一节将要介绍的内容，无论是数据的插入还是数据的删除都会向 memtable 中添加一条记录。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Memtable-Key-Value-Format.jpg-1000width" alt="LevelDB-Memtable-Key-Value-Format" /></p>

<p>添加和删除的记录的区别就是它们使用了不用的 <code>ValueType</code> 标记，插入的数据会将其设置为 <code>kTypeValue</code>，删除的操作会标记为 <code>kTypeDeletion</code>；但是它们实际上都向 memtable 中插入了一条数据。</p>

<pre><code class="language-cpp">virtual void Put(const Slice&amp; key, const Slice&amp; value) {
  mem_-&gt;Add(sequence_, kTypeValue, key, value);
  sequence_++;
}
virtual void Delete(const Slice&amp; key) {
  mem_-&gt;Add(sequence_, kTypeDeletion, key, Slice());
  sequence_++;
}
</code></pre>

<p>我们可以看到它们都调用了 memtable 的 <code>Add</code> 方法，向其内部的数据结构 skiplist 以上图展示的格式插入数据，这条数据中既包含了该记录的键值、序列号以及这条记录的种类，这些字段会在拼接后存入 skiplist；既然我们并没有在 memtable 中对数据进行删除，那么我们是如何保证每次取到的数据都是最新的呢？首先，在 skiplist 中，我们使用了自己定义的一个 <code>comparator</code>：</p>

<pre><code class="language-cpp">int InternalKeyComparator::Compare(const Slice&amp; akey, const Slice&amp; bkey) const {
  int r = user_comparator_-&gt;Compare(ExtractUserKey(akey), ExtractUserKey(bkey));
  if (r == 0) {
    const uint64_t anum = DecodeFixed64(akey.data() + akey.size() - 8);
    const uint64_t bnum = DecodeFixed64(bkey.data() + bkey.size() - 8);
    if (anum &gt; bnum) {
      r = -1;
    } else if (anum &lt; bnum) {
      r = +1;
    }
  }
  return r;
}
</code></pre>

<blockquote>
  <p>比较的两个 key 中的数据可能包含的内容都不完全相同，有的会包含键值、序列号等全部信息，但是例如从 <code>Get</code> 方法调用过来的 key 中可能就只包含键的长度、键值和序列号了，但是这并不影响这里对数据的提取，因为我们只从每个 key 的头部提取信息，所以无论是完整的 key/value 还是单独的 key，我们都不会取到 key 之外的任何数据。</p>
</blockquote>

<p>该方法分别从两个不同的 key 中取出键和序列号，然后对它们进行比较；比较的过程就是使用 <code>InternalKeyComparator</code> 比较器，它通过 <code>user_key</code> 和 <code>sequence_number</code> 进行排序，其中 <code>user_key</code> 按照递增的顺序排序、<code>sequence_number</code> 按照递减的顺序排序，因为随着数据的插入序列号是不断递增的，所以我们可以保证先取到的都是最新的数据或者删除信息。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-MemTable-SkipList.jpg-1000width" alt="LevelDB-MemTable-SkipList" /></p>

<p>在序列号的帮助下，我们并不需要对历史数据进行删除，同时也能加快写操作的速度，提升 LevelDB 的写性能。</p>

<h3 id="数据的读取">数据的读取</h3>

<p>从 LevelDB 中读取数据其实并不复杂，memtable 和 imm 更像是两级缓存，它们在内存中提供了更快的访问速度，如果能直接从内存中的这两处直接获取到响应的值，那么它们一定是最新的数据。</p>

<blockquote>
  <p>LevelDB 总会将新的键值对写在最前面，并在数据压缩时删除历史数据。</p>
</blockquote>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Read-Processes.jpg-1000width" alt="LevelDB-Read-Processes" /></p>

<p>数据的读取是按照 MemTable、Immutable MemTable 以及不同层级的 SSTable 的顺序进行的，前两者都是在内存中，后面不同层级的 SSTable 都是以 <code>*.ldb</code> 文件的形式持久存储在磁盘上，而正是因为有着不同层级的 SSTable，所以我们的数据库的名字叫做 LevelDB。</p>

<p>精简后的读操作方法的实现代码是这样的，方法的脉络非常清晰，作者相信这里也不需要过多的解释：</p>

<pre><code class="language-cpp">Status DBImpl::Get(const ReadOptions&amp; options, const Slice&amp; key, std::string* value) {
  LookupKey lkey(key, versions_-&gt;LastSequence());
  if (mem_-&gt;Get(lkey, value, NULL)) {
    // Done
  } else if (imm_ != NULL &amp;&amp; imm_-&gt;Get(lkey, value, NULL)) {
    // Done
  } else {
    versions_-&gt;current()-&gt;Get(options, lkey, value, NULL);
  }

  MaybeScheduleCompaction();
  return Status::OK();
}
</code></pre>

<p>当 LevelDB 在 memtable 和 imm 中查询到结果时，如果查询到了数据并不一定表示当前的值一定存在，它仍然需要判断 <code>ValueType</code> 来确定当前记录是否被删除。</p>

<h4 id="多层级的-sstable">多层级的 SSTable</h4>

<p>当 LevelDB 在内存中没有找到对应的数据时，它才会到磁盘中多个层级的 SSTable 中进行查找，这个过程就稍微有一点复杂了，LevelDB 会在多个层级中逐级进行查找，并且不会跳过其中的任何层级；在查找的过程就涉及到一个非常重要的数据结构 <code>FileMetaData</code>：</p>

<p><img src="https://img.nju520.me/2017-08-12-FileMetaData.jpg-1000width" alt="FileMetaData" /></p>

<p><code>FileMetaData</code> 中包含了整个文件的全部信息，其中包括键的最大值和最小值、允许查找的次数、文件被引用的次数、文件的大小以及文件号，因为所有的 <code>SSTable</code> 都是以固定的形式存储在同一目录下的，所以我们可以通过文件号轻松查找到对应的文件。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Level0-Layer.jpg-1000width" alt="LevelDB-Level0-Laye" /></p>

<p>查找的顺序就是从低到高了，LevelDB 首先会在 Level0 中查找对应的键。但是，与其他层级不同，Level0 中多个 SSTable 的键的范围有重合部分的，在查找对应值的过程中，会依次查找 Level0 中固定的 4 个 SSTable。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-LevelN-Layers.jpg-1000width" alt="LevelDB-LevelN-Layers" /></p>

<p>但是当涉及到更高层级的 SSTable 时，因为同一层级的 SSTable 都是没有重叠部分的，所以我们在查找时可以利用已知的 SSTable 中的极值信息 <code>smallest/largest</code> 快速查找到对应的 SSTable，再判断当前的 SSTable 是否包含查询的 key，如果不存在，就继续查找下一个层级直到最后的一个层级 <code>kNumLevels</code>（默认为 7 级）或者查询到了对应的值。</p>

<h4 id="sstable-的合并">SSTable 的『合并』</h4>

<p>既然 LevelDB 中的数据是通过多个层级的 SSTable 组织的，那么它是如何对不同层级中的 SSTable 进行合并和压缩的呢；与 Bigtable 论文中描述的两种 Compaction 几乎完全相同，LevelDB 对这两种压缩的方式都进行了实现。</p>

<p>无论是读操作还是写操作，在执行的过程中都可能调用 <code>MaybeScheduleCompaction</code> 来尝试对数据库中的 SSTable 进行合并，当合并的条件满足时，最终都会执行 <code>BackgroundCompaction</code> 方法在后台完成这个步骤。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-BackgroundCompaction-Processes.jpg-1000width" alt="LevelDB-BackgroundCompaction-Processes" /></p>

<p>这种合并分为两种情况，一种是 Minor Compaction，即内存中的数据超过了 memtable 大小的最大限制，改 memtable 被冻结为不可变的 imm，然后执行方法 <code>CompactMemTable()</code> 对内存表进行压缩。</p>

<pre><code class="language-cpp">void DBImpl::CompactMemTable() {
  VersionEdit edit;
  Version* base = versions_-&gt;current();
  WriteLevel0Table(imm_, &amp;edit, base);
  versions_-&gt;LogAndApply(&amp;edit, &amp;mutex_);
  DeleteObsoleteFiles();
}
</code></pre>

<p><code>CompactMemTable</code> 会执行 <code>WriteLevel0Table</code> 将当前的 imm 转换成一个 Level0 的 SSTable 文件，同时由于 Level0 层级的文件变多，可能会继续触发一个新的 Major Compaction，在这里我们就需要在这里选择需要压缩的合适的层级：</p>

<pre><code class="language-cpp">Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) {
  FileMetaData meta;
  meta.number = versions_-&gt;NewFileNumber();
  Iterator* iter = mem-&gt;NewIterator();
  BuildTable(dbname_, env_, options_, table_cache_, iter, &amp;meta);

  const Slice min_user_key = meta.smallest.user_key();
  const Slice max_user_key = meta.largest.user_key();
  int level = base-&gt;PickLevelForMemTableOutput(min_user_key, max_user_key);
  edit-&gt;AddFile(level, meta.number, meta.file_size, meta.smallest, meta.largest);
  return Status::OK();
}
</code></pre>

<p>所有对当前 SSTable 数据的修改由一个统一的 <code>VersionEdit</code> 对象记录和管理，我们会在后面介绍这个对象的作用和实现，如果成功写入了就会返回这个文件的元数据 <code>FileMetaData</code>，最后调用 <code>VersionSet</code> 的方法 <code>LogAndApply</code> 将文件中的全部变化如实记录下来，最后做一些数据的清理工作。</p>

<p>当然如果是 Major Compaction 就稍微有一些复杂了，不过整理后的 <code>BackgroundCompaction</code> 方法的逻辑非常清晰：</p>

<pre><code class="language-cpp">void DBImpl::BackgroundCompaction() {
  if (imm_ != NULL) {
    CompactMemTable();
    return;
  }

  Compaction* c = versions_-&gt;PickCompaction();
  CompactionState* compact = new CompactionState(c);
  DoCompactionWork(compact);
  CleanupCompaction(compact);
  DeleteObsoleteFiles();
}
</code></pre>

<p>我们从当前的 <code>VersionSet</code> 中找到需要压缩的文件信息，将它们打包存入一个 <code>Compaction</code> 对象，该对象需要选择两个层级的 SSTable，低层级的表很好选择，只需要选择大小超过限制的或者查询次数太多的 SSTable；当我们选择了低层级的一个 SSTable 后，就在更高的层级选择与该 SSTable 有重叠键的 SSTable 就可以了，通过 <code>FileMetaData</code> 中数据的帮助我们可以很快找到待压缩的全部数据。</p>

<blockquote>
  <p>查询次数太多的意思就是，当客户端调用多次 <code>Get</code> 方法时，如果这次 <code>Get</code> 方法在某个层级的 SSTable 中找到了对应的键，那么就算做上一层级中包含该键的 SSTable 的一次查找，也就是这次查找由于不同层级键的覆盖范围造成了更多的耗时，每个 SSTable 在创建之后的 <code>allowed_seeks</code> 都为 100 次，当 <code>allowed_seeks &lt; 0</code> 时就会触发该文件的与更高层级和合并，以减少以后查询的查找次数。</p>
</blockquote>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Pick-Compactions.jpg-1000width" alt="LevelDB-Pick-Compactions" /></p>

<p>LevelDB 中的 <code>DoCompactionWork</code> 方法会对所有传入的 SSTable 中的键值使用归并排序进行合并，最后会在高高层级（图中为 Level2）中生成一个新的 SSTable。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-After-Compactions.jpg-1000width" alt="LevelDB-After-Compactions" /></p>

<p>这样下一次查询 17~40 之间的值时就可以减少一次对 SSTable 中数据的二分查找以及读取文件的时间，提升读写的性能。</p>

<h4 id="存储-db-状态的-versionset">存储 db 状态的 VersionSet</h4>

<p>LevelDB 中的所有状态其实都是被一个 <code>VersionSet</code> 结构所存储的，一个 <code>VersionSet</code> 包含一组 <code>Version</code> 结构体，所有的 <code>Version</code> 包括历史版本都是通过双向链表连接起来的，但是只有一个版本是当前版本。</p>

<p><img src="https://img.nju520.me/2017-08-12-VersionSet-Version-And-VersionEdit.jpg-1000width" alt="VersionSet-Version-And-VersionEdit" /></p>

<p>当 LevelDB 中的 SSTable 发生变动时，它会生成一个 <code>VersionEdit</code> 结构，最终执行 <code>LogAndApply</code> 方法：</p>

<pre><code class="language-cpp">Status VersionSet::LogAndApply(VersionEdit* edit, port::Mutex* mu) {
  Version* v = new Version(this);
  Builder builder(this, current_);
  builder.Apply(edit);
  builder.SaveTo(v);

  std::string new_manifest_file;
  new_manifest_file = DescriptorFileName(dbname_, manifest_file_number_);
  env_-&gt;NewWritableFile(new_manifest_file, &amp;descriptor_file_);

  std::string record;
  edit-&gt;EncodeTo(&amp;record);
  descriptor_log_-&gt;AddRecord(record);
  descriptor_file_-&gt;Sync();

  SetCurrentFile(env_, dbname_, manifest_file_number_);
  AppendVersion(v);

  return Status::OK();
}
</code></pre>

<p>该方法的主要工作是使用当前版本和 <code>VersionEdit</code> 创建一个新的版本对象，然后将 <code>Version</code> 的变更追加到 MANIFEST 日志中，并且改变数据库中全局当前版本信息。</p>

<blockquote>
  <p>MANIFEST 文件中记录了 LevelDB 中所有层级中的表、每一个 SSTable 的 Key 范围和其他重要的元数据，它以日志的格式存储，所有对文件的增删操作都会追加到这个日志中。</p>
</blockquote>

<h4 id="sstable-的格式">SSTable 的格式</h4>

<p>SSTable 中其实存储的不只是数据，其中还保存了一些元数据、索引等信息，用于加速读写操作的速度，虽然在 Bigtable 的论文中并没有给出 SSTable 的数据格式，不过在 LevelDB 的实现中，我们可以发现 SSTable 是以这种格式存储数据的：</p>

<p><img src="https://img.nju520.me/2017-08-12-SSTable-Format.jpg-1000width" alt="SSTable-Format" /></p>

<p>当 LevelDB 读取 SSTable 存在的 <code>ldb</code> 文件时，会先读取文件中的 <code>Footer</code> 信息。</p>

<p><img src="https://img.nju520.me/2017-08-12-SSTable-Footer.jpg-1000width" alt="SSTable-Foote" /></p>

<p>整个 <code>Footer</code> 在文件中占用 48 个字节，我们能在其中拿到 MetaIndex 块和 Index 块的位置，再通过其中的索引继而找到对应值存在的位置。</p>

<p><code>TableBuilder::Rep</code> 结构体中就包含了一个文件需要创建的全部信息，包括数据块、索引块等等：</p>

<pre><code class="language-cpp">struct TableBuilder::Rep {
  WritableFile* file;
  uint64_t offset;
  BlockBuilder data_block;
  BlockBuilder index_block;
  std::string last_key;
  int64_t num_entries;
  bool closed;
  FilterBlockBuilder* filter_block;
  ...
}
</code></pre>

<p>到这里，我们就完成了对整个数据读取过程的解析了；对于读操作，我们可以理解为 LevelDB 在它内部的『多级缓存』中依次查找是否存在对应的键，如果存在就会直接返回，唯一与缓存不同可能就是，在数据『命中』后，它并不会把数据移动到更近的地方，而是会把数据移到更远的地方来减少下一次的访问时间，虽然这么听起来却是不可思议，不过仔细想一下确实是这样。</p>

<h2 id="小结-1">小结</h2>

<p>在这篇文章中，我们通过对 LevelDB 源代码中读写操作的分析，了解了整个框架的绝大部分实现细节，包括 LevelDB 中存储数据的格式、多级 SSTable、如何进行合并以及管理版本等信息，不过由于篇幅所限，对于其中的一些问题并没有展开详细地进行介绍和分析，例如错误恢复以及缓存等问题；但是对 LevelDB 源代码的阅读，加深了我们对 Bigtable 论文中描述的分布式 KV 存储数据库的理解。</p>

<p>LevelDB 的源代码非常易于阅读，也是学习 C++ 语言非常优秀的资源，如果对文章的内容有疑问，可以在博客下面留言。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a></li>
  <li><a href="https://github.com/google/leveldb">LevelDB</a></li>
  <li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf">The Chubby lock service for loosely-coupled distributed systems</a></li>
  <li><a href="https://github.com/google/leveldb/blob/master/doc/impl.md">LevelDB · Impl</a></li>
  <li><a href="http://bean-li.github.io/leveldb-sstable/">leveldb 中的 SSTable</a></li>
</ul>

  ]]></description>
</item>

<item>
  <title>『浅入浅出』MySQL 和 InnoDB</title>
  <link>//mysql-innodb</link>
  <author>nju520</author>
  <pubDate>2017-08-06T00:00:00+08:00</pubDate>
  <guid>//mysql-innodb</guid>
  <description><![CDATA[
  <p>作为一名开发人员，在日常的工作中会难以避免地接触到数据库，无论是基于文件的 sqlite 还是工程上使用非常广泛的 MySQL、PostgreSQL，但是一直以来也没有对数据库有一个非常清晰并且成体系的认知，所以最近两个月的时间看了几本数据库相关的书籍并且阅读了 MySQL 的官方文档，希望对各位了解数据库的、不了解数据库的有所帮助。</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/mysql.png" alt="mysql" /></p>

<p>本文中对于数据库的介绍以及研究都是在 MySQL 上进行的，如果涉及到了其他数据库的内容或者实现会在文中单独指出。</p>

<h2 id="数据库的定义">数据库的定义</h2>

<p>很多开发者在最开始时其实都对数据库有一个比较模糊的认识，觉得数据库就是一堆数据的集合，但是实际却比这复杂的多，数据库领域中有两个词非常容易混淆，也就是<em>数据库</em>和<em>实例</em>：</p>

<ul>
  <li>数据库：物理操作文件系统或其他形式文件类型的集合；</li>
  <li>实例：MySQL 数据库由后台线程以及一个共享内存区组成；</li>
</ul>

<blockquote>
  <p>对于数据库和实例的定义都来自于 <a href="https://book.douban.com/subject/24708143/">MySQL 技术内幕：InnoDB 存储引擎</a> 一书，想要了解 InnoDB 存储引擎的读者可以阅读这本书籍。</p>
</blockquote>

<h3 id="数据库和实例">数据库和实例</h3>

<p>在 MySQL 中，实例和数据库往往都是一一对应的，而我们也无法直接操作数据库，而是要通过数据库实例来操作数据库文件，可以理解为数据库实例是数据库为上层提供的一个专门用于操作的接口。</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Database - Instance.jpg" alt="Database - Instance" /></p>

<p>在 Unix 上，启动一个 MySQL 实例往往会产生两个进程，<code>mysqld</code> 就是真正的数据库服务守护进程，而 <code>mysqld_safe</code> 是一个用于检查和设置 <code>mysqld</code> 启动的控制程序，它负责监控 MySQL 进程的执行，当 <code>mysqld</code> 发生错误时，<code>mysqld_safe</code> 会对其状态进行检查并在合适的条件下重启。</p>

<h3 id="mysql-的架构">MySQL 的架构</h3>

<p>MySQL 从第一个版本发布到现在已经有了 20 多年的历史，在这么多年的发展和演变中，整个应用的体系结构变得越来越复杂：</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Logical-View-of-MySQL-Architecture.jpg" alt="Logical-View-of-MySQL-Architecture" /></p>

<p>最上层用于连接、线程处理的部分并不是 MySQL 『发明』的，很多服务都有类似的组成部分；第二层中包含了大多数 MySQL 的核心服务，包括了对 SQL 的解析、分析、优化和缓存等功能，存储过程、触发器和视图都是在这里实现的；而第三层就是 MySQL 中真正负责数据的存储和提取的存储引擎，例如：<a href="https://en.wikipedia.org/wiki/InnoDB">InnoDB</a>、<a href="https://en.wikipedia.org/wiki/MyISAM">MyISAM</a> 等，文中对存储引擎的介绍都是对 InnoDB 实现的分析。</p>

<h2 id="数据的存储">数据的存储</h2>

<p>在整个数据库体系结构中，我们可以使用不同的存储引擎来存储数据，而绝大多数存储引擎都以二进制的形式存储数据；这一节会介绍 InnoDB 中对数据是如何存储的。</p>

<p>在 InnoDB 存储引擎中，所有的数据都被<strong>逻辑地</strong>存放在表空间中，表空间（tablespace）是存储引擎中最高的存储逻辑单位，在表空间的下面又包括段（segment）、区（extent）、页（page）：</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Tablespace-segment-extent-page-row.jpg" alt="Tablespace-segment-extent-page-row" /></p>

<p>同一个数据库实例的所有表空间都有相同的页大小；默认情况下，表空间中的页大小都为 16KB，当然也可以通过改变 <code>innodb_page_size</code> 选项对默认大小进行修改，需要注意的是不同的页大小最终也会导致区大小的不同：</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Relation Between Page Size - Extent Size.png" alt="Relation Between Page Size - Extent Size" /></p>

<p>从图中可以看出，在 InnoDB 存储引擎中，一个区的大小最小为 1MB，页的数量最少为 64 个。</p>

<h3 id="如何存储表">如何存储表</h3>

<p>MySQL 使用 InnoDB 存储表时，会将<strong>表的定义</strong>和<strong>数据索引</strong>等信息分开存储，其中前者存储在 <code>.frm</code> 文件中，后者存储在 <code>.ibd</code> 文件中，这一节就会对这两种不同的文件分别进行介绍。</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/frm-and-ibd-file.jpg" alt="frm-and-ibd-file" /></p>

<h4 id="frm-文件">.frm 文件</h4>

<p>无论在 MySQL 中选择了哪个存储引擎，所有的 MySQL 表都会在硬盘上创建一个 <code>.frm</code> 文件用来描述表的格式或者说定义；<code>.frm</code> 文件的格式在不同的平台上都是相同的。</p>

<pre><code class="language-sql">CREATE TABLE test_frm(
    column1 CHAR(5),
    column2 INTEGER
);
</code></pre>

<p>当我们使用上面的代码创建表时，会在磁盘上的 <code>datadir</code> 文件夹中生成一个 <code>test_frm.frm</code> 的文件，这个文件中就包含了表结构相关的信息：</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/frm-file-hex.png" alt="frm-file-hex" /></p>

<blockquote>
  <p>MySQL 官方文档中的 <a href="https://dev.mysql.com/doc/internals/en/frm-file-format.html">11.1 MySQL .frm File Format</a> 一文对于 <code>.frm</code> 文件格式中的二进制的内容有着非常详细的表述，在这里就不展开介绍了。</p>
</blockquote>

<h4 id="ibd-文件">.ibd 文件</h4>

<p>InnoDB 中用于存储数据的文件总共有两个部分，一是系统表空间文件，包括 <code>ibdata1</code>、<code>ibdata2</code> 等文件，其中存储了 InnoDB 系统信息和用户数据库表数据和索引，是所有表公用的。</p>

<p>当打开 <code>innodb_file_per_table</code> 选项时，<code>.ibd</code> 文件就是每一个表独有的表空间，文件存储了当前表的数据和相关的索引数据。</p>

<h3 id="如何存储记录">如何存储记录</h3>

<p>与现有的大多数存储引擎一样，InnoDB 使用页作为磁盘管理的最小单位；数据在 InnoDB 存储引擎中都是按行存储的，每个 16KB 大小的页中可以存放 2-200 行的记录。</p>

<p>当 InnoDB 存储数据时，它可以使用不同的行格式进行存储；MySQL 5.7 版本支持以下格式的行存储方式：</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Antelope-Barracuda-Row-Format.jpg" alt="Antelope-Barracuda-Row-Format" /></p>

<blockquote>
  <p>Antelope 是 InnoDB 最开始支持的文件格式，它包含两种行格式 Compact 和 Redundant，它最开始并没有名字；Antelope 的名字是在新的文件格式 Barracuda 出现后才起的，Barracuda 的出现引入了两种新的行格式 Compressed 和 Dynamic；InnoDB 对于文件格式都会向前兼容，而官方文档中也对之后会出现的新文件格式预先定义好了名字：Cheetah、Dragon、Elk 等等。</p>
</blockquote>

<p>两种行记录格式 Compact 和 Redundant 在磁盘上按照以下方式存储：</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/COMPACT-And-REDUNDANT-Row-Format.jpg" alt="COMPACT-And-REDUNDANT-Row-Format" /></p>

<p>Compact 和 Redundant 格式最大的不同就是记录格式的第一个部分；在 Compact 中，行记录的第一部分倒序存放了一行数据中列的长度（Length），而 Redundant 中存的是每一列的偏移量（Offset），从总体上上看，Compact 行记录格式相比 Redundant 格式能够减少 20% 的存储空间。</p>

<h4 id="行溢出数据">行溢出数据</h4>

<p>当 InnoDB 使用 Compact 或者 Redundant 格式存储极长的 VARCHAR 或者 BLOB 这类大对象时，我们并不会直接将所有的内容都存放在数据页节点中，而是将行数据中的前 768 个字节存储在数据页中，后面会通过偏移量指向溢出页。</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Row-Overflow.jpg" alt="Row-Overflo" /></p>

<p>但是当我们使用新的行记录格式 Compressed 或者 Dynamic 时都只会在行记录中保存 20 个字节的指针，实际的数据都会存放在溢出页面中。</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Row-Overflow-in-Barracuda.jpg" alt="Row-Overflow-in-Barracuda" /></p>

<p>当然在实际存储中，可能会对不同长度的 TEXT 和 BLOB 列进行优化，不过这就不是本文关注的重点了。</p>

<blockquote>
  <p>想要了解更多与 InnoDB 存储引擎中记录的数据格式的相关信息，可以阅读 <a href="https://dev.mysql.com/doc/internals/en/innodb-record-structure.html">InnoDB Record Structure</a></p>
</blockquote>

<h3 id="数据页结构">数据页结构</h3>

<p>页是 InnoDB 存储引擎管理数据的最小磁盘单位，而 B-Tree 节点就是实际存放表中数据的页面，我们在这里将要介绍页是如何组织和存储记录的；首先，一个 InnoDB 页有以下七个部分：</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/InnoDB-B-Tree-Node.jpg" alt="InnoDB-B-Tree-Node" /></p>

<p>每一个页中包含了两对 header/trailer：内部的 Page Header/Page Directory 关心的是页的状态信息，而 Fil Header/Fil Trailer 关心的是记录页的头信息。</p>

<p>在页的头部和尾部之间就是用户记录和空闲空间了，每一个数据页中都包含 Infimum 和 Supremum 这两个<strong>虚拟</strong>的记录（可以理解为占位符），Infimum 记录是比该页中任何主键值都要小的值，Supremum 是该页中的最大值：</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Infimum-Rows-Supremum.jpg" alt="Infimum-Rows-Supremum" /></p>

<p>User Records 就是整个页面中真正用于存放行记录的部分，而 Free Space 就是空余空间了，它是一个链表的数据结构，为了保证插入和删除的效率，整个页面并不会按照主键顺序对所有记录进行排序，它会自动从左侧向右寻找空白节点进行插入，行记录在物理存储上并不是按照顺序的，它们之间的顺序是由 <code>next_record</code> 这一指针控制的。</p>

<p>B+ 树在查找对应的记录时，并不会直接从树中找出对应的行记录，它只能获取记录所在的页，将整个页加载到内存中，再通过 Page Directory 中存储的稀疏索引和 <code>n_owned</code>、<code>next_record</code> 属性取出对应的记录，不过因为这一操作是在内存中进行的，所以通常会忽略这部分查找的耗时。</p>

<p>InnoDB 存储引擎中对数据的存储是一个非常复杂的话题，这一节中也只是对表、行记录以及页面的存储进行一定的分析和介绍，虽然作者相信这部分知识对于大部分开发者已经足够了，但是想要真正消化这部分内容还需要很多的努力和实践。</p>

<h2 id="索引">索引</h2>

<p>索引是数据库中非常非常重要的概念，它是存储引擎能够快速定位记录的秘密武器，对于提升数据库的性能、减轻数据库服务器的负担有着非常重要的作用；<strong>索引优化是对查询性能优化的最有效手段</strong>，它能够轻松地将查询的性能提高几个数量级。</p>

<h3 id="索引的数据结构">索引的数据结构</h3>

<p>在上一节中，我们谈了行记录的存储和页的存储，在这里我们就要从更高的层面看 InnoDB 中对于数据是如何存储的；InnoDB 存储引擎在绝大多数情况下使用 B+ 树建立索引，这是关系型数据库中查找最为常用和有效的索引，但是 B+ 树索引并不能找到一个给定键对应的具体值，它只能找到数据行对应的页，然后正如上一节所提到的，数据库把整个页读入到内存中，并在内存中查找具体的数据行。</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/B+Tree.jpg" alt="B+Tree" /></p>

<p>B+ 树是平衡树，它查找任意节点所耗费的时间都是完全相同的，比较的次数就是 B+ 树的高度；在这里，我们并不会深入分析或者动手实现一个 B+ 树，只是对它的特性进行简单的介绍。</p>

<h3 id="聚集索引和辅助索引">聚集索引和辅助索引</h3>

<p>数据库中的 B+ 树索引可以分为聚集索引（clustered index）和辅助索引（secondary index），它们之间的最大区别就是，聚集索引中存放着一条行记录的全部信息，而辅助索引中只包含索引列和一个用于查找对应行记录的『书签』。</p>

<h4 id="聚集索引">聚集索引</h4>

<p>InnoDB 存储引擎中的表都是使用索引组织的，也就是按照键的顺序存放；聚集索引就是按照表中主键的顺序构建一颗 B+ 树，并在叶节点中存放表中的行记录数据。</p>

<pre><code class="language-sql">CREATE TABLE users(
    id INT NOT NULL,
    first_name VARCHAR(20) NOT NULL,
    last_name VARCHAR(20) NOT NULL,
    age INT NOT NULL,
    PRIMARY KEY(id),
    KEY(last_name, first_name, age)
    KEY(first_name)
);
</code></pre>

<p>如果使用上面的 SQL 在数据库中创建一张表，B+ 树就会使用 <code>id</code> 作为索引的键，并在叶子节点中存储一条记录中的<strong>所有</strong>信息。</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Clustered-Index.jpg" alt="Clustered-Index" /></p>

<blockquote>
  <p>图中对 B+ 树的描述与真实情况下 B+ 树中的数据结构有一些差别，不过这里想要表达的主要意思是：聚集索引叶节点中保存的是整条行记录，而不是其中的一部分。</p>
</blockquote>

<p>聚集索引与表的物理存储方式有着非常密切的关系，所有正常的表应该<strong>有且仅有一个</strong>聚集索引（绝大多数情况下都是主键），表中的所有行记录数据都是按照<strong>聚集索引</strong>的顺序存放的。</p>

<p>当我们使用聚集索引对表中的数据进行检索时，可以直接获得聚集索引所对应的整条行记录数据所在的页，不需要进行第二次操作。</p>

<h4 id="辅助索引">辅助索引</h4>

<p>数据库将所有的非聚集索引都划分为辅助索引，但是这个概念对我们理解辅助索引并没有什么帮助；辅助索引也是通过 B+ 树实现的，但是它的叶节点并不包含行记录的全部数据，仅包含索引中的所有键和一个用于查找对应行记录的『书签』，在 InnoDB 中这个书签就是当前记录的主键。</p>

<p>辅助索引的存在并不会影响聚集索引，因为聚集索引构成的 B+ 树是数据实际存储的形式，而辅助索引只用于加速数据的查找，所以一张表上往往有多个辅助索引以此来提升数据库的性能。</p>

<blockquote>
  <p>一张表一定包含一个聚集索引构成的 B+ 树以及若干辅助索引的构成的 B+ 树。</p>
</blockquote>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Secondary-Index.jpg" alt="Secondary-Index" /></p>

<p>如果在表 <code>users</code> 中存在一个辅助索引 <code>(first_name, age)</code>，那么它构成的 B+ 树大致就是上图这样，按照 <code>(first_name, age)</code> 的字母顺序对表中的数据进行排序，当查找到主键时，再通过聚集索引获取到整条行记录。</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Clustered-Secondary-Index.jpg" alt="Clustered-Secondary-Index" /></p>

<p>上图展示了一个使用辅助索引查找一条表记录的过程：通过辅助索引查找到对应的主键，最后在聚集索引中使用主键获取对应的行记录，这也是通常情况下行记录的查找方式。</p>

<h3 id="索引的设计">索引的设计</h3>

<p>索引的设计其实是一个非常重要的内容，同时也是一个非常复杂的内容；索引的设计与创建对于提升数据库的查询性能至关重要，不过这不是本文想要介绍的内容，有关索引的设计与优化可以阅读 <a href="数据库索引设计与优化">数据库索引设计与优化</a> 一书，书中提供了一种非常科学合理的方法能够帮助我们在数据库中建立最适合的索引，当然作者也可能会在之后的文章中对索引的设计进行简单的介绍和分析。</p>

<h2 id="锁">锁</h2>

<p>我们都知道锁的种类一般分为乐观锁和悲观锁两种，InnoDB 存储引擎中使用的就是悲观锁，而按照锁的粒度划分，也可以分成行锁和表锁。</p>

<h3 id="并发控制机制">并发控制机制</h3>

<p>乐观锁和悲观锁其实都是并发控制的机制，同时它们在原理上就有着本质的差别；</p>

<ul>
  <li>乐观锁是一种思想，它其实并不是一种真正的『锁』，它会先尝试对资源进行修改，在写回时判断资源是否进行了改变，如果没有发生改变就会写回，否则就会进行重试，在整个的执行过程中其实都<strong>没有对数据库进行加锁</strong>；</li>
  <li>悲观锁就是一种真正的锁了，它会在获取资源前对资源进行加锁，确保同一时刻只有有限的线程能够访问该资源，其他想要尝试获取资源的操作都会进入等待状态，直到该线程完成了对资源的操作并且释放了锁后，其他线程才能重新操作资源；</li>
</ul>

<p>虽然乐观锁和悲观锁在本质上并不是同一种东西，一个是一种思想，另一个是一种真正的锁，但是它们都是一种并发控制机制。</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Optimistic-Pessimistic-Locks.jpg" alt="Optimistic-Pessimistic-Locks" /></p>

<p>乐观锁不会存在死锁的问题，但是由于更新后验证，所以当<strong>冲突频率</strong>和<strong>重试成本</strong>较高时更推荐使用悲观锁，而需要非常高的<strong>响应速度</strong>并且<strong>并发量</strong>非常大的时候使用乐观锁就能较好的解决问题，在这时使用悲观锁就可能出现严重的性能问题；在选择并发控制机制时，需要综合考虑上面的四个方面（冲突频率、重试成本、响应速度和并发量）进行选择。</p>

<h3 id="锁的种类">锁的种类</h3>

<p>对数据的操作其实只有两种，也就是读和写，而数据库在实现锁时，也会对这两种操作使用不同的锁；InnoDB 实现了标准的行级锁，也就是共享锁（Shared Lock）和互斥锁（Exclusive Lock）；共享锁和互斥锁的作用其实非常好理解：</p>

<ul>
  <li><strong>共享锁（读锁）</strong>：允许事务对一条行数据进行读取；</li>
  <li><strong>互斥锁（写锁）</strong>：允许事务对一条行数据进行删除或更新；</li>
</ul>

<p>而它们的名字也暗示着各自的另外一个特性，共享锁之间是兼容的，而互斥锁与其他任意锁都不兼容：</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Shared-Exclusive-Lock.jpg" alt="Shared-Exclusive-Lock" /></p>

<p>稍微对它们的使用进行思考就能想明白它们为什么要这么设计，因为共享锁代表了读操作、互斥锁代表了写操作，所以我们可以在数据库中<strong>并行读</strong>，但是只能<strong>串行写</strong>，只有这样才能保证不会发生线程竞争，实现线程安全。</p>

<h3 id="锁的粒度">锁的粒度</h3>

<p>无论是共享锁还是互斥锁其实都只是对某一个数据行进行加锁，InnoDB 支持多种粒度的锁，也就是行锁和表锁；为了支持多粒度锁定，InnoDB 存储引擎引入了意向锁（Intention Lock），意向锁就是一种表级锁。</p>

<p>与上一节中提到的两种锁的种类相似的是，意向锁也分为两种：</p>

<ul>
  <li><strong>意向共享锁</strong>：事务想要在获得表中某些记录的共享锁，需要在表上先加意向共享锁；</li>
  <li><strong>意向互斥锁</strong>：事务想要在获得表中某些记录的互斥锁，需要在表上先加意向互斥锁；</li>
</ul>

<p>随着意向锁的加入，锁类型之间的兼容矩阵也变得愈加复杂：</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Lock-Type-Compatibility-Matrix.jpg" alt="Lock-Type-Compatibility-Matrix" /></p>

<p>意向锁其实不会阻塞全表扫描之外的任何请求，它们的主要目的是为了表示<strong>是否有人请求锁定表中的某一行数据</strong>。</p>

<blockquote>
  <p>有的人可能会对意向锁的目的并不是完全的理解，我们在这里可以举一个例子：如果没有意向锁，当已经有人使用行锁对表中的某一行进行修改时，如果另外一个请求要对全表进行修改，那么就需要对所有的行是否被锁定进行扫描，在这种情况下，效率是非常低的；不过，在引入意向锁之后，当有人使用行锁对表中的某一行进行修改之前，会先为表添加意向互斥锁（IX），再为行记录添加互斥锁（X），在这时如果有人尝试对全表进行修改就不需要判断表中的每一行数据是否被加锁了，只需要通过等待意向互斥锁被释放就可以了。</p>
</blockquote>

<h3 id="锁的算法">锁的算法</h3>

<p>到目前为止已经对 InnoDB 中锁的粒度有一定的了解，也清楚了在对数据库进行读写时会获取不同的锁，在这一小节将介绍锁是如何添加到对应的数据行上的，我们会分别介绍三种锁的算法：Record Lock、Gap Lock 和 Next-Key Lock。</p>

<h4 id="record-lock">Record Lock</h4>

<p>记录锁（Record Lock）是加到<strong>索引记录</strong>上的锁，假设我们存在下面的一张表 <code>users</code>：</p>

<pre><code class="language-sql">CREATE TABLE users(
    id INT NOT NULL AUTO_INCREMENT,
    last_name VARCHAR(255) NOT NULL,
    first_name VARCHAR(255),
    age INT,
    PRIMARY KEY(id),
    KEY(last_name),
    KEY(age)
);
</code></pre>

<p>如果我们使用 <code>id</code> 或者 <code>last_name</code> 作为 SQL 中 <code>WHERE</code> 语句的过滤条件，那么 InnoDB 就可以通过索引建立的 B+ 树找到行记录并添加索引，但是如果使用 <code>first_name</code> 作为过滤条件时，由于 InnoDB 不知道待修改的记录具体存放的位置，也无法对将要修改哪条记录提前做出判断就会锁定整个表。</p>

<h4 id="gap-lock">Gap Lock</h4>

<p>记录锁是在存储引擎中最为常见的锁，除了记录锁之外，InnoDB 中还存在间隙锁（Gap Lock），间隙锁是对索引记录中的一段连续区域的锁；当使用类似 <code>SELECT * FROM users WHERE id BETWEEN 10 AND 20 FOR UPDATE;</code> 的 SQL 语句时，就会阻止其他事务向表中插入 <code>id = 15</code> 的记录，因为整个范围都被间隙锁锁定了。</p>

<blockquote>
  <p>间隙锁是存储引擎对于性能和并发做出的权衡，并且只用于某些事务隔离级别。</p>
</blockquote>

<p>虽然间隙锁中也分为共享锁和互斥锁，不过它们之间并不是互斥的，也就是不同的事务可以同时持有一段相同范围的共享锁和互斥锁，它唯一阻止的就是<strong>其他事务向这个范围中添加新的记录</strong>。</p>

<h4 id="next-key-lock">Next-Key Lock</h4>

<p>Next-Key 锁相比前两者就稍微有一些复杂，它是记录锁和记录前的间隙锁的结合，在 <code>users</code> 表中有以下记录：</p>

<pre><code class="language-sql">+------|-------------|--------------|-------+
|   id | last_name   | first_name   |   age |
|------|-------------|--------------|-------|
|    4 | stark       | tony         |    21 |
|    1 | tom         | hiddleston   |    30 |
|    3 | morgan      | freeman      |    40 |
|    5 | jeff        | dean         |    50 |
|    2 | donald      | trump        |    80 |
+------|-------------|--------------|-------+
</code></pre>

<p>如果使用 Next-Key 锁，那么 Next-Key 锁就可以在需要的时候锁定以下的范围：</p>

<pre><code class="language-sql">(-∞, 21]
(21, 30]
(30, 40]
(40, 50]
(50, 80]
(80, ∞)
</code></pre>

<blockquote>
  <p>既然叫 Next-Key 锁，锁定的应该是当前值和后面的范围，但是实际上却不是，Next-Key 锁锁定的是当前值和前面的范围。</p>
</blockquote>

<p>当我们更新一条记录，比如 <code>SELECT * FROM users WHERE age = 30 FOR UPDATE;</code>，InnoDB 不仅会在范围 <code>(21, 30]</code> 上加 Next-Key 锁，还会在这条记录后面的范围 <code>(30, 40]</code> 加间隙锁，所以插入 <code>(21, 40]</code> 范围内的记录都会被锁定。</p>

<p>Next-Key 锁的作用其实是为了解决幻读的问题，我们会在下一节谈事务的时候具体介绍。</p>

<h3 id="死锁的发生">死锁的发生</h3>

<p>既然 InnoDB 中实现的锁是悲观的，那么不同事务之间就可能会互相等待对方释放锁造成死锁，最终导致事务发生错误；想要在 MySQL 中制造死锁的问题其实非常容易：</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Deadlocks.jpg" alt="Deadlocks" /></p>

<p>两个会话都持有一个锁，并且尝试获取对方的锁时就会发生死锁，不过 MySQL 也能在发生死锁时及时发现问题，并保证其中的一个事务能够正常工作，这对我们来说也是一个好消息。</p>

<h2 id="事务与隔离级别">事务与隔离级别</h2>

<p>在介绍了锁之后，我们再来谈谈数据库中一个非常重要的概念 —— 事务；相信只要是一个合格的软件工程师就对事务的特性有所了解，其中被人经常提起的就是事务的原子性，在数据提交工作时，要么保证所有的修改都能够提交，要么就所有的修改全部回滚。</p>

<p>但是事务还遵循包括原子性在内的 ACID 四大特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）；文章不会对这四大特性全部展开进行介绍，相信你能够通过 Google 和数据库相关的书籍轻松获得有关它们的概念，本文最后要介绍的就是事务的四种隔离级别。</p>

<h3 id="几种隔离级别">几种隔离级别</h3>

<p>事务的隔离性是数据库处理数据的几大基础之一，而隔离级别其实就是提供给用户用于在性能和可靠性做出选择和权衡的配置项。</p>

<p>ISO 和 ANIS SQL 标准制定了四种事务隔离级别，而 InnoDB 遵循了 SQL:1992 标准中的四种隔离级别：<code>READ UNCOMMITED</code>、<code>READ COMMITED</code>、<code>REPEATABLE READ</code> 和 <code>SERIALIZABLE</code>；每个事务的隔离级别其实都比上一级多解决了一个问题：</p>

<ul>
  <li><code>RAED UNCOMMITED</code>：使用查询语句不会加锁，可能会读到未提交的行（Dirty Read）；</li>
  <li><code>READ COMMITED</code>：只对记录加记录锁，而不会在记录之间加间隙锁，所以允许新的记录插入到被锁定记录的附近，所以再多次使用查询语句时，可能得到不同的结果（Non-Repeatable Read）；</li>
  <li><code>REPEATABLE READ</code>：多次读取同一范围的数据会返回第一次查询的快照，不会返回不同的数据行，但是可能发生幻读（Phantom Read）；</li>
  <li><code>SERIALIZABLE</code>：InnoDB 隐式地将全部的查询语句加上共享锁，解决了幻读的问题；</li>
</ul>

<p>MySQL 中默认的事务隔离级别就是 <code>REPEATABLE READ</code>，但是它通过 Next-Key 锁也能够在某种程度上解决幻读的问题。</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Transaction-Isolation-Matrix.jpg" alt="Transaction-Isolation-Matrix" /></p>

<p>接下来，我们将数据库中创建如下的表并通过个例子来展示在不同的事务隔离级别之下，会发生什么样的问题：</p>

<pre><code class="language-sql">CREATE TABLE test(
    id INT NOT NULL,
    UNIQUE(id)
);
</code></pre>

<h3 id="脏读">脏读</h3>

<p>当事务的隔离级别为 <code>READ UNCOMMITED</code> 时，我们在 <code>SESSION 2</code> 中插入的<strong>未提交</strong>数据在 <code>SESSION 1</code> 中是可以访问的。</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Read-Uncommited-Dirty-Read.jpg" alt="Read-Uncommited-Dirty-Read" /></p>

<h3 id="不可重复读">不可重复读</h3>

<p>当事务的隔离级别为 <code>READ COMMITED</code> 时，虽然解决了脏读的问题，但是如果在 <code>SESSION 1</code> 先查询了一个范围的数据，在这之后 <code>SESSION 2</code> 中插入一条数据并且提交了修改，在这时，如果 <code>SESSION 1</code> 中再次使用相同的查询语句，就会发现两次查询的结果不一样。</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Read-Commited-Non-Repeatable-Read.jpg" alt="Read-Commited-Non-Repeatable-Read" /></p>

<p>不可重复读的原因就是，在 <code>READ COMMITED</code> 的隔离级别下，存储引擎不会在查询记录时添加间隙锁，锁定 <code>id &lt; 5</code> 这个范围。</p>

<h3 id="幻读">幻读</h3>

<p>重新开启了两个会话 <code>SESSION 1</code> 和 <code>SESSION 2</code>，在 <code>SESSION 1</code> 中我们查询全表的信息，没有得到任何记录；在 <code>SESSION 2</code> 中向表中插入一条数据并提交；由于 <code>REPEATABLE READ</code> 的原因，再次查询全表的数据时，我们获得到的仍然是空集，但是在向表中插入同样的数据却出现了错误。</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Repeatable-Read-Phantom-Read.jpg" alt="Repeatable-Read-Phantom-Read" /></p>

<p>这种现象在数据库中就被称作幻读，虽然我们使用查询语句得到了一个空的集合，但是插入数据时却得到了错误，好像之前的查询是幻觉一样。</p>

<p>在标准的事务隔离级别中，幻读是由更高的隔离级别 <code>SERIALIZABLE</code> 解决的，但是它也可以通过 MySQL 提供的 Next-Key 锁解决：</p>

<p><img src="https://raw.githubusercontent.com/nju520/Analyze/master/contents/Database/images/mysql/Repeatable-with-Next-Key-Lock.jpg" alt="Repeatable-with-Next-Key-Lock" /></p>

<p><code>REPERATABLE READ</code> 和 <code>READ UNCOMMITED</code> 其实是矛盾的，如果保证了前者就看不到已经提交的事务，如果保证了后者，就会导致两次查询的结果不同，MySQL 为我们提供了一种折中的方式，能够在 <code>REPERATABLE READ</code> 模式下加锁访问已经提交的数据，其本身并不能解决幻读的问题，而是通过文章前面提到的 Next-Key 锁来解决。</p>

<h2 id="总结">总结</h2>

<blockquote>
  <p>文章中的内容大都来自于 <a href="https://book.douban.com/subject/23008813/">高性能 MySQL</a>、<a href="https://book.douban.com/subject/24708143/">MySQL 技术内幕：InnoDB 存储引擎</a>、<a href="https://book.douban.com/subject/26419771/">数据库索引设计与优化</a> 以及 MySQL 的 <a href="https://dev.mysql.com/doc/">官方文档</a>。</p>
</blockquote>

<p>由于篇幅所限仅能对数据库中一些重要内容进行简单的介绍和总结，文中内容难免有所疏漏，如果对文章内容的有疑问，可以在博客下面评论留言。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://dba.stackexchange.com/questions/35962/mysqld-safe-version-different-than-mysqld">mysqld_safe version different than mysqld?</a></li>
  <li><a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-file-space.html">File Space Management</a></li>
  <li><a href="http://mysqlserverteam.com/externally-stored-fields-in-innodb/">Externally Stored Fields in InnoDB</a></li>
  <li><a href="https://dev.mysql.com/doc/internals/en/innodb-record-structure.html">InnoDB Record Structure</a></li>
  <li><a href="https://dev.mysql.com/doc/internals/en/innodb-page-structure.html">InnoDB Page Structure</a></li>
  <li><a href="https://stackoverflow.com/questions/5070529/difference-between-clustered-and-nonclustered-index">Difference between clustered and nonclustered index</a></li>
  <li><a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html">InnoDB Locking</a></li>
  <li><a href="http://www.cnblogs.com/Bob-FD/p/3352216.html">乐观锁与悲观锁的区别</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Optimistic_concurrency_control">Optimistic concurrency control</a></li>
  <li><a href="http://www.cnblogs.com/zhoujinyi/p/3437475.html">MySQL 四种事务隔离级的说明</a></li>
</ul>

  ]]></description>
</item>


  </channel>
</rss>
