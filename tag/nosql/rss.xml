<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    
    <title>nju520.me</title>
    
    <link>http://localhost:4000</link>
    <description>nju520's Blog</description>
    <language>en-uk</language>
    <managingEditor> nju520</managingEditor>
    <atom:link href="rss" rel="self" type="application/rss+xml" />
    
<item>
  <title>分布式键值存储 Dynamo 的实现原理</title>
  <link>//dynamo</link>
  <author>nju520</author>
  <pubDate>2017-10-24T00:00:00+08:00</pubDate>
  <guid>//dynamo</guid>
  <description><![CDATA[
  <p>在最近的一周时间里，一直都在研究和阅读 Amazon 的一篇论文 <a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo: Amazon’s Highly Available Key-value Store</a>，论文中描述了 Amazon 的高可用分布式键值存储服务 Dynamo 的实现原理。</p>

<p><img src="https://img.nju520.me/2017-10-24-dynamodb.png" alt="dynamodb" /></p>

<p>之前在阅读 Google 的 <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a> 时写了一篇 <a href="http://hwbnju.com/bigtable-leveldb">浅析 Bigtable 和 LevelDB 的实现</a> 文章分析了 Bigtable 的单机版 LevelDB 的实现原理；在研究 Dynamo 时，作者发现 Dynamo 虽然和 Bigtable 同为 NoSQL，但是它们的实现却有着很大的不同，最主要的原因来自不同的应用场景和不同的目的。</p>

<h2 id="bigtable-和-dynamo">Bigtable 和 Dynamo</h2>

<p>Bigtable 和 Dynamo 两者分别是 Google 和 Amazon 两大巨头给出的存储海量数据的解决方法，作为 NoSQL 两者都具有分布式、容错以及可扩展的几大特性。</p>

<p><img src="https://img.nju520.me/2017-10-24-nosql-main-characteristics.png" alt="nosql-main-characteristics" /></p>

<p>虽然两者都是 NoSQL，并且有着相似的特性，但是它们在侧重的方向上有非常明显的不同，从两个数据库论文的标题中，我们就能看到 Amazon 的 Dynamo 追求的是高可用性并且提供的是类似 MongoDB 的 Key-value 文档存储，而 Bigtable 中描述的数据库却可以用于结构化的数据存储。</p>

<p>由于 Bigtable 和 Dynamo 都属于同一个类别 - NoSQL，所以它们经常会被放在一起进行对比，这篇文章不仅会介绍 Dynamo 的设计理念以及架构等问题，还会就其中的部分问题与 Bigtable 中相对应的概念进行对比，这样能够让我们更加清楚地了解不同的数据库对不同问题，因设计理念的差异做出的权衡。</p>

<h2 id="架构">架构</h2>

<p>在数据库领域中尤其是分布式数据库，最重要的就是服务的架构，多数的分布式系统在设计时都会假设服务运行在廉价的节点上，并没有出众的性能和也不能提供稳定的服务，所以水平扩展和容错的能力是分布式数据库的标配；但是不同的分布式数据库选用了不同的架构来组织大量的节点。</p>

<p>很多的分布式服务例如 GFS 和 Bigtable 都使用了带有主节点的架构来维护整个系统中的元数据，包括节点的位置等信息，而 Dynamo 的实现不同于这些中心化的分布式服务，在 Dynamo 中所有的节点都有着完全相同的职责，会对外界提供同样的服务，所以在整个系统中并不会出现单点故障的问题。</p>

<p><img src="https://img.nju520.me/2017-10-24-dynamo-architecture.png" alt="dynamo-architecture" /></p>

<p>去中心化的架构使得系统的水平扩展非常容易，节点可以在任何时候直接加入到整个 Dynamo 的集群中，并且只会造成集群中少量数据的迁移。</p>

<p>Bigtable 使用了中心化的架构，通过主节点来维护整个系统中全部的元数据信息，但是 Bigtable 本身其实并不会处理来自客户端的读写请求，所有请求都会由客户端直接和从节点通信，不过由于有了中心化的主节点，所以主节点一旦发生故障宕机就会造成服务的不可用，虽然 Bigtable 以及类似的服务通过其他方式解决这个问题，但是这个问题仍然是中心化的设计所造成的。</p>

<p><img src="https://img.nju520.me/2017-10-24-centralized-architecture.png" alt="centralized-architecture" /></p>

<p>中心化或者去中心化并不是一个绝对好或者绝对坏的选择，选择中心化的解决方案能够降低系统实现的复杂度，而去中心化的方式能够避免单点故障，让系统能够更好更快地增加新的节点，提供优秀的水平扩展能力。</p>

<h2 id="分片和复制">分片和复制</h2>

<p>Dynamo 在设计之初就定下了<strong>增量扩展</strong>（Incremental Scalability）的核心需求，这也就需要一种能够在一组节点中动态分片的机制，Dynamo 的分片策略依赖于<em>一致性哈希</em>，通过这种策略 Dynamo 能够将负载合理的分配到不同的存储节点上。</p>

<p>所有的键在存储之前都会通过哈希函数得到一个唯一的值，哈希函数的输出被看做是一个固定长度的环，也就是其输出的最大值和最小值是『连接』到一起的：</p>

<p><img src="https://img.nju520.me/2017-10-24-partition-in-dynamo.png" alt="partition-in-dynamo" /></p>

<p>每一个节点都会被 Dynamo 在这个环中分配一个随机的位置，而这个节点会处理从哈希的输出在当前节点前的所有键；假设我们有一个键值对 <code>(hacker, developer)</code>，<code>Hash(hacker)</code> 的结果位于上图中的绿色区域，从环中的位置开始按照<strong>顺时针</strong>的顺序寻找，找到的以第一个节点 B 就会成为协调者（coordinator）负责处理当前的键值对，上图中的每一个节点都会负责与其颜色相同的部分。</p>

<p>由于 Dynamo 系统中的每一个节点在刚刚加入当前的集群时，会被分配一个随机的位置，所以由于算法的随机性可能会导致不同节点处理的范围有所不同，最终每一个节点的负载也并不相同；为了解决这个问题，Dynamo 使用了一致性哈希算法的变种，将同一个物理节点分配到环中的多个位置（标记），成为多个虚拟节点，但是在这种策略下，如果当前的 Dynamo 节点一天处理上百万的请求，那么新增节点为了不影响已有节点的性能，会在后台进行启动，整个过程大约会<strong>消耗一整天</strong>的时间，这其实是很难接受的，除此之外这种策略还会造成系统进行日常归档极其缓慢。</p>

<p><img src="https://img.nju520.me/2017-10-24-equal-size-partition-in-dynamo.png" alt="equal-size-partition-in-dynamo" /></p>

<p>为了解决负载的不均衡的问题，除了上面使用虚拟节点的策略之外，Dynamo 论文中还提供了另外两种策略，其中性能相对较好的是将数据的哈希分成 Q 个大小相等的区域，S 个节点每一个处理 Q/S 个分区，当某一个节点因为故障或者其他原因需要退出集群时，会将它处理的数据分片随机分配给其它的节点，当有节点加入系统时，会从其它的节点中『接管』对应的数据分片。上图只是对这种策略下的分片情况简单展示，在真实环境中分片数 Q 的值远远大于节点数 S。</p>

<p>Dynamo 为了达到高可用性和持久性，防止由于节点宕机故障或者数据丢失，将同一份数据在协调者和随后的 <code>N-1</code> 个节点上备份了多次，N 是一个可以配置的值，在一般情况下都为 3。</p>

<p><img src="https://img.nju520.me/2017-10-24-replication-in-dynamo.png" alt="replication-in-dynamo" /></p>

<p>也就是说，上图中黄色区域的值会存储在三个节点 A、B 和 C 中，绿色的区域会被 B、C、D 三个节点处理，从另一个角度来看，A 节点会处理范围在 <code>(C, A]</code> 之间的值，而 B 节点会处理从 <code>(D, B]</code> 区域内的值。</p>

<p><img src="https://img.nju520.me/2017-10-24-replication-range-in-dynamo.png" alt="replication-range-in-dynamo" /></p>

<p>负责存储某一个特定键值对的节点列表叫做偏好列表（preference list），因为虚拟节点在环中会随机存在，为了保证出现节点故障时不会影响可用性和持久性，偏好列表中的全部节点必须都为<strong>不同的物理节点</strong>。</p>

<p>Bigtable 中对分片和复制的实现其实就与 Dynamo 中完全不同，这不仅是因为 Bigtable 的节点有主从之分，还因为 Bigtable 的设计理念与 Dynamo 完全不同。在 Bigtable 中，数据是按照键的顺序存储的，数据存储的单位都是 tablet，每一张表都由多个 tablet 组成，而每一个的 tablet 都有一个 tablet 服务器来处理，而 tablet 的位置都存储在 METADATA 表中。</p>

<p><img src="https://img.nju520.me/2017-10-24-partition-in-bigtable.png" alt="partition-in-bigtable" /></p>

<p>在 Bigtable 中，所有的 tablet 都在 GFS 中以 SSTable 的格式存储起来，这些 SSTable 都被分成了固定大小的块在 chunkserver 上存储，而每一个块也都会在存储在多个 chunkserver 中。</p>

<h2 id="读写请求的执行">读写请求的执行</h2>

<p>Dynamo 集群中的任意节点都能够接受来自客户端的对于任意键的读写请求，所有的请求都通过 RPC 调用执行，客户端在选择节点时有两种不同的策略：一种是通过一个负载均衡器根据负载选择不同的节点，另一种是通过一个清楚当前集群分片的库直接请求相应的节点。</p>

<p><img src="https://img.nju520.me/2017-10-24-node-selecting-strategies.png" alt="node-selecting-strategies" /></p>

<p>从上面我们就已经知道了处理读写请求的节点就叫做协调者（coordinator），前 N 个『健康』的节点会参与读写请求的处理；Dynamo 使用了 Quorum 一致性协议来保证系统中的一致性，协议中有两个可以配置的值：R 和 W，其中 R 是成功参与一个读请求的最小节点数，而 W 是成功参与写请求的最小节点数。</p>

<p><img src="https://img.nju520.me/2017-10-24-dynamo-read-write-operation.png" alt="dynamo-read-write-operation" /></p>

<p>当 R = 2 时，所有的读请求必须等待两个节点成功返回对应键的结果，才认为当前的请求结束了，也就是说读请求的时间取决于返回最慢的节点，对于写请求来说也是完全相同的；当协调者接收到了来自客户端的写请求 <code>put()</code> 时，它会创建一个新的向量时钟（vector clock），然后将新版本的信息存储在本地，之后向偏好列表（preference list）中的前 <code>N-1</code> 个节点发送消息，直到其中的 <code>W-1</code> 个返回这次请求才成功结束，读请求 <code>get()</code> 与上述请求的唯一区别就是，如果协调者发现节点中的数据出现了冲突，就会对冲突尝试进行解决并将结果重新写回对应的节点。</p>

<h2 id="冲突和向量时钟">冲突和向量时钟</h2>

<p>Dynamo 与目前的绝大多数分布式系统一样都提供了<strong>最终一致性</strong>，最终一致性能够允许我们异步的更新集群中的节点，<code>put()</code> 请求可能会在所有的节点后更新前就返回对应的结果了，在这时随后的 <code>get()</code> 就可能获取到过期的数据。</p>

<p><img src="https://img.nju520.me/2017-10-24-inconsistent-in-dynamo.png" alt="inconsistent-in-dynamo" /></p>

<p>如果在系统中出现了节点故障宕机，那么数据的更新可能在一段时间内都不会到达失效的节点，这也是在使用 Dynamo 或者使用相似原理的系统时会遇到的问题，Amazon 中的很多应用虽然都能够忍受这种数据层面可能发生的不一致性，但是有些对业务数据一致性非常高的应用在选择 Dynamo 时就需要好好考虑了。</p>

<p>因为 Dynamo 在工作的过程中不同的节点可能会发生数据不一致的问题，这种问题肯定是需要解决的，Dynamo 能够确保<strong>一旦数据之间发生了冲突不会丢失</strong>，但是可能会有<strong>已被删除的数据重新出现</strong>的问题。</p>

<p>在多数情况下，Dynamo 中的最新版本的数据都会取代之前的版本，系统在这时可以通过语法调解（syntactic reconcile）数据库中的正确版本。但是版本也可能会出现分支，在这时，Dynamo 就会返回所有它无法处理的数据版本，由客户端在多个版本的数据中选择或者创建（collapse）合适的版本返回给 Dynamo，其实这个过程比较像出现冲突的 <code>git merge</code> 操作，git 没有办法判断当前的哪个版本是合适的，所以只能由开发者对分支之间的冲突进行处理。</p>

<p><img src="https://img.nju520.me/2017-10-24-version-evolution-in-dynamo.png" alt="version-evolution-in-dynamo" /></p>

<p>上图中的每一个对象的版本 Dx 中存储着一个或多个向量时钟 <code>[Sn, N]</code>，每次 Dynamo 对数据进行写入时都会更新向量时钟的版本，节点 Sx 第一次写入时向量时钟为 <code>[Sx, 1]</code>，第二次为 <code>[Sx, 2]</code>，在这时假设节点 Sy 和 Sz 都不知道 Sx 已经对节点进行写入了，它们接收到了来自其他客户端的请求，在本地也对同样键做出了写入并分别生成了不同的时钟 <code>[Sy, 1]</code> 和 <code>[Sz, 1]</code>，当客户端再次使用 <code>get()</code> 请求时就会发现数据出现了冲突，由于 Dynamo 无法根据向量时钟自动解决，所以它需要手动合并三个不同的数据版本。</p>

<p>论文中对 24 小时内的请求进行了统计，其中 99.94% 的请求仅会返回一个版本，0.00057% 的请求会返回两个版本，0.00047 的请求会返回三个版本，0.000009% 的请求会返回四个版本，虽然论文中说：</p>

<blockquote>
  <p>This shows that divergent versions are created rarely.</p>
</blockquote>

<p>但是作者仍然认为在海量的数据面前 99.94% 并不是一个特别高的百分比，处理分歧的数据版本仍然会带来额外的工作量和负担。虽然在这种情况下，数据库本身确实没有足够的信息来解决数据的不一致问题，也确实只能由客户端去解决冲突，但是这种将问题抛给上层去解决的方式并不友好，论文中也提到了 Amazon 中使用 Dynamo 的应用程序也都是能够适应并解决这些数据不一致的问题的，不过对于作者来说，仅仅这一个问题就成为不选择 Dynamo 的理由了。</p>

<h2 id="节点的增删">节点的增删</h2>

<p>因为在分布式系统中节点的失效是非常常见的事情，而节点也很少会因为某些原因永久失效，往往大部分节点会临时宕机然后快速重新加入系统；由于这些原因，Dynamo 选择使用了显式的机制向系统中添加和移除节点。</p>

<p><img src="https://img.nju520.me/2017-10-24-ring-membership.png" alt="ring-membership" /></p>

<p>添加节点时可以使用命令行工具或者浏览器连接 Dynamo 中的任意节点后触发一个成员变动的事件，这个事件会从当前的环中移除或者向环中添加一个新的节点，当节点的信息发生改变时，该节点会通过 Gossip 协议通知它所能通知的最多的节点。</p>

<p><img src="https://img.nju520.me/2017-10-24-gossip-protocol.png" alt="gossip-protoco" /></p>

<p>在 Gossip 协议中，每次通讯的两个节点会对当前系统中的节点信息达成一致；通过节点之间互相传递成员信息，最终整个 Dyanmo 的集群中所有的节点都会就成员信息达成一致，如上图所示，”gossip” 首先会被 C 节点接收，然后它会传递给它能接触到的最多的节点 A、D、F、G 四个节点，然后 “gossip” 会进行二次传播传递给系统中的灰色节点，到此为止系统中的所有节点都得到了最新的 “gossip” 消息。</p>

<p>当我们向 Dynamo 中加入了新的节点时，会发生节点之间的分片转移，假设我们连接上了 Dynamo 数据库，然后添加了一个 X 节点，该节点被分配到了如下图所示的 A 和 B 节点之间。</p>

<p><img src="https://img.nju520.me/2017-10-24-adding-storage-node.png" alt="adding-storage-node" /></p>

<p>新引入的节点 X 会从三个节点 C、D、E 中接受它们管理的分片的一部分，也就是上图中彩色的 <code>(E, A]</code>、<code>(A, B]</code> 和 <code>(B, X]</code> 三个部分，在 X 节点加入集群之前分别属于与其颜色相同的节点管理。</p>

<p>Dynamo 由于其去中心化的架构，节点增删的事件都需要通过 Gossip 协议进行传递，然而拥有主从节点之分的 Bigtable 就不需要上述的方式对集群中的节点进行增删了，它可以直接通过用于管理其他从节点的服务直接注册新的节点或者撤下已有的节点。</p>

<h2 id="副本同步">副本同步</h2>

<p>在 Dynamo 运行的过程中，由于一些情况会造成不同节点中的数据不一致的问题，Dynamo 使用了反信息熵（anti-entropy）的策略保证所有的副本存储的信息都是同步的。</p>

<p>为了快速确认多个副本之间的数据的一致性并避免大量的数据传输，Dynamo 使用了 <a href="https://en.wikipedia.org/wiki/Merkle_tree">Merkle tree</a> 对不同节点中的数据进行快速验证。</p>

<p><img src="https://img.nju520.me/2017-10-24-merkle-hash-tree.png" alt="merkle-hash-tree" /></p>

<p>在 Merkle 树中，所有父节点中的内容都是叶子节点的哈希，通过这种方式构建的树形结构能够保证整棵树不会被篡改，任何的改动都能被立刻发现。</p>

<p>Dynamo 中的每一个节点都为其持有的键的范围维护了一颗 Merkle 树，在验证两份节点中的数据是否相同时，只需要发送根节点中的哈希值，如果相同那么说明两棵树的内容全部相同，否则就会依次对比不同层级节点中的内容，直到找出不同的副本，这种做法虽然能够减少数据的传输并能够快速找到副本之间的不同，但是当有新的节点加入或者旧的节点退出时会导致大量的 Merkle 树重新计算。</p>

<h2 id="总结">总结</h2>

<p>在 Dynamo 的论文公开之后，有一篇文章将 Dynamo 的设计称作 <a href="http://jsensarma.com/blog/?p=55">“A flawed architecture”</a>，这篇文章的作者在文中对 Dynamo 的实现进行了分析，主要对其最终一致性和 Quorom 机制进行了批评，它在 <a href="https://news.ycombinator.com/item?id=915212">HackerNews</a> 上也引起了广泛的讨论，帖子中的很多内容都值得一看，能够帮助我们了解 Dynamo 的设计原理，而 Amazon 的 CTO 对于这篇文章也发了一条 Twitter：</p>

<p><img src="https://img.nju520.me/2017-10-24-amazon-cto-twitter-about-dynamo.png" alt="amazon-cto-twitter-about-dynamo" /></p>

<p>不管如何，Dynamo 作为支撑亚马逊业务的底层服务，其实现原理和思想对于整个社区都是非常有价值的，然而它使用的去中心化的策略也带了很多问题，虽然作者可能会因为这个原因在选择数据库时不会 Dynamo，不过相信它也是有合适的应用场景的。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo: Amazon’s Highly Available Key-value Store</a></li>
  <li><a href="http://jsensarma.com/blog/?p=55">Dynamo: A flawed architecture – Part I</a></li>
  <li><a href="http://jsensarma.com/blog/?p=64">Dynamo – Part I: a followup and re-rebuttals</a></li>
  <li><a href="https://www.slideshare.net/GrishaWeintraub/presentation-46722530">Dynamo and BigTable - Review and Comparison</a></li>
  <li><a href="http://vschart.com/compare/dynamo-db/vs/bigtable">DynamoDB vs. BigTable · vsChart</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Merkle_tree">Merkle tree</a></li>
  <li><a href="https://link.springer.com/content/pdf/10.1007/3-540-48184-2_32.pdf">A Digital Signature Based on a Conventional Encryption Function</a></li>
  <li><a href="http://www.raychase.net/2396">Dynamo 的实现技术和去中心化</a></li>
  <li><a href="http://hwbnju.com/bigtable-leveldb">浅析 Bigtable 和 LevelDB 的实现</a></li>
</ul>

  ]]></description>
</item>

<item>
  <title>『浅入浅出』MongoDB 和 WiredTiger</title>
  <link>//mongodb-wiredtiger</link>
  <author>nju520</author>
  <pubDate>2017-09-06T00:00:00+08:00</pubDate>
  <guid>//mongodb-wiredtiger</guid>
  <description><![CDATA[
  <p>MongoDB 是目前主流的 NoSQL 数据库之一，与关系型数据库和其它的 NoSQL 不同，MongoDB 使用了面向文档的数据存储方式，将数据以类似 JSON 的方式存储在磁盘上，因为项目上的一些历史遗留问题，作者在最近的工作中也不得不经常与 MongoDB 打交道，这也是这篇文章出现的原因。</p>

<p><img src="https://img.nju520.me/2017-09-06-logo.png-1000width" alt="logo" /></p>

<p>虽然在之前也对 MongoDB 有所了解，但是真正在项目中大规模使用还是第一次，使用过程中也暴露了大量的问题，不过在这里，我们主要对 MongoDB 中的一些重要概念的原理进行介绍，也会与 MySQL 这种传统的关系型数据库做一个对比，让读者自行判断它们之间的优势和劣势。</p>

<h2 id="概述">概述</h2>

<p>MongoDB 虽然也是数据库，但是它与传统的 RDBMS 相比有着巨大的不同，很多开发者都认为或者被灌输了一种思想，MongoDB 这种无 Scheme 的数据库相比 RDBMS 有着巨大的性能提升，这个判断其实是一种误解；因为数据库的性能不止与数据库本身的设计有关系，还与开发者对表结构和索引的设计、存储引擎的选择和业务有着巨大的关系，如果认为<strong>仅进行了数据库的替换就能得到数量级的性能提升</strong>，那还是太年轻了。</p>

<p><img src="https://img.nju520.me/2017-09-06-its-not-always-simple-banner.jpg-1000width" alt="its-not-always-simple-banner" /></p>

<h3 id="架构">架构</h3>

<p>现有流行的数据库其实都有着非常相似的架构，MongoDB 其实就与 MySQL 中的架构相差不多，底层都使用了『可插拔』的存储引擎以满足用户的不同需要。</p>

<p><img src="https://img.nju520.me/2017-09-06-MongoDB-Architecture.jpg-1000width" alt="MongoDB-Architecture" /></p>

<p>用户可以根据表中的数据特征选择不同的存储引擎，它们可以在同一个 MongoDB 的实例中使用；在最新版本的 MongoDB 中使用了 WiredTiger 作为默认的存储引擎，WiredTiger 提供了不同粒度的并发控制和压缩机制，能够为不同种类的应用提供了最好的性能和存储效率。</p>

<p>在不同的存储引擎上层的就是 MongoDB 的数据模型和查询语言了，与关系型数据库不同，由于 MongoDB 对数据的存储与 RDBMS 有较大的差异，所以它创建了一套不同的查询语言；虽然 MongoDB 查询语言非常强大，支持的功能也很多，同时也是可编程的，不过其中包含的内容非常繁杂、API 设计也不是非常优雅，所以还是需要一些学习成本的，对于长时间使用 MySQL 的开发者肯定会有些不习惯。</p>

<pre><code class="language-javascript">db.collection.updateMany(
   &lt;filter&gt;,
   &lt;update&gt;,
   {
     upsert: &lt;boolean&gt;,
     writeConcern: &lt;document&gt;,
     collation: &lt;document&gt;
   }
)
</code></pre>

<p>查询语言的复杂是因为 MongoDB 支持了很多的数据类型，同时每一条数据记录也就是文档有着非常复杂的结构，这点是从设计上就没有办法避免的，所以还需要使用 MongoDB 的开发者花一些时间去学习各种各样的 API。</p>

<h3 id="rdbms-与-mongodb">RDBMS 与 MongoDB</h3>

<p>MongoDB 使用面向文档的的数据模型，导致很多概念都与 RDBMS 有一些差别，虽然从总体上来看两者都有相对应的概念，不过概念之间细微的差别其实也会影响我们对 MongoDB 的理解：</p>

<p><img src="https://img.nju520.me/2017-09-06-Translating-Between-RDBMS-and-MongoDB.jpg-1000width" alt="Translating-Between-RDBMS-and-MongoDB" /></p>

<p>传统的 RDBMS 其实使用 <code>Table</code> 的格式将数据逻辑地存储在一张二维的表中，其中不包括任何复杂的数据结构，但是由于 MongoDB 支持嵌入文档、数组和哈希等多种复杂数据结构的使用，所以它最终将所有的数据以 <a href="http://bsonspec.org">BSON</a> 的数据格式存储起来。</p>

<p>RDBMS 和 MongoDB 中的概念都有着相互对应的关系，数据库、表、行和索引的概念在两中数据库中都非常相似，唯独最后的 <code>JOIN</code> 和 <code>Embedded Document</code> 或者 <code>Reference</code> 有着巨大的差别。这一点差别其实也影响了在使用 MongoDB 时对集合（Collection）Schema 的设计，如果我们在 MongoDB 中遵循了与 RDBMS 中相同的思想对 Collection 进行设计，那么就不可避免的使用很多的 “JOIN” 语句，而 MongoDB 是不支持 “JOIN” 的，在应用内做这种查询的性能非常非常差，在这时使用嵌入式的文档其实就可以解决这种问题了，嵌入式的文档虽然可能会造成很多的数据冗余导致我们在更新时会很痛苦，但是查询时确实非常迅速。</p>

<pre><code class="language-javascript">{
  _id: &lt;ObjectId1&gt;,
  name: "nju520",
  books: [
    {
      _id: &lt;ObjectId2&gt;,
      name: "MongoDB: The Definitive Guide"
    },
    {
      _id: &lt;ObjectId3&gt;,
      name: "High Performance MySQL"
    }
  ]
}
</code></pre>

<p>在 MongoDB 的使用时，我们一定要忘记很多 RDBMS 中对于表设计的规则，同时想清楚 MongoDB 的优势，仔细思考如何对表进行设计才能利用 MongoDB 提供的诸多特性提升查询的效率。</p>

<h2 id="数据模型">数据模型</h2>

<p>MongoDB 与 RDBMS 之间最大的不同，就是数据模型的设计有着非常明显的差异，数据模型的不同决定了它有着非常不同的特性，存储在 MongoDB 中的数据有着非常灵活的 Schema，我们不需要像 RDBMS 一样，在插入数据之前就决定并且定义表中的数据结构，MongoDB 的结合不对 Collection 的数据结构进行任何限制，但是在实际使用中，同一个 Collection 中的大多数文档都具有类似的结构。</p>

<p><img src="https://img.nju520.me/2017-09-06-Different-Data-Structure.jpg-1000width" alt="Different-Data-Structure" /></p>

<p>在为 MongoDB 应用设计数据模型时，如何表示数据模型之间的关系其实是需要开发者需要仔细考虑的，MongoDB 为表示文档之间的关系提供了两种不同的方法：引用和嵌入。</p>

<h3 id="标准化数据模型">标准化数据模型</h3>

<p>引用（Reference）在 MongoDB 中被称为标准化的数据模型，它与 MySQL 的外键非常相似，每一个文档都可以通过一个 <code>xx_id</code> 的字段『链接』到其他的文档：</p>

<p><img src="https://img.nju520.me/2017-09-06-Reference-MongoDB.jpg-1000width" alt="Reference-MongoDB" /></p>

<p>但是 MongoDB 中的这种引用不像 MySQL 中可以直接通过 JOIN 进行查找，我们需要使用额外的查询找到该引用对应的模型，这虽然提供了更多的灵活性，不过由于增加了客户端和 MongoDB 之间的交互次数（Round-Trip）也会导致查询变慢，甚至非常严重的性能问题。</p>

<p>MongoDB 中的引用并不会对引用对应的数据模型是否真正存在做出任何的约束，所以如果在应用层级没有对文档之间的关系有所约束，那么就可能会出现引用了指向不存在的文档的问题：</p>

<p><img src="https://img.nju520.me/2017-09-06-Not-Found-Document.jpg-1000width" alt="Not-Found-Document" /></p>

<p>虽然引用有着比较严重的性能问题并且在数据库层面没有对模型是否被删除加上限制，不过它提供的一些特点是嵌入式的文档无法给予了，当我们需要表示多对多关系或者更加庞大的数据集时，就可以考虑使用标准化的数据模型 — 引用了。</p>

<h3 id="嵌入式数据模型">嵌入式数据模型</h3>

<p>除了与 MySQL 中非常相似的引用，MongoDB 由于其独特的数据存储方式，还提供了嵌入式的数据模型，嵌入式的数据模型也被认为是不标准的数据模型：</p>

<p><img src="https://img.nju520.me/2017-09-06-Embedded-Data-Models-MongoDB.jpg-1000width" alt="Embedded-Data-Models-MongoDB" /></p>

<p>因为 MongoDB 使用 BSON 的数据格式对数据进行存储，而嵌入式数据模型中的子文档其实就是父文档中的另一个值，只是其中存储的是一个对象：</p>

<pre><code class="language-javascript">{
  _id: &lt;ObjectId1&gt;,
  username: "nju520",
  age: 20,
  contact: [
    {
      _id: &lt;ObjectId2&gt;,
      email: "i@nju520.me"
    }
  ]
}
</code></pre>

<p>嵌入式的数据模型允许我们将有相同的关系的信息存储在同一个数据记录中，这样应用就可以更快地对相关的数据进行查询和更新了；当我们的数据模型中有『包含』这样的关系或者模型经常需要与其他模型一起出现（查询）时，比如文章和评论，那么就可以考虑使用嵌入式的关系对数据模型进行设计。</p>

<p>总而言之，嵌入的使用让我们在更少的请求中获得更多的相关数据，能够为读操作提供更高的性能，也为在同一个写请求中同时更新相关数据提供了支持。</p>

<blockquote>
  <p>MongoDB 底层的 WiredTiger 存储引擎能够保证对于同一个文档的操作都是原子的，任意一个写操作都不能原子性地影响多个文档或者多个集合。</p>
</blockquote>

<h2 id="主键和索引">主键和索引</h2>

<p>在这一节中，我们将主要介绍 MongoDB 中不同类型的索引，当然也包括每个文档中非常重要的字段 <code>_id</code>，可以<strong>理解</strong>为 MongoDB 的『主键』，除此之外还会介绍单字段索引、复合索引以及多键索引等类型的索引。</p>

<p>MongoDB 中索引的概念其实与 MySQL 中的索引相差不多，无论是底层的数据结构还是基本的索引类型都几乎完全相同，两者之间的区别就在于因为 MongoDB 支持了不同类型的数据结构，所以也理所应当地提供了更多的索引种类。</p>

<p><img src="https://img.nju520.me/2017-09-06-MongoDB-Indexes.jpg-1000width" alt="MongoDB-Indexes" /></p>

<h3 id="默认索引">默认索引</h3>

<p>MySQL 中的每一个数据行都具有一个主键，数据库中的数据都是按照以主键作为键物理地存储在文件中的；除了用于数据的存储，主键由于其特性也能够加速数据库的查询语句。</p>

<p>而 MongoDB 中所有的文档也都有一个唯一的 <code>_id</code> 字段，在默认情况下所有的文档都使用一个长 12 字节的 <code>ObjectId</code> 作为默认索引：</p>

<p><img src="https://img.nju520.me/2017-09-06-MongoDB-ObjectId.jpg-1000width" alt="MongoDB-ObjectId" /></p>

<p>前四位代表当前 <code>_id</code> 生成时的 Unix 时间戳，在这之后是三位的机器标识符和两位的处理器标识符，最后是一个三位的计数器，初始值就是一个随机数；通过这种方式代替递增的 <code>id</code> 能够解决分布式的 MongoDB 生成唯一标识符的问题，同时可以在一定程度上保证 <code>id</code> 的的增长是递增的。</p>

<h3 id="单字段索引single-field">单字段索引（Single Field）</h3>

<p>除了 MongoDB 提供的默认 <code>_id</code> 字段之外，我们还可以建立其它的单键索引，而且其中不止支持顺序的索引，还支持对索引倒排：</p>

<pre><code class="language-javasciprt">db.users.createIndex( { age: -1 } )
</code></pre>

<p>MySQL8.0 之前的索引都只能是正序排列的，在 8.0 之后才引入了逆序的索引，单一字段索引可以说是 MySQL 中的辅助（Secondary）索引的一个子集，它只是对除了 <code>_id</code> 外的任意单一字段建立起正序或者逆序的索引树。</p>

<p><img src="https://img.nju520.me/2017-09-06-Single-Field-Index.jpg-1000width" alt="Single-Field-Index" /></p>

<h3 id="复合索引compound">复合索引（Compound）</h3>

<p>除了单一字段索引这种非常简单的索引类型之外，MongoDB 还支持多个不同字段组成的复合索引（Compound Index），由于 MongoDB 中支持对同一字段的正逆序排列，所以相比于 MySQL 中的辅助索引就会出现更多的情况：</p>

<pre><code class="language-javascript">db.users.createIndex( { username: 1, age: -1 } )
db.users.createIndex( { username: 1, age: 1 } )
</code></pre>

<p>上面的两个索引是完全不同的，在磁盘上的 B+ 树其实也按照了完全不同的顺序进行存储，虽然 <code>username</code> 字段都是升序排列的，但是对于 <code>age</code> 来说，两个索引的处理是完全相反的：</p>

<p><img src="https://img.nju520.me/2017-09-06-Compound-Index.jpg-1000width" alt="Compound-Index" /></p>

<p>这也就造成了在使用查询语句对集合中数据进行查找时，如果约定了正逆序，那么其实是会使用不同的索引的，所以在索引创建时一定要考虑好使用的场景，避免创建无用的索引。</p>

<h3 id="多键索引multikey">多键索引（Multikey）</h3>

<p>由于 MongoDB 支持了类似数组的数据结构，所以也提供了名为多键索引的功能，可以将数组中的每一个元素进行索引，索引的创建其实与单字段索引没有太多的区别：</p>

<pre><code class="language-javascript">db.collection.createIndex( { address: 1 } )
</code></pre>

<p>如果一个字段是值是数组，那么在使用上述代码时会自动为这个字段创建一个多键索引，能够加速对数组中元素的查找。</p>

<h3 id="文本索引text">文本索引（Text）</h3>

<p>文本索引是 MongoDB 为我们提供的另一个比较实用的功能，不过在这里也只是对这种类型的索引提一下，也不打算深入去谈谈这东西的性能如何，如果真的要做全文索引的话，还是推荐使用 Elasticsearch 这种更专业的东西来做，而不是使用 MongoDB 提供的这项功能。</p>

<h2 id="存储">存储</h2>

<p>如何存储数据就是一个比较重要的问题，在前面我们已经提到了 MongoDB 与 MySQL 一样都提供了插件化的存储引擎支持，作为 MongoDB 的主要组件之一，存储引擎全权负责了 MongoDB 对数据的管理。</p>

<p><img src="https://img.nju520.me/2017-09-06-Multiple-Storage-Engines.jpg-1000width" alt="Multiple-Storage-Engines" /></p>

<h3 id="wiredtiger">WiredTiger</h3>

<p>MongoDB3.2 之后 WiredTiger 就是默认的存储引擎了，如果对各个存储引擎并不了解，那么还是不要改变 MongoDB 的默认存储引擎；它有着非常多的优点，比如拥有效率非常高的缓存机制：</p>

<p><img src="https://img.nju520.me/2017-09-06-WiredTiger-Cache.jpg-1000width" alt="WiredTiger-Cache" /></p>

<p>WiredTiger 还支持在内存中和磁盘上对索引进行压缩，在压缩时也使用了前缀压缩的方式以减少 RAM 的使用，在后面的文章中我们会详细介绍和分析 WiredTiger 存储引擎是如何对各种数据进行存储的。</p>

<h3 id="journaling">Journaling</h3>

<p>为了在数据库宕机保证 MongoDB 中数据的持久性，MongoDB 使用了 Write Ahead Logging 向磁盘上的 journal 文件预先进行写入；除了 journal 日志，MongoDB 还使用检查点（Checkpoint）来保证数据的一致性，当数据库发生宕机时，我们就需要 Checkpoint 和 journal 文件协作完成数据的恢复工作：</p>

<ol>
  <li>在数据文件中查找上一个检查点的标识符；</li>
  <li>在 journal 文件中查找标识符对应的记录；</li>
  <li>重做对应记录之后的全部操作；</li>
</ol>

<p>MongoDB 会每隔 60s 或者在 journal 数据的写入达到 2GB 时设置一次检查点，当然我们也可以通过在写入时传入 <code>j: true</code> 的参数强制 journal 文件的同步。</p>

<p><img src="https://img.nju520.me/2017-09-06-Checkpoints-Conditions.jpg-1000width" alt="Checkpoints-Conditions" /></p>

<p>这篇文章并不会介绍 Journal 文件的格式以及相关的内容，作者可能会在之后介绍分析 WiredTiger 的文章中简单分析其存储格式以及一些其它特性。</p>

<h2 id="总结">总结</h2>

<p>这篇文章中只是对 MongoDB 的一些基本特性以及数据模型做了简单的介绍，虽然『无限』扩展是 MongoDB 非常重要的特性，但是由于篇幅所限，我们并没有介绍任何跟 MongoDB 集群相关的信息，不过会在之后的文章中专门介绍多实例的 MongoDB 是如何协同工作的。</p>

<p>在这里，我想说的是，如果各位读者接收到了类似 MongoDB 比 MySQL 性能好很多的断言，但是在使用 MongoDB 的过程中仍然遵循以往 RDBMS 对数据库的设计方式，那么我相信性能在最终也不会有太大的提升，反而可能会不升反降；只有真正理解 MongoDB 的数据模型，并且根据业务的需要进行设计才能很好地利用类似嵌入式文档等特性并提升 MongoDB 的性能。</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://www.mongodb.com/mongodb-architecture">MongoDB Architecture</a></li>
  <li><a href="https://www.mongodb.com/blog/post/thinking-documents-part-1?jmp=docs">Thinking in Documents: Part 1</a></li>
  <li><a href="https://db-engines.com/en/ranking">DB-Engines Ranking</a></li>
  <li><a href="https://docs.mongodb.com/manual/core/data-modeling-introduction/">Data Modeling Introduction</a></li>
  <li><a href="https://www.mongodb.com/blog/post/building-applications-with-mongodbs-pluggable-storage-engines-part-1?jmp=docs">Building Applications with MongoDB’s Pluggable Storage Engines: Part 1</a></li>
</ul>

  ]]></description>
</item>

<item>
  <title>浅析 Bigtable 和 LevelDB 的实现</title>
  <link>//bigtable-leveldb</link>
  <author>nju520</author>
  <pubDate>2017-08-12T00:00:00+08:00</pubDate>
  <guid>//bigtable-leveldb</guid>
  <description><![CDATA[
  <p>在 2006 年的 OSDI 上，Google 发布了名为 <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a> 的论文，其中描述了一个用于管理结构化数据的分布式存储系统  - Bigtable 的数据模型、接口以及实现等内容。</p>

<p><img src="https://img.nju520.me/2017-08-12-leveldb-logo.png-1000width" alt="leveldb-logo" /></p>

<p>本文会先对 Bigtable 一文中描述的分布式存储系统进行简单的描述，然后对 Google 开源的 KV 存储数据库 <a href="https://github.com/google/leveldb">LevelDB</a> 进行分析；LevelDB 可以理解为单点的 Bigtable 的系统，虽然其中没有 Bigtable 中与 tablet 管理以及一些分布式相关的逻辑，不过我们可以通过对 LevelDB 源代码的阅读增加对 Bigtable 的理解。</p>

<h2 id="bigtable">Bigtable</h2>

<p>Bigtable 是一个用于管理<strong>结构化数据</strong>的分布式存储系统，它有非常优秀的扩展性，可以同时处理上千台机器中的 PB 级别的数据；Google 中的很多项目，包括 Web 索引都使用 Bigtable 来存储海量的数据；Bigtable 的论文中声称它实现了四个目标：</p>

<p><img src="https://img.nju520.me/2017-08-12-Goals-of-Bigtable.jpg-1000width" alt="Goals-of-Bigtable" /></p>

<p>在作者看来这些目标看看就好，其实并没有什么太大的意义，所有的项目都会对外宣称它们达到了高性能、高可用性等等特性，我们需要关注的是 Bigtable 到底是如何实现的。</p>

<h3 id="数据模型">数据模型</h3>

<p>Bigtable 与数据库在很多方面都非常相似，但是它提供了与数据库不同的接口，它并没有支持全部的关系型数据模型，反而使用了简单的数据模型，使数据可以被更灵活的控制和管理。</p>

<p>在实现中，Bigtable 其实就是一个稀疏的、分布式的、多维持久有序哈希。</p>

<blockquote>
  <p>A Bigtable is a sparse, distributed, persistent multi-dimensional sorted map.</p>
</blockquote>

<p>它的定义其实也就决定了其数据模型非常简单并且易于实现，我们使用 <code>row</code>、<code>column</code> 和 <code>timestamp</code> 三个字段作为这个哈希的键，值就是一个字节数组，也可以理解为字符串。</p>

<p><img src="https://img.nju520.me/2017-08-12-Bigtable-DataModel-Row-Column-Timestamp-Value.jpg-1000width" alt="Bigtable-DataModel-Row-Column-Timestamp-Value" /></p>

<p>这里最重要的就是 <code>row</code> 的值，它的长度最大可以为 64KB，对于同一 <code>row</code> 下数据的读写都可以看做是原子的；因为 Bigtable 是按照 <code>row</code> 的值使用字典顺序进行排序的，每一段 <code>row</code> 的范围都会被 Bigtable 进行分区，并交给一个 tablet 进行处理。</p>

<h3 id="实现">实现</h3>

<p>在这一节中，我们将介绍 Bigtable 论文对于其本身实现的描述，其中包含很多内容：tablet 的组织形式、tablet 的管理、读写请求的处理以及数据的压缩等几个部分。</p>

<h4 id="tablet-的组织形式">tablet 的组织形式</h4>

<p>我们使用类似 B+ 树的三层结构来存储 tablet 的位置信息，第一层是一个单独的 <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf">Chubby</a> 文件，其中保存了根 tablet 的位置。</p>

<blockquote>
  <p>Chubby 是一个分布式锁服务，我们可能会在后面的文章中介绍它。</p>
</blockquote>

<p><img src="https://img.nju520.me/2017-08-12-Tablet-Location-Hierarchy.jpg-1000width" alt="Tablet-Location-Hierarchy" /></p>

<p>每一个 METADATA tablet 包括根节点上的 tablet 都存储了 tablet 的位置和该 tablet 中 key 的最小值和最大值；每一个 METADATA 行大约在内存中存储了 1KB 的数据，如果每一个 METADATA tablet 的大小都为 128MB，那么整个三层结构可以存储 2^61 字节的数据。</p>

<h4 id="tablet-的管理">tablet 的管理</h4>

<p>既然在整个 Bigtable 中有着海量的 tablet 服务器以及数据的分片 tablet，那么 Bigtable 是如何管理海量的数据呢？Bigtable 与很多的分布式系统一样，使用一个主服务器将 tablet 分派给不同的服务器节点。</p>

<p><img src="https://img.nju520.me/2017-08-12-Master-Manage-Tablet-Servers-And-Tablets.jpg-1000width" alt="Master-Manage-Tablet-Servers-And-Tablets" /></p>

<p>为了减轻主服务器的负载，所有的客户端仅仅通过 Master 获取 tablet 服务器的位置信息，它并不会在每次读写时都请求 Master 节点，而是直接与 tablet 服务器相连，同时客户端本身也会保存一份 tablet 服务器位置的缓存以减少与 Master 通信的次数和频率。</p>

<h4 id="读写请求的处理">读写请求的处理</h4>

<p>从读写请求的处理，我们其实可以看出整个 Bigtable 中的各个部分是如何协作的，包括日志、memtable 以及 SSTable 文件。</p>

<p><img src="https://img.nju520.me/2017-08-12-Tablet-Serving.jpg-1000width" alt="Tablet-Serving" /></p>

<p>当有客户端向 tablet 服务器发送写操作时，它会先向 tablet 服务器中的日志追加一条记录，在日志成功追加之后再向 memtable 中插入该条记录；这与现在大多的数据库的实现完全相同，通过顺序写向日志追加记录，然后再向数据库随机写，因为随机写的耗时远远大于追加内容，如果直接进行随机写，可能由于发生设备故障造成数据丢失。</p>

<p>当 tablet 服务器接收到读操作时，它会在 memtable 和 SSTable 上进行合并查找，因为 memtable 和 SSTable 中对于键值的存储都是字典顺序的，所以整个读操作的执行会非常快。</p>

<h4 id="表的压缩">表的压缩</h4>

<p>随着写操作的进行，memtable 会随着事件的推移逐渐增大，当 memtable 的大小超过一定的阈值时，就会将当前的 memtable 冻结，并且创建一个新的 memtable，被冻结的 memtable 会被转换为一个 SSTable 并且写入到 GFS 系统中，这种压缩方式也被称作 <em>Minor Compaction</em>。</p>

<p><img src="https://img.nju520.me/2017-08-12-Minor-Compaction.jpg-1000width" alt="Minor-Compaction" /></p>

<p>每一个 Minor Compaction 都能够创建一个新的 SSTable，它能够有效地降低内存的占用并且降低服务进程异常退出后，过大的日志导致的过长的恢复时间。既然有用于压缩 memtable 中数据的 Minor Compaction，那么就一定有一个对应的 Major Compaction 操作。</p>

<p><img src="https://img.nju520.me/2017-08-12-Major-Compaction.jpg-1000width" alt="Major-Compaction" /></p>

<p>Bigtable 会在<strong>后台周期性</strong>地进行 <em>Major Compaction</em>，将 memtable 中的数据和一部分的 SSTable 作为输入，将其中的键值进行归并排序，生成新的 SSTable 并移除原有的 memtable 和 SSTable，新生成的 SSTable 中包含前两者的全部数据和信息，并且将其中一部分标记未删除的信息彻底清除。</p>

<h4 id="小结">小结</h4>

<p>到这里为止，对于 Google 的 Bigtable 论文的介绍就差不多完成了，当然本文只介绍了其中的一部分内容，关于压缩算法的实现细节、缓存以及提交日志的实现等问题我们都没有涉及，想要了解更多相关信息的读者，这里强烈推荐去看一遍 Bigtable 这篇论文的原文 <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a> 以增强对其实现的理解。</p>

<h2 id="leveldb">LevelDB</h2>

<p>文章前面对于 Bigtable 的介绍其实都是对 <a href="https://github.com/google/leveldb">LevelDB</a> 这部分内容所做的铺垫，当然这并不是说前面的内容就不重要，LevelDB 是对 Bigtable 论文中描述的键值存储系统的单机版的实现，它提供了一个极其高速的键值存储系统，并且由 Bigtable 的作者 <a href="https://research.google.com/pubs/jeff.html">Jeff Dean</a> 和 <a href="https://research.google.com/pubs/SanjayGhemawat.html">Sanjay Ghemawat</a> 共同完成，可以说高度复刻了 Bigtable 论文中对于其实现的描述。</p>

<p>因为 Bigtable 只是一篇论文，同时又因为其实现依赖于 Google 的一些不开源的基础服务：GFS、Chubby 等等，我们很难接触到它的源代码，不过我们可以通过 LevelDB 更好地了解这篇论文中提到的诸多内容和思量。</p>

<h3 id="概述">概述</h3>

<p>LevelDB 作为一个键值存储的『仓库』，它提供了一组非常简单的增删改查接口：</p>

<pre><code class="language-cpp">class DB {
 public:
  virtual Status Put(const WriteOptions&amp; options, const Slice&amp; key, const Slice&amp; value) = 0;
  virtual Status Delete(const WriteOptions&amp; options, const Slice&amp; key) = 0;
  virtual Status Write(const WriteOptions&amp; options, WriteBatch* updates) = 0;
  virtual Status Get(const ReadOptions&amp; options, const Slice&amp; key, std::string* value) = 0;
}
</code></pre>

<blockquote>
  <p><code>Put</code> 方法在内部最终会调用 <code>Write</code> 方法，只是在上层为调用者提供了两个不同的选择。</p>
</blockquote>

<p><code>Get</code> 和 <code>Put</code> 是 LevelDB 为上层提供的用于读写的接口，如果我们能够对读写的过程有一个非常清晰的认知，那么理解 LevelDB 的实现就不是那么困难了。</p>

<p>在这一节中，我们将先通过对读写操作的分析了解整个工程中的一些实现，并在遇到问题和新的概念时进行解释，我们会在这个过程中一步一步介绍 LevelDB 中一些重要模块的实现以达到掌握它的原理的目标。</p>

<h3 id="从写操作开始">从写操作开始</h3>

<p>首先来看 <code>Get</code> 和 <code>Put</code> 两者中的写方法：</p>

<pre><code class="language-cpp">Status DB::Put(const WriteOptions&amp; opt, const Slice&amp; key, const Slice&amp; value) {
  WriteBatch batch;
  batch.Put(key, value);
  return Write(opt, &amp;batch);
}

Status DBImpl::Write(const WriteOptions&amp; options, WriteBatch* my_batch) {
    ...
}
</code></pre>

<p>正如上面所介绍的，<code>DB::Put</code> 方法将传入的参数封装成了一个 <code>WritaBatch</code>，然后仍然会执行 <code>DBImpl::Write</code> 方法向数据库中写入数据；写入方法 <code>DBImpl::Write</code> 其实是一个是非常复杂的过程，包含了很多对上下文状态的判断，我们先来看一个写操作的整体逻辑：</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Put.jpg-1000width" alt="LevelDB-Put" /></p>

<p>从总体上看，LevelDB 在对数据库执行写操作时，会有三个步骤：</p>

<ol>
  <li>调用 <code>MakeRoomForWrite</code> 方法为即将进行的写入提供足够的空间；
    <ul>
      <li>在这个过程中，由于 memtable 中空间的不足可能会冻结当前的 memtable，发生 Minor Compaction 并创建一个新的 <code>MemTable</code> 对象；</li>
      <li>在某些条件满足时，也可能发生 Major Compaction，对数据库中的 SSTable 进行压缩；</li>
    </ul>
  </li>
  <li>通过 <code>AddRecord</code> 方法向日志中追加一条写操作的记录；</li>
  <li>再向日志成功写入记录后，我们使用 <code>InsertInto</code> 直接插入 memtable 中，完成整个写操作的流程；</li>
</ol>

<p>在这里，我们并不会提供 LevelDB 对于 <code>Put</code> 方法实现的全部代码，只会展示一份精简后的代码，帮助我们大致了解一下整个写操作的流程：</p>

<pre><code class="language-cpp">Status DBImpl::Write(const WriteOptions&amp; options, WriteBatch* my_batch) {
  Writer w(&amp;mutex_);
  w.batch = my_batch;

  MakeRoomForWrite(my_batch == NULL);

  uint64_t last_sequence = versions_-&gt;LastSequence();
  Writer* last_writer = &amp;w;
  WriteBatch* updates = BuildBatchGroup(&amp;last_writer);
  WriteBatchInternal::SetSequence(updates, last_sequence + 1);
  last_sequence += WriteBatchInternal::Count(updates);

  log_-&gt;AddRecord(WriteBatchInternal::Contents(updates));
  WriteBatchInternal::InsertInto(updates, mem_);

  versions_-&gt;SetLastSequence(last_sequence);
  return Status::OK();
}
</code></pre>

<h4 id="不可变的-memtable">不可变的 memtable</h4>

<p>在写操作的实现代码 <code>DBImpl::Put</code> 中，写操作的准备过程 <code>MakeRoomForWrite</code> 是我们需要注意的一个方法：</p>

<pre><code class="language-cpp">Status DBImpl::MakeRoomForWrite(bool force) {
  uint64_t new_log_number = versions_-&gt;NewFileNumber();
  WritableFile* lfile = NULL;
  env_-&gt;NewWritableFile(LogFileName(dbname_, new_log_number), &amp;lfile);

  delete log_;
  delete logfile_;
  logfile_ = lfile;
  logfile_number_ = new_log_number;
  log_ = new log::Writer(lfile);
  imm_ = mem_;
  has_imm_.Release_Store(imm_);
  mem_ = new MemTable(internal_comparator_);
  mem_-&gt;Ref();
  MaybeScheduleCompaction();
  return Status::OK();
}
</code></pre>

<p>当 LevelDB 中的 memtable 已经被数据填满导致内存已经快不够用的时候，我们会开始对 memtable 中的数据进行冻结并创建一个新的 <code>MemTable</code> 对象。</p>

<p><img src="https://img.nju520.me/2017-08-12-Immutable-MemTable.jpg-1000width" alt="Immutable-MemTable" /></p>

<p>你可以看到，与 Bigtable 中论文不同的是，LevelDB 中引入了一个不可变的 memtable 结构 imm，它的结构与 memtable 完全相同，只是其中的所有数据都是不可变的。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Serving.jpg-1000width" alt="LevelDB-Serving" /></p>

<p>在切换到新的 memtable 之后，还可能会执行 <code>MaybeScheduleCompaction</code> 来触发一次 Minor Compaction 将 imm 中数据固化成数据库中的 SSTable；imm 的引入能够解决由于 memtable 中数据过大导致压缩时不可写入数据的问题。</p>

<p>引入 imm 后，如果 memtable 中的数据过多，我们可以直接将 memtable 指针赋值给 imm，然后创建一个新的 MemTable 实例，这样就可以继续接受外界的写操作，不再需要等待 Minor Compaction 的结束了。</p>

<h4 id="日志记录的格式">日志记录的格式</h4>

<p>作为一个持久存储的 KV 数据库，LevelDB 一定要有日志模块以支持错误发生时恢复数据，我们想要深入了解 LevelDB 的实现，那么日志的格式是一定绕不开的问题；这里并不打算展示用于追加日志的方法 <code>AddRecord</code> 的实现，因为方法中只是实现了对表头和字符串的拼接。</p>

<p>日志在 LevelDB 是以块的形式存储的，每一个块的长度都是 32KB，<strong>固定的块长度</strong>也就决定了日志可能存放在块中的任意位置，LevelDB 中通过引入一位 <code>RecordType</code> 来表示当前记录在块中的位置：</p>

<pre><code class="language-cpp">enum RecordType {
  // Zero is reserved for preallocated files
  kZeroType = 0,
  kFullType = 1,
  // For fragments
  kFirstType = 2,
  kMiddleType = 3,
  kLastType = 4
};
</code></pre>

<p>日志记录的类型存储在该条记录的头部，其中还存储了 4 字节日志的 CRC 校验、记录的长度等信息：</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-log-format-and-recordtype.jpg-1000width" alt="LevelDB-log-format-and-recordtype" /></p>

<p>上图中一共包含 4 个块，其中存储着 6 条日志记录，我们可以通过 <code>RecordType</code> 对每一条日志记录或者日志记录的一部分进行标记，并在日志需要使用时通过该信息重新构造出这条日志记录。</p>

<pre><code class="language-cpp">virtual Status Sync() {
  Status s = SyncDirIfManifest();
  if (fflush_unlocked(file_) != 0 ||
      fdatasync(fileno(file_)) != 0) {
    s = Status::IOError(filename_, strerror(errno));
  }
  return s;
}
</code></pre>

<p>因为向日志中写新记录都是顺序写的，所以它写入的速度非常快，当在内存中写入完成时，也会直接将缓冲区的这部分的内容 <code>fflush</code> 到磁盘上，实现对记录的持久化，用于之后的错误恢复等操作。</p>

<h4 id="记录的插入">记录的插入</h4>

<p>当一条数据的记录写入日志时，这条记录仍然无法被查询，只有当该数据写入 memtable 后才可以被查询，而这也是这一节将要介绍的内容，无论是数据的插入还是数据的删除都会向 memtable 中添加一条记录。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Memtable-Key-Value-Format.jpg-1000width" alt="LevelDB-Memtable-Key-Value-Format" /></p>

<p>添加和删除的记录的区别就是它们使用了不用的 <code>ValueType</code> 标记，插入的数据会将其设置为 <code>kTypeValue</code>，删除的操作会标记为 <code>kTypeDeletion</code>；但是它们实际上都向 memtable 中插入了一条数据。</p>

<pre><code class="language-cpp">virtual void Put(const Slice&amp; key, const Slice&amp; value) {
  mem_-&gt;Add(sequence_, kTypeValue, key, value);
  sequence_++;
}
virtual void Delete(const Slice&amp; key) {
  mem_-&gt;Add(sequence_, kTypeDeletion, key, Slice());
  sequence_++;
}
</code></pre>

<p>我们可以看到它们都调用了 memtable 的 <code>Add</code> 方法，向其内部的数据结构 skiplist 以上图展示的格式插入数据，这条数据中既包含了该记录的键值、序列号以及这条记录的种类，这些字段会在拼接后存入 skiplist；既然我们并没有在 memtable 中对数据进行删除，那么我们是如何保证每次取到的数据都是最新的呢？首先，在 skiplist 中，我们使用了自己定义的一个 <code>comparator</code>：</p>

<pre><code class="language-cpp">int InternalKeyComparator::Compare(const Slice&amp; akey, const Slice&amp; bkey) const {
  int r = user_comparator_-&gt;Compare(ExtractUserKey(akey), ExtractUserKey(bkey));
  if (r == 0) {
    const uint64_t anum = DecodeFixed64(akey.data() + akey.size() - 8);
    const uint64_t bnum = DecodeFixed64(bkey.data() + bkey.size() - 8);
    if (anum &gt; bnum) {
      r = -1;
    } else if (anum &lt; bnum) {
      r = +1;
    }
  }
  return r;
}
</code></pre>

<blockquote>
  <p>比较的两个 key 中的数据可能包含的内容都不完全相同，有的会包含键值、序列号等全部信息，但是例如从 <code>Get</code> 方法调用过来的 key 中可能就只包含键的长度、键值和序列号了，但是这并不影响这里对数据的提取，因为我们只从每个 key 的头部提取信息，所以无论是完整的 key/value 还是单独的 key，我们都不会取到 key 之外的任何数据。</p>
</blockquote>

<p>该方法分别从两个不同的 key 中取出键和序列号，然后对它们进行比较；比较的过程就是使用 <code>InternalKeyComparator</code> 比较器，它通过 <code>user_key</code> 和 <code>sequence_number</code> 进行排序，其中 <code>user_key</code> 按照递增的顺序排序、<code>sequence_number</code> 按照递减的顺序排序，因为随着数据的插入序列号是不断递增的，所以我们可以保证先取到的都是最新的数据或者删除信息。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-MemTable-SkipList.jpg-1000width" alt="LevelDB-MemTable-SkipList" /></p>

<p>在序列号的帮助下，我们并不需要对历史数据进行删除，同时也能加快写操作的速度，提升 LevelDB 的写性能。</p>

<h3 id="数据的读取">数据的读取</h3>

<p>从 LevelDB 中读取数据其实并不复杂，memtable 和 imm 更像是两级缓存，它们在内存中提供了更快的访问速度，如果能直接从内存中的这两处直接获取到响应的值，那么它们一定是最新的数据。</p>

<blockquote>
  <p>LevelDB 总会将新的键值对写在最前面，并在数据压缩时删除历史数据。</p>
</blockquote>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Read-Processes.jpg-1000width" alt="LevelDB-Read-Processes" /></p>

<p>数据的读取是按照 MemTable、Immutable MemTable 以及不同层级的 SSTable 的顺序进行的，前两者都是在内存中，后面不同层级的 SSTable 都是以 <code>*.ldb</code> 文件的形式持久存储在磁盘上，而正是因为有着不同层级的 SSTable，所以我们的数据库的名字叫做 LevelDB。</p>

<p>精简后的读操作方法的实现代码是这样的，方法的脉络非常清晰，作者相信这里也不需要过多的解释：</p>

<pre><code class="language-cpp">Status DBImpl::Get(const ReadOptions&amp; options, const Slice&amp; key, std::string* value) {
  LookupKey lkey(key, versions_-&gt;LastSequence());
  if (mem_-&gt;Get(lkey, value, NULL)) {
    // Done
  } else if (imm_ != NULL &amp;&amp; imm_-&gt;Get(lkey, value, NULL)) {
    // Done
  } else {
    versions_-&gt;current()-&gt;Get(options, lkey, value, NULL);
  }

  MaybeScheduleCompaction();
  return Status::OK();
}
</code></pre>

<p>当 LevelDB 在 memtable 和 imm 中查询到结果时，如果查询到了数据并不一定表示当前的值一定存在，它仍然需要判断 <code>ValueType</code> 来确定当前记录是否被删除。</p>

<h4 id="多层级的-sstable">多层级的 SSTable</h4>

<p>当 LevelDB 在内存中没有找到对应的数据时，它才会到磁盘中多个层级的 SSTable 中进行查找，这个过程就稍微有一点复杂了，LevelDB 会在多个层级中逐级进行查找，并且不会跳过其中的任何层级；在查找的过程就涉及到一个非常重要的数据结构 <code>FileMetaData</code>：</p>

<p><img src="https://img.nju520.me/2017-08-12-FileMetaData.jpg-1000width" alt="FileMetaData" /></p>

<p><code>FileMetaData</code> 中包含了整个文件的全部信息，其中包括键的最大值和最小值、允许查找的次数、文件被引用的次数、文件的大小以及文件号，因为所有的 <code>SSTable</code> 都是以固定的形式存储在同一目录下的，所以我们可以通过文件号轻松查找到对应的文件。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Level0-Layer.jpg-1000width" alt="LevelDB-Level0-Laye" /></p>

<p>查找的顺序就是从低到高了，LevelDB 首先会在 Level0 中查找对应的键。但是，与其他层级不同，Level0 中多个 SSTable 的键的范围有重合部分的，在查找对应值的过程中，会依次查找 Level0 中固定的 4 个 SSTable。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-LevelN-Layers.jpg-1000width" alt="LevelDB-LevelN-Layers" /></p>

<p>但是当涉及到更高层级的 SSTable 时，因为同一层级的 SSTable 都是没有重叠部分的，所以我们在查找时可以利用已知的 SSTable 中的极值信息 <code>smallest/largest</code> 快速查找到对应的 SSTable，再判断当前的 SSTable 是否包含查询的 key，如果不存在，就继续查找下一个层级直到最后的一个层级 <code>kNumLevels</code>（默认为 7 级）或者查询到了对应的值。</p>

<h4 id="sstable-的合并">SSTable 的『合并』</h4>

<p>既然 LevelDB 中的数据是通过多个层级的 SSTable 组织的，那么它是如何对不同层级中的 SSTable 进行合并和压缩的呢；与 Bigtable 论文中描述的两种 Compaction 几乎完全相同，LevelDB 对这两种压缩的方式都进行了实现。</p>

<p>无论是读操作还是写操作，在执行的过程中都可能调用 <code>MaybeScheduleCompaction</code> 来尝试对数据库中的 SSTable 进行合并，当合并的条件满足时，最终都会执行 <code>BackgroundCompaction</code> 方法在后台完成这个步骤。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-BackgroundCompaction-Processes.jpg-1000width" alt="LevelDB-BackgroundCompaction-Processes" /></p>

<p>这种合并分为两种情况，一种是 Minor Compaction，即内存中的数据超过了 memtable 大小的最大限制，改 memtable 被冻结为不可变的 imm，然后执行方法 <code>CompactMemTable()</code> 对内存表进行压缩。</p>

<pre><code class="language-cpp">void DBImpl::CompactMemTable() {
  VersionEdit edit;
  Version* base = versions_-&gt;current();
  WriteLevel0Table(imm_, &amp;edit, base);
  versions_-&gt;LogAndApply(&amp;edit, &amp;mutex_);
  DeleteObsoleteFiles();
}
</code></pre>

<p><code>CompactMemTable</code> 会执行 <code>WriteLevel0Table</code> 将当前的 imm 转换成一个 Level0 的 SSTable 文件，同时由于 Level0 层级的文件变多，可能会继续触发一个新的 Major Compaction，在这里我们就需要在这里选择需要压缩的合适的层级：</p>

<pre><code class="language-cpp">Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) {
  FileMetaData meta;
  meta.number = versions_-&gt;NewFileNumber();
  Iterator* iter = mem-&gt;NewIterator();
  BuildTable(dbname_, env_, options_, table_cache_, iter, &amp;meta);

  const Slice min_user_key = meta.smallest.user_key();
  const Slice max_user_key = meta.largest.user_key();
  int level = base-&gt;PickLevelForMemTableOutput(min_user_key, max_user_key);
  edit-&gt;AddFile(level, meta.number, meta.file_size, meta.smallest, meta.largest);
  return Status::OK();
}
</code></pre>

<p>所有对当前 SSTable 数据的修改由一个统一的 <code>VersionEdit</code> 对象记录和管理，我们会在后面介绍这个对象的作用和实现，如果成功写入了就会返回这个文件的元数据 <code>FileMetaData</code>，最后调用 <code>VersionSet</code> 的方法 <code>LogAndApply</code> 将文件中的全部变化如实记录下来，最后做一些数据的清理工作。</p>

<p>当然如果是 Major Compaction 就稍微有一些复杂了，不过整理后的 <code>BackgroundCompaction</code> 方法的逻辑非常清晰：</p>

<pre><code class="language-cpp">void DBImpl::BackgroundCompaction() {
  if (imm_ != NULL) {
    CompactMemTable();
    return;
  }

  Compaction* c = versions_-&gt;PickCompaction();
  CompactionState* compact = new CompactionState(c);
  DoCompactionWork(compact);
  CleanupCompaction(compact);
  DeleteObsoleteFiles();
}
</code></pre>

<p>我们从当前的 <code>VersionSet</code> 中找到需要压缩的文件信息，将它们打包存入一个 <code>Compaction</code> 对象，该对象需要选择两个层级的 SSTable，低层级的表很好选择，只需要选择大小超过限制的或者查询次数太多的 SSTable；当我们选择了低层级的一个 SSTable 后，就在更高的层级选择与该 SSTable 有重叠键的 SSTable 就可以了，通过 <code>FileMetaData</code> 中数据的帮助我们可以很快找到待压缩的全部数据。</p>

<blockquote>
  <p>查询次数太多的意思就是，当客户端调用多次 <code>Get</code> 方法时，如果这次 <code>Get</code> 方法在某个层级的 SSTable 中找到了对应的键，那么就算做上一层级中包含该键的 SSTable 的一次查找，也就是这次查找由于不同层级键的覆盖范围造成了更多的耗时，每个 SSTable 在创建之后的 <code>allowed_seeks</code> 都为 100 次，当 <code>allowed_seeks &lt; 0</code> 时就会触发该文件的与更高层级和合并，以减少以后查询的查找次数。</p>
</blockquote>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-Pick-Compactions.jpg-1000width" alt="LevelDB-Pick-Compactions" /></p>

<p>LevelDB 中的 <code>DoCompactionWork</code> 方法会对所有传入的 SSTable 中的键值使用归并排序进行合并，最后会在高高层级（图中为 Level2）中生成一个新的 SSTable。</p>

<p><img src="https://img.nju520.me/2017-08-12-LevelDB-After-Compactions.jpg-1000width" alt="LevelDB-After-Compactions" /></p>

<p>这样下一次查询 17~40 之间的值时就可以减少一次对 SSTable 中数据的二分查找以及读取文件的时间，提升读写的性能。</p>

<h4 id="存储-db-状态的-versionset">存储 db 状态的 VersionSet</h4>

<p>LevelDB 中的所有状态其实都是被一个 <code>VersionSet</code> 结构所存储的，一个 <code>VersionSet</code> 包含一组 <code>Version</code> 结构体，所有的 <code>Version</code> 包括历史版本都是通过双向链表连接起来的，但是只有一个版本是当前版本。</p>

<p><img src="https://img.nju520.me/2017-08-12-VersionSet-Version-And-VersionEdit.jpg-1000width" alt="VersionSet-Version-And-VersionEdit" /></p>

<p>当 LevelDB 中的 SSTable 发生变动时，它会生成一个 <code>VersionEdit</code> 结构，最终执行 <code>LogAndApply</code> 方法：</p>

<pre><code class="language-cpp">Status VersionSet::LogAndApply(VersionEdit* edit, port::Mutex* mu) {
  Version* v = new Version(this);
  Builder builder(this, current_);
  builder.Apply(edit);
  builder.SaveTo(v);

  std::string new_manifest_file;
  new_manifest_file = DescriptorFileName(dbname_, manifest_file_number_);
  env_-&gt;NewWritableFile(new_manifest_file, &amp;descriptor_file_);

  std::string record;
  edit-&gt;EncodeTo(&amp;record);
  descriptor_log_-&gt;AddRecord(record);
  descriptor_file_-&gt;Sync();

  SetCurrentFile(env_, dbname_, manifest_file_number_);
  AppendVersion(v);

  return Status::OK();
}
</code></pre>

<p>该方法的主要工作是使用当前版本和 <code>VersionEdit</code> 创建一个新的版本对象，然后将 <code>Version</code> 的变更追加到 MANIFEST 日志中，并且改变数据库中全局当前版本信息。</p>

<blockquote>
  <p>MANIFEST 文件中记录了 LevelDB 中所有层级中的表、每一个 SSTable 的 Key 范围和其他重要的元数据，它以日志的格式存储，所有对文件的增删操作都会追加到这个日志中。</p>
</blockquote>

<h4 id="sstable-的格式">SSTable 的格式</h4>

<p>SSTable 中其实存储的不只是数据，其中还保存了一些元数据、索引等信息，用于加速读写操作的速度，虽然在 Bigtable 的论文中并没有给出 SSTable 的数据格式，不过在 LevelDB 的实现中，我们可以发现 SSTable 是以这种格式存储数据的：</p>

<p><img src="https://img.nju520.me/2017-08-12-SSTable-Format.jpg-1000width" alt="SSTable-Format" /></p>

<p>当 LevelDB 读取 SSTable 存在的 <code>ldb</code> 文件时，会先读取文件中的 <code>Footer</code> 信息。</p>

<p><img src="https://img.nju520.me/2017-08-12-SSTable-Footer.jpg-1000width" alt="SSTable-Foote" /></p>

<p>整个 <code>Footer</code> 在文件中占用 48 个字节，我们能在其中拿到 MetaIndex 块和 Index 块的位置，再通过其中的索引继而找到对应值存在的位置。</p>

<p><code>TableBuilder::Rep</code> 结构体中就包含了一个文件需要创建的全部信息，包括数据块、索引块等等：</p>

<pre><code class="language-cpp">struct TableBuilder::Rep {
  WritableFile* file;
  uint64_t offset;
  BlockBuilder data_block;
  BlockBuilder index_block;
  std::string last_key;
  int64_t num_entries;
  bool closed;
  FilterBlockBuilder* filter_block;
  ...
}
</code></pre>

<p>到这里，我们就完成了对整个数据读取过程的解析了；对于读操作，我们可以理解为 LevelDB 在它内部的『多级缓存』中依次查找是否存在对应的键，如果存在就会直接返回，唯一与缓存不同可能就是，在数据『命中』后，它并不会把数据移动到更近的地方，而是会把数据移到更远的地方来减少下一次的访问时间，虽然这么听起来却是不可思议，不过仔细想一下确实是这样。</p>

<h2 id="小结-1">小结</h2>

<p>在这篇文章中，我们通过对 LevelDB 源代码中读写操作的分析，了解了整个框架的绝大部分实现细节，包括 LevelDB 中存储数据的格式、多级 SSTable、如何进行合并以及管理版本等信息，不过由于篇幅所限，对于其中的一些问题并没有展开详细地进行介绍和分析，例如错误恢复以及缓存等问题；但是对 LevelDB 源代码的阅读，加深了我们对 Bigtable 论文中描述的分布式 KV 存储数据库的理解。</p>

<p>LevelDB 的源代码非常易于阅读，也是学习 C++ 语言非常优秀的资源，如果对文章的内容有疑问，可以在博客下面留言。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a></li>
  <li><a href="https://github.com/google/leveldb">LevelDB</a></li>
  <li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf">The Chubby lock service for loosely-coupled distributed systems</a></li>
  <li><a href="https://github.com/google/leveldb/blob/master/doc/impl.md">LevelDB · Impl</a></li>
  <li><a href="http://bean-li.github.io/leveldb-sstable/">leveldb 中的 SSTable</a></li>
</ul>

  ]]></description>
</item>

<item>
  <title>Redis 是如何处理命令的（客户端）</title>
  <link>//redis-cli</link>
  <author>nju520</author>
  <pubDate>2016-12-23T23:23:15+08:00</pubDate>
  <guid>//redis-cli</guid>
  <description><![CDATA[
  <p>在使用 Redis 的过程中经常会好奇，在 Redis-Cli 中键入 <code>SET KEY MSG</code> 并回车之后，Redis 客户端和服务是如何对命令进行解析处理的，而在内部的实现过程是什么样的。</p>

<p>这两篇文章会分别介绍 Redis 客户端和服务端分别对命令是如何处理的，本篇文章介绍的是 Redis 客户端如何处理输入的命令、向服务发送命令以及取得服务端回复并输出到终端等过程。</p>

<p><img src="https://img.nju520.me/2016-12-23-redis-client-server.jpg-1000width" alt="redis-client-serve" /></p>

<p>文章中会将 Redis 服务看做一个输入为 Redis 命令，输出为命令执行结果的黑箱，对从命令到结果的过程不做任何解释，只会着眼于客户端的逻辑，也就是上图中的 1 和 4 两个过程。</p>

<h2 id="从-main-函数开始">从 main 函数开始</h2>

<p>与其它的 C 语言框架/服务类似，Redis 的客户端 <code>redis-cli</code> 也是从 <code>main</code> 函数开始执行的，位于 <code>redis-cli.c</code> 文件的最后：</p>

<pre><code class="language-c">int main(int argc, char **argv) {
    ...
    if (argc == 0 &amp;&amp; !config.eval) {
        repl();
    }
    ...
}
</code></pre>

<p>在一般情况下，Redis 客户端都会进入 <code>repl</code> 模式，对输入进行解析；</p>

<blockquote>
  <p>Redis 中有好多模式，包括：Latency、Slave、Pipe、Stat、Scan、LRU test 等等模式，不过这些模式都不是这篇文章关注的重点，我们只会关注最常见的 repl 模式。</p>
</blockquote>

<pre><code class="language-c">static void repl(void) {
    char *line;
    int argc;
    sds *argv;

    ...

    while((line = linenoise(context ? config.prompt : "not connected&gt; ")) != NULL) {
        if (line[0] != '\0') {
            argv = cliSplitArgs(line,&amp;argc);

            if (argv == NULL) {
                printf("Invalid argument(s)\n");
                continue;
            }
            if (strcasecmp(argv[0],"???") == 0) {
                ...
            } else {
                issueCommandRepeat(argc, argv, 1);
            }
        }
    }
    exit(0);
}
</code></pre>

<p>在上述代码中，我们省略了大量的实现细节，只保留整个 <code>repl</code> 中循环的主体部分，方便进行理解和分析，在 <code>while</code> 循环中的条件你可以看到 <code>linenoise</code> 方法的调用，通过其中的 <code>prompt</code> 和 <code>not connected&gt; </code> 可以判断出，这里向终端中输出了提示符，同时会调用 <code>fgets</code> 从标准输入中读取字符串：</p>

<pre><code class="language-c">127.0.0.1:6379&gt;
</code></pre>

<p>全局搜一下 <code>config.prompt</code> 不难发现这一行代码，也就是控制命令行提示的 <code>prompt</code>：</p>

<pre><code class="language-c">anetFormatAddr(config.prompt, sizeof(config.prompt),config.hostip, config.hostport);
</code></pre>

<p>接下来执行的 <code>cliSplitArgs</code> 函数会将 <code>line</code> 中的字符串分割成几个不同的参数，然后根据字符串 <code>argv[0]</code> 的不同执行的命令，在这里省略了很多原有的代码：</p>

<pre><code class="language-c">if (strcasecmp(argv[0],"quit") == 0 ||
    strcasecmp(argv[0],"exit") == 0)
{
    exit(0);
} else if (argv[0][0] == ':') {
    cliSetPreferences(argv,argc,1);
    continue;
} else if (strcasecmp(argv[0],"restart") == 0) {
    ...
} else if (argc == 3 &amp;&amp; !strcasecmp(argv[0],"connect")) {
    ...
} else if (argc == 1 &amp;&amp; !strcasecmp(argv[0],"clear")) {
} else {
    issueCommandRepeat(argc, argv, 1);
}
</code></pre>

<p>在遇到 <code>quit</code>、<code>exit</code> 等跟<strong>客户端状态有关的命令</strong>时，就会直接执行相应的代码；否则就会将命令和参数 <code>issueCommandRepeat</code> 函数。</p>

<h3 id="追踪一次命令的执行">追踪一次命令的执行</h3>

<blockquote>
  <p>Redis Commit： <code>790310d89460655305bd615bc442eeaf7f0f1b38</code></p>

  <p>lldb： lldb-360.1.65</p>

  <p>macOS 10.11.6</p>
</blockquote>

<p>在继续分析 <code>issueCommandRepeat</code> 之前，我们先对 Redis 中的这部分代码进行调试追踪，在使用 <code>make</code> 编译了 Redis 源代码，启动 <code>redis-server</code> 之后；启动 lldb 对 Redis 客户端进行调试：</p>

<pre><code class="language-shell">$ lldb src/redis-cli
(lldb) target create "src/redis-cli"
Current executable set to 'src/redis-cli' (x86_64).
(lldb) b redis-cli.c:1290
Breakpoint 1: where = redis-cli`repl + 228 at redis-cli.c:1290, address = 0x0000000100008cd4
(lldb) process launch
Process 8063 launched: '~/redis/src/redis-cli' (x86_64)
127.0.0.1:6379&gt;
</code></pre>

<p>在 <code>redis-cli.c:1290</code> 也就是下面这行代码的地方打断点之后：</p>

<pre><code class="language-c">-&gt; 1290	        if (line[0] != '\0') {
</code></pre>

<p>执行 <code>process launch</code> 启动 <code>redis-cli</code>，然后输入 <code>SET KEY MSG</code> 回车以及 Ctrl-C：</p>

<blockquote>
  <p>在 lldb 中调试时，回车的输入经常会有问题，在这里输入 Ctrl-C 进入信号处理器，在通过 continue 命令进入断点：</p>
</blockquote>

<pre><code class="language-c">127.0.0.1:6379&gt; SET KEY MSG
^C
8063 stopped
* thread #1: tid = 0xa95147, 0x00007fff90923362 libsystem_kernel.dylib`read + 10, stop reason = signal SIGSTOP
    frame #0: 0x00007fff90923362 libsystem_kernel.dylib`read + 10
libsystem_kernel.dylib`read:
-&gt;  0x7fff90923362 &lt;+10&gt;: jae    0x7fff9092336c            ; &lt;+20&gt;
    0x7fff90923364 &lt;+12&gt;: movq   %rax, %rdi
    0x7fff90923367 &lt;+15&gt;: jmp    0x7fff9091c7f2            ; cerror
    0x7fff9092336c &lt;+20&gt;: retq
(lldb) c
Process 8063 resuming

Process 8063 stopped
* thread #1: tid = 0xa95147, 0x0000000100008cd4 redis-cli`repl + 228 at redis-cli.c:1290, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1
    frame #0: 0x0000000100008cd4 redis-cli`repl + 228 at redis-cli.c:1290
   1287
   1288	    cliRefreshPrompt();
   1289	    while((line = linenoise(context ? config.prompt : "not connected&gt; ")) != NULL) {
-&gt; 1290	        if (line[0] != '\0') {
   1291	            argv = cliSplitArgs(line,&amp;argc);
   1292	            if (history) linenoiseHistoryAdd(line);
   1293	            if (historyfile) linenoiseHistorySave(historyfile);
(lldb)
</code></pre>

<p>输入两次 <code>n</code> 之后，打印 <code>argv</code> 和 <code>argc</code> 的值：</p>

<pre><code class="language-c">(lldb) p argc
(int) $1 = 3
(lldb) p *argv
(sds) $2 = 0x0000000100106cc3 "SET"
(lldb) p *(argv+1)
(sds) $3 = 0x0000000100106ce3 "KEY"
(lldb) p *(argv+2)
(sds) $4 = 0x0000000100106cf3 "MSG"
(lldb) p line
(char *) $5 = 0x0000000100303430 "SET KEY MSG\n"
</code></pre>

<p><code>cliSplitArgs</code> 方法成功将 <code>line</code> 中的字符串分隔成字符串参数，在多次执行 <code>n</code> 之后，进入 <code>issueCommandRepeat</code> 方法：</p>

<pre><code class="language-c">-&gt; 1334	                    issueCommandRepeat(argc-skipargs, argv+skipargs, repeat);
</code></pre>

<h2 id="对输入命令的处理">对输入命令的处理</h2>

<p>上一阶段执行 <code>issueCommandRepeat</code> 的函数调用栈中，会发现 Redis 并不会直接把所有的命令发送到服务端：</p>

<pre><code class="language-c">issueCommandRepeat
    cliSendCommand
        redisAppendCommandArgv
            redisFormatCommandArgv
            __redisAppendCommand
</code></pre>

<p>而是会在 <code>redisFormatCommandArgv</code> 中对所有的命令进行格式化处理，将字符串转换为符合 RESP 协议的数据。</p>

<h3 id="resp-协议">RESP 协议</h3>

<p>Redis 客户端与 Redis 服务进行通讯时，会使用名为 <strong>RESP</strong>（REdis Serialization Protocol） 的协议，它的使用非常简单，并且可以序列化多种数据类型包括整数、字符串以及数组等。</p>

<p>对于 RESP 协议的详细介绍可以看官方文档中的 <a href="https://redis.io/topics/protocol">Redis Protocol specification</a>，在这里对这个协议进行简单的介绍。</p>

<p>在将不同的数据类型序列化时，会使用第一个 byte 来表示当前数据的数据类型，以便在客户端或服务器在处理时能恢复原来的数据格式。</p>

<p><img src="https://img.nju520.me/2016-12-23-redis-resp-data-byte.jpg-1000width" alt="redis-resp-data-byte" /></p>

<p>举一个简单的例子，字符串 <code>OK</code> 以及错误<code>Error Message</code> 等不同种类的信息的 RESP 表示如下：</p>

<p><img src="https://img.nju520.me/2016-12-23-redis-resp-type-and-examples.jpg-1000width" alt="redis-resp-type-and-examples" /></p>

<p>在这篇文章中我们需要简单了解的就是 RESP “数据格式”的<strong>第一个字节用来表示数据类型</strong>，然后<strong>逻辑上属于不同部分的内容通过 CRLF（\r\n）分隔</strong>。</p>

<h3 id="数据格式的转换">数据格式的转换</h3>

<p>在 <code>redisFormatCommandArgv</code> 方法中几乎没有需要删减的代码，所有的命令都会以字符串数组的形式发送到客户端：</p>

<pre><code class="language-c">int redisFormatCommandArgv(char **target, int argc, const char **argv, const size_t *argvlen) {
    char *cmd = NULL;
    int pos;
    size_t len;
    int totlen, j;

    totlen = 1+intlen(argc)+2;
    for (j = 0; j &lt; argc; j++) {
        len = argvlen ? argvlen[j] : strlen(argv[j]);
        totlen += bulklen(len);
    }

    cmd = malloc(totlen+1);
    if (cmd == NULL)
        return -1;

    pos = sprintf(cmd,"*%d\r\n",argc);
    for (j = 0; j &lt; argc; j++) {
        len = argvlen ? argvlen[j] : strlen(argv[j]);
        pos += sprintf(cmd+pos,"$%zu\r\n",len);
        memcpy(cmd+pos,argv[j],len);
        pos += len;
        cmd[pos++] = '\r';
        cmd[pos++] = '\n';
    }
    assert(pos == totlen);
    cmd[pos] = '\0';

    *target = cmd;
    return totlen;
}
</code></pre>

<p><code>SET KEY MSG</code> 这一命令，经过这个方法的处理会变成：</p>

<pre><code class="language-c">*3\r\n$3\r\nSET\r\n$3\r\nKEY\r\n$3\r\nMSG\r\n
</code></pre>

<p>你可以这么理解上面的结果：</p>

<pre><code class="language-c">*3\r\n
    $3\r\nSET\r\n
    $3\r\nKEY\r\n
    $3\r\nMSG\r\n
</code></pre>

<p>这是一个由三个字符串组成的数组，数组中的元素是 <code>SET</code>、<code>KEY</code> 以及 <code>MSG</code> 三个字符串。</p>

<p>如果在这里打一个断点并输出 <code>target</code> 中的内容：</p>

<p><img src="https://img.nju520.me/2016-12-23-redis-lldb-cmd.png-1000width" alt="redis-lldb-cmd" /></p>

<p>到这里就完成了对输入命令的格式化，在格式化之后还会将当前命令写入全局的 <code>redisContext</code> 的 <code>write</code> 缓冲区 <code>obuf</code> 中，也就是在上面的缓冲区看到的第二个方法：</p>

<pre><code class="language-c">int __redisAppendCommand(redisContext *c, const char *cmd, size_t len) {
    sds newbuf;

    newbuf = sdscatlen(c-&gt;obuf,cmd,len);
    if (newbuf == NULL) {
        __redisSetError(c,REDIS_ERR_OOM,"Out of memory");
        return REDIS_ERR;
    }

    c-&gt;obuf = newbuf;
    return REDIS_OK;
}
</code></pre>

<h3 id="rediscontext">redisContext</h3>

<p>再继续介绍下一部分之前需要简单介绍一下 <code>redisContext</code> 结构体：</p>

<pre><code class="language-c">typedef struct redisContext {
    int err;
    char errstr[128];
    int fd;
    int flags;
    char *obuf;
    redisReader *reader;
} redisContext;
</code></pre>

<p>每一个 <code>redisContext</code> 的结构体都表示一个 Redis 客户端对服务的连接，而这个上下文会在每一个 redis-cli 中作为静态变量仅保存一个：</p>

<pre><code class="language-c">static redisContext *context;
</code></pre>

<p><code>obuf</code> 中包含了客户端未写到服务端的数据；而 <code>reader</code> 是用来处理 RESP 协议的结构体；<code>fd</code> 就是 Redis 服务对应的文件描述符；其他的内容就不多做解释了。</p>

<p>到这里，对命令的格式化处理就结束了，接下来就到了向服务端发送命令的过程了。</p>

<h2 id="向服务器发送命令">向服务器发送命令</h2>

<p>与对输入命令的处理差不多，向服务器发送命令的方法也在 <code>issueCommandRepeat</code> 的调用栈中，而且藏得更深，如果不仔细阅读源代码其实很难发现：</p>

<pre><code class="language-c">issueCommandRepeat
    cliSendCommand
        cliReadReply
            redisGetReply
               redisBufferWrite
</code></pre>

<p>Redis 在 <code>redisGetReply</code> 中完成对命令的发送：</p>

<pre><code class="language-c">int redisGetReply(redisContext *c, void **reply) {
    int wdone = 0;
    void *aux = NULL;

    if (aux == NULL &amp;&amp; c-&gt;flags &amp; REDIS_BLOCK) {
        do {
            if (redisBufferWrite(c,&amp;wdone) == REDIS_ERR)
                return REDIS_ERR;
        } while (!wdone);

        ...
        } while (aux == NULL);
    }

    if (reply != NULL) *reply = aux;
    return REDIS_OK;
}
</code></pre>

<p>上面的代码向 <code>redisBufferWrite</code> 函数中传递了全局的静态变量 <code>redisContext</code>，其中的 <code>obuf</code> 中存储了没有向 Redis 服务发送的命令：</p>

<pre><code class="language-c">int redisBufferWrite(redisContext *c, int *done) {
    int nwritten;

    if (sdslen(c-&gt;obuf) &gt; 0) {
        nwritten = write(c-&gt;fd,c-&gt;obuf,sdslen(c-&gt;obuf));
        if (nwritten == -1) {
            if ((errno == EAGAIN &amp;&amp; !(c-&gt;flags &amp; REDIS_BLOCK)) || (errno == EINTR)) {
            } else {
                __redisSetError(c,REDIS_ERR_IO,NULL);
                return REDIS_ERR;
            }
        } else if (nwritten &gt; 0) {
            if (nwritten == (signed)sdslen(c-&gt;obuf)) {
                sdsfree(c-&gt;obuf);
                c-&gt;obuf = sdsempty();
            } else {
                sdsrange(c-&gt;obuf,nwritten,-1);
            }
        }
    }
    if (done != NULL) *done = (sdslen(c-&gt;obuf) == 0);
    return REDIS_OK;
}
</code></pre>

<p>代码的逻辑其实十分清晰，调用 <code>write</code> 向 Redis 服务代表的文件描述符发送写缓冲区 <code>obuf</code> 中的数据，然后根据返回值做出相应的处理，如果命令发送成功就会清空 <code>obuf</code> 并将 <code>done</code> 指针标记为真，然后返回，这样就完成了向服务器发送命令这一过程。</p>

<p><img src="https://img.nju520.me/2016-12-23-redis-lldb-nwritten.png-1000width" alt="redis-lldb-nwritten" /></p>

<h2 id="获取服务器回复">获取服务器回复</h2>

<p>其实获取服务器回复和上文中的发送命令过程基本上差不多，调用栈也几乎完全一样：</p>

<pre><code class="language-c">issueCommandRepeat
    cliSendCommand
        cliReadReply
            redisGetReply
                redisBufferRead
                redisGetReplyFromReader
            cliFormatReplyRaw
            fwrite
</code></pre>

<p>同样地，在 <code>redisGetReply</code> 中获取服务器的响应：</p>

<pre><code class="language-c">int redisGetReply(redisContext *c, void **reply) {
    int wdone = 0;
    void *aux = NULL;

    if (aux == NULL &amp;&amp; c-&gt;flags &amp; REDIS_BLOCK) {
        do {
            if (redisBufferWrite(c,&amp;wdone) == REDIS_ERR)
                return REDIS_ERR;
        } while (!wdone);

        do {
            if (redisBufferRead(c) == REDIS_ERR)
                return REDIS_ERR;
            if (redisGetReplyFromReader(c,&amp;aux) == REDIS_ERR)
                return REDIS_ERR;
        } while (aux == NULL);
    }

    if (reply != NULL) *reply = aux;
    return REDIS_OK;
}
</code></pre>

<p>在 <code>redisBufferWrite</code> 成功发送命令并返回之后，就会开始等待服务端的回复，总共分为两个部分，一是使用 <code>redisBufferRead</code> 从服务端读取原始格式的回复（符合 RESP 协议）：</p>

<pre><code class="language-c">int redisBufferRead(redisContext *c) {
    char buf[1024*16];
    int nread;

    nread = read(c-&gt;fd,buf,sizeof(buf));
    if (nread == -1) {
        if ((errno == EAGAIN &amp;&amp; !(c-&gt;flags &amp; REDIS_BLOCK)) || (errno == EINTR)) {
        } else {
            __redisSetError(c,REDIS_ERR_IO,NULL);
            return REDIS_ERR;
        }
    } else if (nread == 0) {
        __redisSetError(c,REDIS_ERR_EOF,"Server closed the connection");
        return REDIS_ERR;
    } else {
        if (redisReaderFeed(c-&gt;reader,buf,nread) != REDIS_OK) {
            __redisSetError(c,c-&gt;reader-&gt;err,c-&gt;reader-&gt;errstr);
            return REDIS_ERR;
        }
    }
    return REDIS_OK;
}
</code></pre>

<p>在 <code>read</code> 从文件描述符中成功读取数据并返回之后，我们可以打印 <code>buf</code> 中的内容：</p>

<p><img src="https://img.nju520.me/2016-12-23-redis-lldb-read.png-1000width" alt="redis-lldb-read" /></p>

<p>刚刚向 <code>buf</code> 中写入的数据还需要经过 <code>redisReaderFeed</code> 方法的处理，截取正确的长度；然后存入 <code>redisReader</code> 中：</p>

<pre><code class="language-c">int redisReaderFeed(redisReader *r, const char *buf, size_t len) {
    sds newbuf;

    if (buf != NULL &amp;&amp; len &gt;= 1) {
        if (r-&gt;len == 0 &amp;&amp; r-&gt;maxbuf != 0 &amp;&amp; sdsavail(r-&gt;buf) &gt; r-&gt;maxbuf) {
            sdsfree(r-&gt;buf);
            r-&gt;buf = sdsempty();
            r-&gt;pos = 0;
            assert(r-&gt;buf != NULL);
        }

        newbuf = sdscatlen(r-&gt;buf,buf,len);
        if (newbuf == NULL) {
            __redisReaderSetErrorOOM(r);
            return REDIS_ERR;
        }

        r-&gt;buf = newbuf;
        r-&gt;len = sdslen(r-&gt;buf);
    }

    return REDIS_OK;
}
</code></pre>

<p>最后的 <code>redisGetReplyFromReader</code> 方法会从 <code>redisContext</code> 中取出 <code>reader</code>，然后反序列化 RESP 对象，最后打印出来。</p>

<p><img src="https://img.nju520.me/2016-12-23-process-end.png-1000width" alt="process-end" /></p>

<p>当我们从终端的输出中看到了 OK 以及这个命令的执行的时间时，<code>SET KEY MSG</code> 这一命令就已经处理完成了。</p>

<h2 id="总结">总结</h2>

<p>处理命令的过程在客户端还是比较简单的：</p>

<ol>
  <li>在一个 <code>while</code> 循环中，输出提示符；</li>
  <li>接收到输入命令时，对输入命令进行格式化处理；</li>
  <li>通过 <code>write</code> 发送到 Redis 服务，并调用 <code>read</code> 阻塞当前进程直到服务端返回为止；</li>
  <li>对服务端返回的数据反序列化；</li>
  <li>将结果打印到终端。</li>
</ol>

<p>用一个简单的图表示，大概是这样的：</p>

<p><img src="https://img.nju520.me/2016-12-23-redis-client-process-commands.jpg-1000width" alt="redis-client-process-commands" /></p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://redis.io/topics/protocol">Redis Protocol specification</a></li>
  <li><a href="http://nju520.me/redis-io-multiplexing/">Redis 和 I/O 多路复用</a></li>
  <li><a href="http://nju520.me/redis-eventloop">Redis 中的事件循环</a></li>
</ul>

<blockquote>

  <p>Source: http://nju520.me/redis-cli</p>
</blockquote>

  ]]></description>
</item>

<item>
  <title>Redis 中的事件循环</title>
  <link>//redis-eventloop</link>
  <author>nju520</author>
  <pubDate>2016-12-09T23:42:05+08:00</pubDate>
  <guid>//redis-eventloop</guid>
  <description><![CDATA[
  <p>在目前的很多服务中，由于需要持续接受客户端或者用户的输入，所以需要一个事件循环来等待并处理外部事件，这篇文章主要会介绍 Redis 中的事件循环是如何处理事件的。</p>

<p>在文章中，我们会先从 Redis 的实现中分析事件是如何被处理的，然后用更具象化的方式了解服务中的不同模块是如何交流的。</p>

<h2 id="aeeventloop">aeEventLoop</h2>

<p>在分析具体代码之前，先了解一下在事件处理中处于核心部分的 <code>aeEventLoop</code> 到底是什么：</p>

<p><img src="https://img.nju520.me/2016-12-09-reids-eventloop.png-1000width" alt="reids-eventloop" /></p>

<p><code>aeEventLoop</code> 在 Redis 就是负责保存待处理文件事件和时间事件的结构体，其中保存大量事件执行的上下文信息，同时持有三个事件数组：</p>

<ul>
  <li><code>aeFileEvent</code></li>
  <li><code>aeTimeEvent</code></li>
  <li><code>aeFiredEvent</code></li>
</ul>

<p><code>aeFileEvent</code> 和 <code>aeTimeEvent</code> 中会存储监听的文件事件和时间事件，而最后的 <code>aeFiredEvent</code> 用于存储待处理的文件事件，我们会在后面的章节中介绍它们是如何工作的。</p>

<h3 id="redis-服务中的-eventloop">Redis 服务中的 EventLoop</h3>

<p>在 <code>redis-server</code> 启动时，首先会初始化一些 redis 服务的配置，最后会调用 <code>aeMain</code> 函数陷入 <code>aeEventLoop</code> 循环中，等待外部事件的发生：</p>

<pre><code class="language-c">int main(int argc, char **argv) {
    ...

    aeMain(server.el);
}
</code></pre>

<p><code>aeMain</code> 函数其实就是一个封装的 <code>while</code> 循环，循环中的代码会一直运行直到 <code>eventLoop</code> 的 <code>stop</code> 被设置为 <code>true</code>：</p>

<pre><code class="language-c">void aeMain(aeEventLoop *eventLoop) {
    eventLoop-&gt;stop = 0;
    while (!eventLoop-&gt;stop) {
        if (eventLoop-&gt;beforesleep != NULL)
            eventLoop-&gt;beforesleep(eventLoop);
        aeProcessEvents(eventLoop, AE_ALL_EVENTS);
    }
}
</code></pre>

<p>它会不停尝试调用 <code>aeProcessEvents</code> 对可能存在的多种事件进行处理，而 <code>aeProcessEvents</code> 就是实际用于处理事件的函数：</p>

<pre><code class="language-c">int aeProcessEvents(aeEventLoop *eventLoop, int flags) {
    int processed = 0, numevents;

    if (!(flags &amp; AE_TIME_EVENTS) &amp;&amp; !(flags &amp; AE_FILE_EVENTS)) return 0;

    if (eventLoop-&gt;maxfd != -1 ||
        ((flags &amp; AE_TIME_EVENTS) &amp;&amp; !(flags &amp; AE_DONT_WAIT))) {
        struct timeval *tvp;

        #1：计算 I/O 多路复用的等待时间 tvp

        numevents = aeApiPoll(eventLoop, tvp);
        for (int j = 0; j &lt; numevents; j++) {
            aeFileEvent *fe = &amp;eventLoop-&gt;events[eventLoop-&gt;fired[j].fd];
            int mask = eventLoop-&gt;fired[j].mask;
            int fd = eventLoop-&gt;fired[j].fd;
            int rfired = 0;

            if (fe-&gt;mask &amp; mask &amp; AE_READABLE) {
                rfired = 1;
                fe-&gt;rfileProc(eventLoop,fd,fe-&gt;clientData,mask);
            }
            if (fe-&gt;mask &amp; mask &amp; AE_WRITABLE) {
                if (!rfired || fe-&gt;wfileProc != fe-&gt;rfileProc)
                    fe-&gt;wfileProc(eventLoop,fd,fe-&gt;clientData,mask);
            }
            processed++;
        }
    }
    if (flags &amp; AE_TIME_EVENTS) processed += processTimeEvents(eventLoop);
    return processed;
}
</code></pre>

<p>上面的代码省略了 I/O 多路复用函数的等待时间，不过不会影响我们对代码的理解，整个方法大体由两部分代码组成，一部分处理文件事件，另一部分处理时间事件。</p>

<blockquote>
  <p>Redis 中会处理两种事件：时间事件和文件事件。</p>
</blockquote>

<h3 id="文件事件">文件事件</h3>

<p>在一般情况下，<code>aeProcessEvents</code> 都会先<strong>计算最近的时间事件发生所需要等待的时间</strong>，然后调用 <code>aeApiPoll</code> 方法在这段时间中等待事件的发生，在这段时间中如果发生了文件事件，就会优先处理文件事件，否则就会一直等待，直到最近的时间事件需要触发：</p>

<pre><code class="language-c">numevents = aeApiPoll(eventLoop, tvp);
for (j = 0; j &lt; numevents; j++) {
    aeFileEvent *fe = &amp;eventLoop-&gt;events[eventLoop-&gt;fired[j].fd];
    int mask = eventLoop-&gt;fired[j].mask;
    int fd = eventLoop-&gt;fired[j].fd;
    int rfired = 0;

    if (fe-&gt;mask &amp; mask &amp; AE_READABLE) {
        rfired = 1;
        fe-&gt;rfileProc(eventLoop,fd,fe-&gt;clientData,mask);
    }
    if (fe-&gt;mask &amp; mask &amp; AE_WRITABLE) {
        if (!rfired || fe-&gt;wfileProc != fe-&gt;rfileProc)
            fe-&gt;wfileProc(eventLoop,fd,fe-&gt;clientData,mask);
    }
    processed++;
}
</code></pre>

<p>文件事件如果绑定了对应的读/写事件，就会执行对应的对应的代码，并传入事件循环、文件描述符、数据以及掩码：</p>

<pre><code class="language-c">fe-&gt;rfileProc(eventLoop,fd,fe-&gt;clientData,mask);
fe-&gt;wfileProc(eventLoop,fd,fe-&gt;clientData,mask);
</code></pre>

<p>其中 <code>rfileProc</code> 和 <code>wfileProc</code> 就是在文件事件被创建时传入的函数指针：</p>

<pre><code class="language-c">int aeCreateFileEvent(aeEventLoop *eventLoop, int fd, int mask, aeFileProc *proc, void *clientData) {
    aeFileEvent *fe = &amp;eventLoop-&gt;events[fd];

    if (aeApiAddEvent(eventLoop, fd, mask) == -1)
        return AE_ERR;
    fe-&gt;mask |= mask;
    if (mask &amp; AE_READABLE) fe-&gt;rfileProc = proc;
    if (mask &amp; AE_WRITABLE) fe-&gt;wfileProc = proc;
    fe-&gt;clientData = clientData;
    if (fd &gt; eventLoop-&gt;maxfd)
        eventLoop-&gt;maxfd = fd;
    return AE_OK;
}
</code></pre>

<p>需要注意的是，传入的 <code>proc</code> 函数会在对应的 <code>mask</code> 位事件发生时执行。</p>

<h3 id="时间事件">时间事件</h3>

<p>在 Redis 中会发生两种时间事件：</p>

<ul>
  <li>一种是定时事件，每隔一段时间会执行一次；</li>
  <li>另一种是非定时事件，只会在某个时间点执行一次；</li>
</ul>

<p>时间事件的处理在 <code>processTimeEvents</code> 中进行，我们会分三部分分析这个方法的实现：</p>

<pre><code class="language-c">static int processTimeEvents(aeEventLoop *eventLoop) {
    int processed = 0;
    aeTimeEvent *te, *prev;
    long long maxId;
    time_t now = time(NULL);

    if (now &lt; eventLoop-&gt;lastTime) {
        te = eventLoop-&gt;timeEventHead;
        while(te) {
            te-&gt;when_sec = 0;
            te = te-&gt;next;
        }
    }
    eventLoop-&gt;lastTime = now;
</code></pre>

<p>由于对系统时间的调整会影响当前时间的获取，进而影响时间事件的执行；如果系统时间先被设置到了未来的时间，又设置成正确的值，这就会导致<strong>时间事件会随机延迟一段时间执行</strong>，也就是说，时间事件不会按照预期的安排尽早执行，而 <code>eventLoop</code> 中的 <code>lastTime</code> 就是用于检测上述情况的变量：</p>

<pre><code class="language-c">typedef struct aeEventLoop {
    ...
    time_t lastTime;     /* Used to detect system clock skew */
    ...
} aeEventLoop;
</code></pre>

<p>如果发现了系统时间被改变（小于上次 <code>processTimeEvents</code> 函数执行的开始时间），就会强制所有时间事件尽早执行。</p>

<pre><code class="language-c">    prev = NULL;
    te = eventLoop-&gt;timeEventHead;
    maxId = eventLoop-&gt;timeEventNextId-1;
    while(te) {
        long now_sec, now_ms;
        long long id;

        if (te-&gt;id == AE_DELETED_EVENT_ID) {
            aeTimeEvent *next = te-&gt;next;
            if (prev == NULL)
                eventLoop-&gt;timeEventHead = te-&gt;next;
            else
                prev-&gt;next = te-&gt;next;
            if (te-&gt;finalizerProc)
                te-&gt;finalizerProc(eventLoop, te-&gt;clientData);
            zfree(te);
            te = next;
            continue;
        }
</code></pre>

<p>Redis 处理时间事件时，不会在当前循环中直接移除不再需要执行的事件，而是会在当前循环中将时间事件的 <code>id</code> 设置为 <code>AE_DELETED_EVENT_ID</code>，然后再下一个循环中删除，并执行绑定的 <code>finalizerProc</code>。</p>

<pre><code class="language-c">        aeGetTime(&amp;now_sec, &amp;now_ms);
        if (now_sec &gt; te-&gt;when_sec ||
            (now_sec == te-&gt;when_sec &amp;&amp; now_ms &gt;= te-&gt;when_ms))
        {
            int retval;

            id = te-&gt;id;
            retval = te-&gt;timeProc(eventLoop, id, te-&gt;clientData);
            processed++;
            if (retval != AE_NOMORE) {
                aeAddMillisecondsToNow(retval,&amp;te-&gt;when_sec,&amp;te-&gt;when_ms);
            } else {
                te-&gt;id = AE_DELETED_EVENT_ID;
            }
        }
        prev = te;
        te = te-&gt;next;
    }
    return processed;
}
</code></pre>

<p>在移除不需要执行的时间事件之后，我们就开始通过比较时间来判断是否需要调用 <code>timeProc</code> 函数，<code>timeProc</code> 函数的返回值 <code>retval</code> 为时间事件执行的时间间隔：</p>

<ul>
  <li><code>retval == AE_NOMORE</code>：将时间事件的 <code>id</code> 设置为 <code>AE_DELETED_EVENT_ID</code>，等待下次 <code>aeProcessEvents</code> 执行时将事件清除；</li>
  <li><code>retval != AE_NOMORE</code>：修改当前时间事件的执行时间并重复利用当前的时间事件；</li>
</ul>

<p>以使用 <code>aeCreateTimeEvent</code> 一个创建的简单时间事件为例：</p>

<pre><code class="language-c">aeCreateTimeEvent(config.el,1,showThroughput,NULL,NULL)
</code></pre>

<p>时间事件对应的函数 <code>showThroughput</code> 在每次执行时会返回一个数字，也就是该事件发生的时间间隔：</p>

<pre><code class="language-c">int showThroughput(struct aeEventLoop *eventLoop, long long id, void *clientData) {
    ...
    float dt = (float)(mstime()-config.start)/1000.0;
    float rps = (float)config.requests_finished/dt;
    printf("%s: %.2f\r", config.title, rps);
    fflush(stdout);
    return 250; /* every 250ms */
}
</code></pre>

<p>这样就不需要重新 <code>malloc</code> 一块相同大小的内存，提高了时间事件处理的性能，并减少了内存的使用量。</p>

<p>我们对 Redis 中对时间事件的处理以流程图的形式简单总结一下：</p>

<p><img src="https://img.nju520.me/2016-12-09-process-time-event.png-1000width" alt="process-time-event" /></p>

<p>创建时间事件的方法实现其实非常简单，在这里不想过多分析这个方法，唯一需要注意的就是时间事件的 <code>id</code> 跟数据库中的大多数主键都是递增的：</p>

<pre><code class="language-c">long long aeCreateTimeEvent(aeEventLoop *eventLoop, long long milliseconds,
        aeTimeProc *proc, void *clientData,
        aeEventFinalizerProc *finalizerProc) {
    long long id = eventLoop-&gt;timeEventNextId++;
    aeTimeEvent *te;

    te = zmalloc(sizeof(*te));
    if (te == NULL) return AE_ERR;
    te-&gt;id = id;
    aeAddMillisecondsToNow(milliseconds,&amp;te-&gt;when_sec,&amp;te-&gt;when_ms);
    te-&gt;timeProc = proc;
    te-&gt;finalizerProc = finalizerProc;
    te-&gt;clientData = clientData;
    te-&gt;next = eventLoop-&gt;timeEventHead;
    eventLoop-&gt;timeEventHead = te;
    return id;
}
</code></pre>

<h2 id="事件的处理">事件的处理</h2>

<blockquote>
  <p>上一章节我们已经从代码的角度对 Redis 中事件的处理有一定的了解，在这里，我想从更高的角度来观察 Redis 对于事件的处理是怎么进行的。</p>
</blockquote>

<p>整个 Redis 服务在启动之后会陷入一个巨大的 while 循环，不停地执行 <code>processEvents</code> 方法处理文件事件 fe 和时间事件 te 。</p>

<blockquote>
  <p>有关 Redis 中的 I/O 多路复用模块可以看这篇文章 <a href="http://nju520.me/redis-io-multiplexing/">Redis 和 I/O 多路复用</a>。</p>
</blockquote>

<p>当文件事件触发时会被标记为 “红色” 交由 <code>processEvents</code> 方法处理，而时间事件的处理都会交给 <code>processTimeEvents</code> 这一子方法：</p>

<p><img src="https://img.nju520.me/2016-12-09-redis-eventloop-proces-event.png-1000width" alt="redis-eventloop-proces-event" /></p>

<p>在每个事件循环中 Redis 都会先处理文件事件，然后再处理时间事件直到整个循环停止，<code>processEvents</code> 和 <code>processTimeEvents</code> 作为 Redis 中发生事件的消费者，每次都会从“事件池”中拉去待处理的事件进行消费。</p>

<h3 id="文件事件的处理">文件事件的处理</h3>

<p>由于文件事件触发条件较多，并且 OS 底层实现差异性较大，底层的 I/O 多路复用模块使用了 <code>eventLoop-&gt;aeFiredEvent</code> 保存对应的文件描述符以及事件，将信息传递给上层进行处理，并抹平了底层实现的差异。</p>

<p>整个 I/O 多路复用模块在事件循环看来就是一个输入事件、输出 <code>aeFiredEvent</code> 数组的一个黑箱：</p>

<p><img src="https://img.nju520.me/2016-12-09-eventloop-file-event-in-redis.png-1000width" alt="eventloop-file-event-in-redis" /></p>

<p>在这个黑箱中，我们使用 <code>aeCreateFileEvent</code>、 <code>aeDeleteFileEvent</code> 来添加删除需要监听的文件描述符以及事件。</p>

<p>在对应事件发生时，当前单元格会“变色”表示发生了可读（黄色）或可写（绿色）事件，调用 <code>aeApiPoll</code> 时会把对应的文件描述符和事件放入 <code>aeFiredEvent</code> 数组，并在 <code>processEvents</code> 方法中执行事件对应的回调。</p>

<h3 id="时间事件的处理">时间事件的处理</h3>

<p>时间事件的处理相比文件事件就容易多了，每次 <code>processTimeEvents</code> 方法调用时都会对整个 <code>timeEventHead</code> 数组进行遍历：</p>

<p><img src="https://img.nju520.me/2016-12-09-process-time-events-in-redis.png-1000width" alt="process-time-events-in-redis" /></p>

<p>遍历的过程中会将时间的触发时间与当前时间比较，然后执行时间对应的 <code>timeProc</code>，并根据 <code>timeProc</code> 的返回值修改当前事件的参数，并在下一个循环的遍历中移除不再执行的时间事件。</p>

<h2 id="总结">总结</h2>

<blockquote>
  <p>笔者对于文章中两个模块的展示顺序考虑了比较久的时间，最后还是觉得，目前这样的顺序更易于理解。</p>
</blockquote>

<p>Redis 对于事件的处理方式十分精巧，通过传入函数指针以及返回值的方式，将时间事件移除的控制权交给了需要执行的处理器 <code>timeProc</code>，在 <code>processTimeEvents</code> 设置 <code>aeApiPoll</code> 超时时间也十分巧妙，充分地利用了每一次事件循环，防止过多的无用的空转，并且保证了该方法不会阻塞太长时间。</p>

<p>事件循环的机制并不能时间事件准确地在某一个时间点一定执行，往往会比实际约定处理的时间稍微晚一些。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://redis.io/topics/internals-rediseventlib">Redis Event Library</a></li>
  <li><a href="http://key-value-stories.blogspot.com/2015/01/redis-core-implementation.html">Redis Core Implementation</a></li>
  <li><a href="http://nju520.me/redis-io-multiplexing/">Redis 和 I/O 多路复用</a></li>
  <li><a href="http://redisbook.com">Redis 设计与实现</a></li>
</ul>

<h2 id="其它">其它</h2>

<blockquote>

  <p>Source: http://nju520.me/redis-eventloop</p>
</blockquote>

  ]]></description>
</item>

<item>
  <title>Redis 和 I/O 多路复用</title>
  <link>//redis-io-multiplexing</link>
  <author>nju520</author>
  <pubDate>2016-11-26T14:07:18+08:00</pubDate>
  <guid>//redis-io-multiplexing</guid>
  <description><![CDATA[
  <p>最近在看 UNIX 网络编程并研究了一下 Redis 的实现，感觉 Redis 的源代码十分适合阅读和分析，其中 I/O 多路复用（mutiplexing）部分的实现非常干净和优雅，在这里想对这部分的内容进行简单的整理。</p>

<h2 id="几种-io-模型">几种 I/O 模型</h2>

<p>为什么 Redis 中要使用 I/O 多路复用这种技术呢？</p>

<p>首先，Redis 是跑在单线程中的，所有的操作都是按照顺序线性执行的，但是由于读写操作等待用户输入或输出都是阻塞的，所以 I/O 操作在一般情况下往往不能直接返回，这会导致某一文件的 I/O 阻塞导致整个进程无法对其它客户提供服务，而 <strong>I/O 多路复用</strong>就是为了解决这个问题而出现的。</p>

<h3 id="blocking-io">Blocking I/O</h3>

<p>先来看一下传统的阻塞 I/O 模型到底是如何工作的：当使用 <code>read</code> 或者 <code>write</code> 对某一个<strong>文件描述符（File Descriptor 以下简称 FD)</strong>进行读写时，如果当前 FD 不可读或不可写，整个 Redis 服务就不会对其它的操作作出响应，导致整个服务不可用。</p>

<p>这也就是传统意义上的，也就是我们在编程中使用最多的阻塞模型：</p>

<p><img src="https://img.nju520.me/2016-11-26-blocking-io.png-1000width" alt="blocking-io" /></p>

<p>阻塞模型虽然开发中非常常见也非常易于理解，但是由于它会影响其他 FD 对应的服务，所以在需要处理多个客户端任务的时候，往往都不会使用阻塞模型。</p>

<h3 id="io-多路复用">I/O 多路复用</h3>

<blockquote>
  <p>虽然还有很多其它的 I/O 模型，但是在这里都不会具体介绍。</p>
</blockquote>

<p>阻塞式的 I/O 模型并不能满足这里的需求，我们需要一种效率更高的 I/O 模型来支撑 Redis 的多个客户（redis-cli），这里涉及的就是 I/O 多路复用模型了：</p>

<p><img src="https://img.nju520.me/2016-11-26-I:O-Multiplexing-Model.png-1000width" alt="I:O-Multiplexing-Mode" /></p>

<p>在 I/O 多路复用模型中，最重要的函数调用就是 <code>select</code>，该方法的能够同时监控多个文件描述符的可读可写情况，当其中的某些文件描述符可读或者可写时，<code>select</code> 方法就会返回可读以及可写的文件描述符个数。</p>

<blockquote>
  <p>关于 <code>select</code> 的具体使用方法，在网络上资料很多，这里就不过多展开介绍了；</p>

  <p>与此同时也有其它的 I/O 多路复用函数 <code>epoll/kqueue/evport</code>，它们相比 <code>select</code> 性能更优秀，同时也能支撑更多的服务。</p>
</blockquote>

<h2 id="reactor-设计模式">Reactor 设计模式</h2>

<p>Redis 服务采用 Reactor 的方式来实现文件事件处理器（每一个网络连接其实都对应一个文件描述符）</p>

<p><img src="https://img.nju520.me/2016-11-26-redis-reactor-pattern.png-1000width" alt="redis-reactor-pattern" /></p>

<p>文件事件处理器使用 I/O 多路复用模块同时监听多个 FD，当 <code>accept</code>、<code>read</code>、<code>write</code> 和 <code>close</code> 文件事件产生时，文件事件处理器就会回调 FD 绑定的事件处理器。</p>

<p>虽然整个文件事件处理器是在单线程上运行的，但是通过 I/O 多路复用模块的引入，实现了同时对多个 FD 读写的监控，提高了网络通信模型的性能，同时也可以保证整个 Redis 服务实现的简单。</p>

<h2 id="io-多路复用模块">I/O 多路复用模块</h2>

<p>I/O 多路复用模块封装了底层的 <code>select</code>、<code>epoll</code>、<code>avport</code> 以及 <code>kqueue</code> 这些 I/O 多路复用函数，为上层提供了相同的接口。</p>

<p><img src="https://img.nju520.me/2016-11-26-ae-module.jpg-1000width" alt="ae-module" /></p>

<p>在这里我们简单介绍 Redis 是如何包装 <code>select</code> 和 <code>epoll</code> 的，简要了解该模块的功能，整个 I/O 多路复用模块抹平了不同平台上 I/O 多路复用函数的差异性，提供了相同的接口：</p>

<ul>
  <li><code>static int  aeApiCreate(aeEventLoop *eventLoop)</code></li>
  <li><code>static int  aeApiResize(aeEventLoop *eventLoop, int setsize)</code></li>
  <li><code>static void aeApiFree(aeEventLoop *eventLoop)</code></li>
  <li><code>static int  aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask)</code></li>
  <li><code>static void aeApiDelEvent(aeEventLoop *eventLoop, int fd, int mask) </code></li>
  <li><code>static int  aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp)</code></li>
</ul>

<p>同时，因为各个函数所需要的参数不同，我们在每一个子模块内部通过一个 <code>aeApiState</code> 来存储需要的上下文信息：</p>

<pre><code class="language-c">// select
typedef struct aeApiState {
    fd_set rfds, wfds;
    fd_set _rfds, _wfds;
} aeApiState;

// epoll
typedef struct aeApiState {
    int epfd;
    struct epoll_event *events;
} aeApiState;
</code></pre>

<p>这些上下文信息会存储在 <code>eventLoop</code> 的 <code>void *state</code> 中，不会暴露到上层，只在当前子模块中使用。</p>

<h3 id="封装-select-函数">封装 select 函数</h3>

<blockquote>
  <p><code>select</code> 可以监控 FD 的可读、可写以及出现错误的情况。</p>
</blockquote>

<p>在介绍 I/O 多路复用模块如何对 <code>select</code> 函数封装之前，先来看一下 <code>select</code> 函数使用的大致流程：</p>

<pre><code class="language-c">int fd = /* file descriptor */

fd_set rfds;
FD_ZERO(&amp;rfds);
FD_SET(fd, &amp;rfds)

for ( ; ; ) {
    select(fd+1, &amp;rfds, NULL, NULL, NULL);
    if (FD_ISSET(fd, &amp;rfds)) {
        /* file descriptor `fd` becomes readable */
    }
}
</code></pre>

<ol>
  <li>初始化一个可读的 <code>fd_set</code> 集合，保存需要监控可读性的 FD；</li>
  <li>使用 <code>FD_SET</code> 将 <code>fd</code> 加入 <code>rfds</code>；</li>
  <li>调用 <code>select</code> 方法监控 <code>rfds</code> 中的 FD 是否可读；</li>
  <li>当 <code>select</code> 返回时，检查 FD 的状态并完成对应的操作。</li>
</ol>

<p>而在 Redis 的 <code>ae_select</code> 文件中代码的组织顺序也是差不多的，首先在 <code>aeApiCreate</code> 函数中初始化 <code>rfds</code> 和 <code>wfds</code>：</p>

<pre><code class="language-c">static int aeApiCreate(aeEventLoop *eventLoop) {
    aeApiState *state = zmalloc(sizeof(aeApiState));
    if (!state) return -1;
    FD_ZERO(&amp;state-&gt;rfds);
    FD_ZERO(&amp;state-&gt;wfds);
    eventLoop-&gt;apidata = state;
    return 0;
}
</code></pre>

<p>而 <code>aeApiAddEvent</code> 和 <code>aeApiDelEvent</code> 会通过 <code>FD_SET</code> 和 <code>FD_CLR</code> 修改 <code>fd_set</code> 中对应 FD 的标志位：</p>

<pre><code class="language-c">static int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) {
    aeApiState *state = eventLoop-&gt;apidata;
    if (mask &amp; AE_READABLE) FD_SET(fd,&amp;state-&gt;rfds);
    if (mask &amp; AE_WRITABLE) FD_SET(fd,&amp;state-&gt;wfds);
    return 0;
}
</code></pre>

<p>整个 <code>ae_select</code> 子模块中最重要的函数就是 <code>aeApiPoll</code>，它是实际调用 <code>select</code> 函数的部分，其作用就是在 I/O 多路复用函数返回时，将对应的 FD 加入 <code>aeEventLoop</code> 的 <code>fired</code> 数组中，并返回事件的个数：</p>

<pre><code class="language-c">static int aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp) {
    aeApiState *state = eventLoop-&gt;apidata;
    int retval, j, numevents = 0;

    memcpy(&amp;state-&gt;_rfds,&amp;state-&gt;rfds,sizeof(fd_set));
    memcpy(&amp;state-&gt;_wfds,&amp;state-&gt;wfds,sizeof(fd_set));

    retval = select(eventLoop-&gt;maxfd+1,
                &amp;state-&gt;_rfds,&amp;state-&gt;_wfds,NULL,tvp);
    if (retval &gt; 0) {
        for (j = 0; j &lt;= eventLoop-&gt;maxfd; j++) {
            int mask = 0;
            aeFileEvent *fe = &amp;eventLoop-&gt;events[j];

            if (fe-&gt;mask == AE_NONE) continue;
            if (fe-&gt;mask &amp; AE_READABLE &amp;&amp; FD_ISSET(j,&amp;state-&gt;_rfds))
                mask |= AE_READABLE;
            if (fe-&gt;mask &amp; AE_WRITABLE &amp;&amp; FD_ISSET(j,&amp;state-&gt;_wfds))
                mask |= AE_WRITABLE;
            eventLoop-&gt;fired[numevents].fd = j;
            eventLoop-&gt;fired[numevents].mask = mask;
            numevents++;
        }
    }
    return numevents;
}
</code></pre>

<h3 id="封装-epoll-函数">封装 epoll 函数</h3>

<p>Redis 对 <code>epoll</code> 的封装其实也是类似的，使用 <code>epoll_create</code> 创建 <code>epoll</code> 中使用的 <code>epfd</code>：</p>

<pre><code class="language-c">static int aeApiCreate(aeEventLoop *eventLoop) {
    aeApiState *state = zmalloc(sizeof(aeApiState));

    if (!state) return -1;
    state-&gt;events = zmalloc(sizeof(struct epoll_event)*eventLoop-&gt;setsize);
    if (!state-&gt;events) {
        zfree(state);
        return -1;
    }
    state-&gt;epfd = epoll_create(1024); /* 1024 is just a hint for the kernel */
    if (state-&gt;epfd == -1) {
        zfree(state-&gt;events);
        zfree(state);
        return -1;
    }
    eventLoop-&gt;apidata = state;
    return 0;
}
</code></pre>

<p>在 <code>aeApiAddEvent</code> 中使用 <code>epoll_ctl</code> 向 <code>epfd</code> 中添加需要监控的 FD 以及监听的事件：</p>

<pre><code class="language-c">static int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) {
    aeApiState *state = eventLoop-&gt;apidata;
    struct epoll_event ee = {0}; /* avoid valgrind warning */
    /* If the fd was already monitored for some event, we need a MOD
     * operation. Otherwise we need an ADD operation. */
    int op = eventLoop-&gt;events[fd].mask == AE_NONE ?
            EPOLL_CTL_ADD : EPOLL_CTL_MOD;

    ee.events = 0;
    mask |= eventLoop-&gt;events[fd].mask; /* Merge old events */
    if (mask &amp; AE_READABLE) ee.events |= EPOLLIN;
    if (mask &amp; AE_WRITABLE) ee.events |= EPOLLOUT;
    ee.data.fd = fd;
    if (epoll_ctl(state-&gt;epfd,op,fd,&amp;ee) == -1) return -1;
    return 0;
}
</code></pre>

<p>由于 <code>epoll</code> 相比 <code>select</code> 机制略有不同，在 <code>epoll_wait</code> 函数返回时并不需要遍历所有的 FD 查看读写情况；在 <code>epoll_wait</code> 函数返回时会提供一个 <code>epoll_event</code> 数组：</p>

<pre><code class="language-c">typedef union epoll_data {
    void    *ptr;
    int      fd; /* 文件描述符 */
    uint32_t u32;
    uint64_t u64;
} epoll_data_t;

struct epoll_event {
    uint32_t     events; /* Epoll 事件 */
    epoll_data_t data;
};
</code></pre>

<blockquote>
  <p>其中保存了发生的 <code>epoll</code> 事件（<code>EPOLLIN</code>、<code>EPOLLOUT</code>、<code>EPOLLERR</code> 和 <code>EPOLLHUP</code>）以及发生该事件的 FD。</p>
</blockquote>

<p><code>aeApiPoll</code> 函数只需要将 <code>epoll_event</code> 数组中存储的信息加入 <code>eventLoop</code> 的 <code>fired</code> 数组中，将信息传递给上层模块：</p>

<pre><code class="language-c">static int aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp) {
    aeApiState *state = eventLoop-&gt;apidata;
    int retval, numevents = 0;

    retval = epoll_wait(state-&gt;epfd,state-&gt;events,eventLoop-&gt;setsize,
            tvp ? (tvp-&gt;tv_sec*1000 + tvp-&gt;tv_usec/1000) : -1);
    if (retval &gt; 0) {
        int j;

        numevents = retval;
        for (j = 0; j &lt; numevents; j++) {
            int mask = 0;
            struct epoll_event *e = state-&gt;events+j;

            if (e-&gt;events &amp; EPOLLIN) mask |= AE_READABLE;
            if (e-&gt;events &amp; EPOLLOUT) mask |= AE_WRITABLE;
            if (e-&gt;events &amp; EPOLLERR) mask |= AE_WRITABLE;
            if (e-&gt;events &amp; EPOLLHUP) mask |= AE_WRITABLE;
            eventLoop-&gt;fired[j].fd = e-&gt;data.fd;
            eventLoop-&gt;fired[j].mask = mask;
        }
    }
    return numevents;
}
</code></pre>

<h3 id="子模块的选择">子模块的选择</h3>

<p>因为 Redis 需要在多个平台上运行，同时为了最大化执行的效率与性能，所以会根据编译平台的不同选择不同的 I/O 多路复用函数作为子模块，提供给上层统一的接口；在 Redis 中，我们通过宏定义的使用，合理的选择不同的子模块：</p>

<pre><code class="language-c">#ifdef HAVE_EVPORT
#include "ae_evport.c"
#else
    #ifdef HAVE_EPOLL
    #include "ae_epoll.c"
    #else
        #ifdef HAVE_KQUEUE
        #include "ae_kqueue.c"
        #else
        #include "ae_select.c"
        #endif
    #endif
#endif
</code></pre>

<p>因为 <code>select</code> 函数是作为 POSIX 标准中的系统调用，在不同版本的操作系统上都会实现，所以将其作为保底方案：</p>

<p><img src="https://img.nju520.me/2016-11-26-redis-choose-io-function.jpg-1000width" alt="redis-choose-io-function" /></p>

<p>Redis 会优先选择时间复杂度为 $O(1)$ 的 I/O 多路复用函数作为底层实现，包括 Solaries 10 中的 <code>evport</code>、Linux 中的 <code>epoll</code> 和 macOS/FreeBSD 中的 <code>kqueue</code>，上述的这些函数都使用了内核内部的结构，并且能够服务几十万的文件描述符。</p>

<p>但是如果当前编译环境没有上述函数，就会选择 <code>select</code> 作为备选方案，由于其在使用时会扫描全部监听的描述符，所以其时间复杂度较差 $O(n)$，并且只能同时服务 1024 个文件描述符，所以一般并不会以 <code>select</code> 作为第一方案使用。</p>

<h2 id="总结">总结</h2>

<p>Redis 对于 I/O 多路复用模块的设计非常简洁，通过宏保证了 I/O 多路复用模块在不同平台上都有着优异的性能，将不同的 I/O 多路复用函数封装成相同的 API 提供给上层使用。</p>

<p>整个模块使 Redis 能以单进程运行的同时服务成千上万个文件描述符，避免了由于多进程应用的引入导致代码实现复杂度的提升，减少了出错的可能性。</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="http://man7.org/linux/man-pages/man2/select.2.html">Select-Man-Pages</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Reactor_pattern">Reactor-Pattern</a></li>
  <li><a href="https://people.eecs.berkeley.edu/~sangjin/2012/12/21/epoll-vs-kqueue.html">epoll vs kqueue</a></li>
</ul>

<h2 id="其它">其它</h2>

<blockquote>

  <p>Source: http://nju520.me/redis-io-multiplexing</p>
</blockquote>

  ]]></description>
</item>


  </channel>
</rss>
